{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Uni.lu HPC Technical Documentation \u00b6 hpc-docs.uni.lu is a resource with the technical details for users to make effective use of the Uni.lu High Performance Computing (ULHPC) Facility 's resources. ULHPC Supercomputers Getting Started Consider our latest ULHPC User Guide (PDF) ULHPC Web Portals \u00b6 ULHPC Home page - center news and information: hpc.uni.lu Helpdesk / Ticket Portal - open tickets, make requests Uni.lu Service Now Portal - use your Uni.lu credentials to access. NEW! ULHPC Discourse - Forum-like community portal ULHPC Tutorials - HPC tutorials covering many (many) topics: ulhpc-tutorials.readthedocs.io Trainings and HPC School Popular documentation pages \u00b6 SSH Management on ULHPC Identity Management Portal (IdM/IPA) Usage Charging Policy Job Status and Reason Codes Job Prioritization Factors Example of Job Launchers - currated example of job launcher scripts Slurm overview - Slurm commands, job script basics, submitting, updating jobs Join and Monitor Jobs File permissions - Unix file permissions ULHPC Software/Modules Environment Compiling/Building your own software About this site The ULHPC Technical Documentation is based on MkDocs and the mkdocs-material theme, and inspired by the (excellent) NERSC documentation site. These pages are hosted from a git repository and contributions are welcome!","title":"Home"},{"location":"#unilu-hpc-technical-documentation","text":"hpc-docs.uni.lu is a resource with the technical details for users to make effective use of the Uni.lu High Performance Computing (ULHPC) Facility 's resources. ULHPC Supercomputers Getting Started Consider our latest ULHPC User Guide (PDF)","title":"Uni.lu HPC Technical Documentation"},{"location":"#ulhpc-web-portals","text":"ULHPC Home page - center news and information: hpc.uni.lu Helpdesk / Ticket Portal - open tickets, make requests Uni.lu Service Now Portal - use your Uni.lu credentials to access. NEW! ULHPC Discourse - Forum-like community portal ULHPC Tutorials - HPC tutorials covering many (many) topics: ulhpc-tutorials.readthedocs.io Trainings and HPC School","title":"ULHPC Web Portals"},{"location":"#popular-documentation-pages","text":"SSH Management on ULHPC Identity Management Portal (IdM/IPA) Usage Charging Policy Job Status and Reason Codes Job Prioritization Factors Example of Job Launchers - currated example of job launcher scripts Slurm overview - Slurm commands, job script basics, submitting, updating jobs Join and Monitor Jobs File permissions - Unix file permissions ULHPC Software/Modules Environment Compiling/Building your own software About this site The ULHPC Technical Documentation is based on MkDocs and the mkdocs-material theme, and inspired by the (excellent) NERSC documentation site. These pages are hosted from a git repository and contributions are welcome!","title":"Popular documentation pages"},{"location":"getting-started/","text":"Getting Started on ULHPC Facilities \u00b6 Welcome to the High Performance Computing (HPC) Facility of the University of Luxembourg (ULHPC)! This page will guide you through the basics of using ULHPC's supercomputers, storage systems, and services. What is ULHPC ? \u00b6 HPC is crucial in academic environments to achieve high-quality results in all application areas. All world-class universities require this type of facility to accelerate its research and ensure cutting-edge results in time to face the global competition. What is High Performance Computing? If you're new to all of this, this is probably the first question you have in mind. Here is a possible definition: \" High Performance Computing (HPC) most generally refers to the practice of aggregating computing power in a way that delivers much higher performance than one could get out of a typical desktop computer or workstation in order to solve large problems in science, engineering, or business. \" Indeed, with the advent of the technological revolution and the digital transformation that made all scientific disciplines becoming computational nowadays, High-Performance Computing (HPC) is increasingly identified as a strategic asset and enabler to accelerate the research performed in all areas requiring intensive computing and large-scale Big Data analytic capabilities. Tasks which would typically require several years or centuries to be computed on a typical desktop computer may only require a couple of hours, days or weeks over an HPC system. For more details, you may want to refer to this Inside HPC article . Since 2007, the University of Luxembourg (UL) has invested tens of millions of euro into its own HPC facilities to responds to the growing needs for increased computing and storage. ULHPC (sometimes referred to as Uni.lu HPC) is the entity providing High Performance Computing and Big Data Storage services and support for UL researchers and its external partners. Led by Dr. Varrette for 15 years (until Aug 31, 2022), the ULHPC team (recently renamed \"Digital Platform\" and headed by Hyacinthe Cartiaux starting Sept 1 st , 2022) manages several research computing facilities located on the Belval campus , offering a cutting-edge research infrastructure to Luxembourg public research while serving as edge access to bigger systems from PRACE or EuroHPC, such as the Euro-HPC Luxembourg supercomputer \" MeluXina \". Warning In particular, the ULHPC is NOT the national HPC center of Luxembourg, but simply one of its strategic partner operating the second largest HPC facility of the country. The HPC facility is one element of the extensive digital research infrastructure and expertise developed by the University over the last years. It also supports the University\u2019s ambitious digital strategy and in particular the creation of a Facility for Data and HPC Sciences. This facility aims to provide a world-class user-driven digital infrastructure and services for fostering the development of collaborative activities related to frontier research and teaching in the fields of Computational and Data Sciences, including High Performance Computing, Data Analytics, Big Data Applications, Artificial Intelligence and Machine Learning. Reference ULHPC Article to cite If you want to get a good overview of the way our facility is setup, managed and evaluated, you can refer to the reference article you are in all cases entitled to refer to when crediting the ULHPC facility as per AUP (see also the publication page instructions ). ACM Reference Format | ORBilu entry | ULHPC blog post | slides : Sebastien Varrette, Hyacinthe Cartiaux, Sarah Peter, Emmanuel Kieffer, Teddy Valette, and Abatcha Olloh. 2022. Management of an Academic HPC & Research Computing Facility: The ULHPC Experience 2.0. In 6 th High Performance Computing and Cluster Technologies Conference (HPCCT 2022), July 08-10, 2022, Fuzhou, China. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3560442.3560445 Supercomputing and Storage Resources at a glance \u00b6 ULHPC is a strategic asset of the university and an important factor for the scientific and therefore also economic competitiveness of the Grand Duchy of Luxembourg. We provide a key research infrastructure featuring state-of-the-art computing and storage resources serving the UL HPC community primarily composed by UL researchers. The UL HPC platform has kept growing over time thanks to the continuous efforts of the core HPC / Digital Platform team - contact: hpc-team@uni.lu , recently completed with the EuroHPC Competence Center Task force (A. Vandeventer (Project Manager), L. Koutsantonis). ULHPC Computing and Storage Capacity (2022) Installed in the premises of the University\u2019s Centre de Calcul (CDC), the UL HPC facilities provides a total computing capacity of 2.76 PetaFlops and a shared storage capacity of around 10 PetaBytes . How big is 1 PetaFlops? 1 PetaByte? 1 PetaFlops = 10 15 floating-point operations per second (PFlops or PF for short), corresponds to the cumulative performance of more than 3510 Macbook Pro 13\" laptops 1 , or 7420 iPhone XS 2 1 PetaByte = 10 15 bytes = 8*10 15 bits, corresponding to the cumulative raw capacity of more than 1950 SSDs 512GB. This places the HPC center of the University of Luxembourg as one of the major actors in HPC and Big Data for the Greater Region Saar-Lor-Lux. In practice, the UL HPC Facility features 3 types of computing resources: \" regular \" nodes: Dual CPU, no accelerators, 128 to 256 GB of RAM \" gpu \" nodes: Dual CPU, 4 Nvidia accelerators, 768 GB RAM \" bigmem \" nodes: Quad-CPU, no accelerators, 3072 GB RAM These resources can be reserved and allocated for the execution of jobs scheduled on the platform thanks to a Resource and Job Management Systems (RJMS) - Slurm in practice. This tool allows for a fine-grain analysis and accounting of the used resources, facilitating the generation of activity reports for a given time period. Iris \u00b6 iris , in production since June 2017, is a Dell/Intel supercomputer with a theoretical peak performance of 1082 TFlop/s , featuring 196 computing nodes (totalling 5824 computing cores) and 96 GPU accelerators (NVidia V100). Iris Detailed system specifications Aion \u00b6 aion , in production since October 2020, is a Bull Sequana XH2000 /AMD supercomputer offering a peak performance of 1692 TFlop/s , featuring 318 compute nodes (totalling 40704 computing cores). Aion Detailed system specifications GPFS/SpectrumScale File System ( $HOME , project) \u00b6 IBM Spectrum Scale , formerly known as the General Parallel File System (GPFS), is global high -performance clustered file system available on all ULHPC computational systems through a DDN GridScaler/GS7K system. It allows sharing homedirs and project data between users, systems, and eventually (i.e. if needed) with the \"outside world\". GPFS/Spectrumscale Detailed specifications Lustre File System ( $SCRATCH ) \u00b6 The Lustre file system is an open-source, parallel file system that supports many requirements of leadership class HPC simulation environments. It is available as a global high -performance file system on all ULHPC computational systems through a DDN ExaScaler and is meant to host temporary scratch data . Lustre Detailed specifications OneFS File System (project, backup, archival) \u00b6 In 2014, the SIU, the UL HPC and the LCSB join their forces (and their funding) to acquire a scalable and modular NAS solution able to sustain the need for an internal big data storage, i.e. provides space for centralized data and backups of all devices used by the UL staff and all research-related data, including the one proceed on the UL HPC platform. A global low -performance Dell/EMC Isilon system is available on all ULHPC computational systems. It is intended for long term storage of data that is not frequently accessed. For more details, see Isilon specifications . Fast Infiniband Network \u00b6 High Performance Computing (HPC) encompasses advanced computation over parallel processing, enabling faster execution of highly compute intensive tasks. The execution time of a given simulation depends upon many factors, such as the number of CPU/GPU cores and their utilisation factor and the interconnect performance, efficiency, and scalability. InfiniBand is the fast interconnect technology implemented within all ULHPC supercomputers , more specifically: Iris relies on a EDR Infiniband (IB) Fabric in a Fat-Tree Topology Aion relies on a HDR100 Infiniband (IB) Fabric in a Fat-Tree Topology For more details, see ULHPC IB Network Detailed specifications . Acceptable Use Policy (AUP) \u00b6 There are a number of policies which apply to ULHPC users. UL HPC Acceptable Use Policy (AUP) [pdf] Important All users of UL HPC resources and PIs must abide by the UL HPC Acceptable Use Policy (AUP) . You should read and keep a signed copy of this document before using the facility. Access and/or usage of any ULHPC system assumes the tacit acknowledgement to this policy. ULHPC Accounts \u00b6 In order to use the ULHPC facilities, you need to have a user account with an associated user login name (also called username) placed under an account hierarchy. Get a ULHPC account Understanding Slurm account hierarchy and accounting rules ULHPC Identity Management (IPA portal) Password policy Usage Charging Policy Connecting to ULHPC supercomputers \u00b6 MFA is strongly encouraged for all ULHPC users It will be soon become mandatory - detailed instructions will be provided soon. SSH Open On Demand Portal ULHPC Login/Access servers Troubleshooting connection problems Data Management \u00b6 Global Directory Structure Transferring data : Tools and recommendations to transfer data both inside and outside of ULHPC. Quotas and Purging Understanding Unix File Permissions User Environment \u00b6 Info $HOME , Project and $SCRATCH directories are shared across all ULHPC systems, meaning that every file/directory pushed or created on the front-end is available on the computing nodes every file/directory pushed or created on the computing nodes is available on the front-end ULHPC User Environment Computing Software Environment \u00b6 The ULHPC Team supplies a large variety of HPC utilities, scientific applications and programming libraries to its user community. The user software environment is generated using Easybuild (EB) and is made available as environment modules through LMod . ULHPC Modules Environment ULHPC Supported Software List . Available modules are reachable from the compute nodes only via module avail ULHPC Easybuild Configuration Running Containers Software building support If you need help to build / develop software, we encourage you to first try using Easybuild as a recipe probably exist for the software you consider. You can then open a ticket on HPC Help Desk Portal and we will evaluate the cost and effort required. You may also ask the help of other ULHPC users using the HPC User community mailing list: (moderated): `hpc-users@uni.lu . Running Jobs \u00b6 Typical usage of the ULHPC supercomputers involves the reservation and allocation of computing resources for the execution of jobs (submitted via launcher scripts ) and scheduled on the platform thanks to a Resource and Job Management Systems (RJMS) - Slurm in our case. Slurm on ULHPC clusters Convenient Slurm Commands Rich set of launcher scripts examples Fairshare Job Priority and Backfilling Job Accounting and Billing Interactive Computing \u00b6 ULHPC also supports interactive computing. Interactive jobs Jupyter Notebook Getting Help \u00b6 ULHPC places a very strong emphasis on enabling science and providing user-oriented systems and services. Documentation \u00b6 We have always maintained an extensive documentation and HPC tutorials available online, which aims at being the most up-to-date and comprehensive while covering many (many) topics. ULHPC Technical Documentation ULHPC Tutorials The ULHPC Team welcomes your contributions These pages are hosted from a git repository and contributions are welcome! Fork this repo Support \u00b6 ULHPC Support Overview Service Now HPC Support Portal Availability and Response Time HPC support is provided on a volunteer basis by UL HPC staff and associated UL experts working at normal business hours. We offer no guarantee on response time except with paid support contracts. Consulting \u00b6 Expert-level service and support can be provided by the University HPC staff in the form of pools of 4-hour specialized help, at 480\u20ac VAT excluded (equivalent to 120\u20ac per hour expert rate EA). Service and support activities include but are not limited to HPC training on facilities utilization, software installation, HPC workflow development. For more details: see ULHPC Consulting Services The best MacBook Pro 13\" in 2020 is equiped with Ice Lake 2 GHz Intel Quad-Core i5 processors with an estimated computing performance of 284.3 Gflops as measured by the Geekbench 4 multi-core benchmarks platform, with SGEMM \u21a9 Apple A12 Bionic, the 64-bit ARM-based system on a chip (SoC) proposed on the iPhone XS has an estimated performance of 134.7 GFlops as measured by the Geekbench 4 multi-core benchmarks platform, with SGEMM \u21a9","title":"Getting Started"},{"location":"getting-started/#getting-started-on-ulhpc-facilities","text":"Welcome to the High Performance Computing (HPC) Facility of the University of Luxembourg (ULHPC)! This page will guide you through the basics of using ULHPC's supercomputers, storage systems, and services.","title":"Getting Started on ULHPC Facilities"},{"location":"getting-started/#what-is-ulhpc","text":"HPC is crucial in academic environments to achieve high-quality results in all application areas. All world-class universities require this type of facility to accelerate its research and ensure cutting-edge results in time to face the global competition. What is High Performance Computing? If you're new to all of this, this is probably the first question you have in mind. Here is a possible definition: \" High Performance Computing (HPC) most generally refers to the practice of aggregating computing power in a way that delivers much higher performance than one could get out of a typical desktop computer or workstation in order to solve large problems in science, engineering, or business. \" Indeed, with the advent of the technological revolution and the digital transformation that made all scientific disciplines becoming computational nowadays, High-Performance Computing (HPC) is increasingly identified as a strategic asset and enabler to accelerate the research performed in all areas requiring intensive computing and large-scale Big Data analytic capabilities. Tasks which would typically require several years or centuries to be computed on a typical desktop computer may only require a couple of hours, days or weeks over an HPC system. For more details, you may want to refer to this Inside HPC article . Since 2007, the University of Luxembourg (UL) has invested tens of millions of euro into its own HPC facilities to responds to the growing needs for increased computing and storage. ULHPC (sometimes referred to as Uni.lu HPC) is the entity providing High Performance Computing and Big Data Storage services and support for UL researchers and its external partners. Led by Dr. Varrette for 15 years (until Aug 31, 2022), the ULHPC team (recently renamed \"Digital Platform\" and headed by Hyacinthe Cartiaux starting Sept 1 st , 2022) manages several research computing facilities located on the Belval campus , offering a cutting-edge research infrastructure to Luxembourg public research while serving as edge access to bigger systems from PRACE or EuroHPC, such as the Euro-HPC Luxembourg supercomputer \" MeluXina \". Warning In particular, the ULHPC is NOT the national HPC center of Luxembourg, but simply one of its strategic partner operating the second largest HPC facility of the country. The HPC facility is one element of the extensive digital research infrastructure and expertise developed by the University over the last years. It also supports the University\u2019s ambitious digital strategy and in particular the creation of a Facility for Data and HPC Sciences. This facility aims to provide a world-class user-driven digital infrastructure and services for fostering the development of collaborative activities related to frontier research and teaching in the fields of Computational and Data Sciences, including High Performance Computing, Data Analytics, Big Data Applications, Artificial Intelligence and Machine Learning. Reference ULHPC Article to cite If you want to get a good overview of the way our facility is setup, managed and evaluated, you can refer to the reference article you are in all cases entitled to refer to when crediting the ULHPC facility as per AUP (see also the publication page instructions ). ACM Reference Format | ORBilu entry | ULHPC blog post | slides : Sebastien Varrette, Hyacinthe Cartiaux, Sarah Peter, Emmanuel Kieffer, Teddy Valette, and Abatcha Olloh. 2022. Management of an Academic HPC & Research Computing Facility: The ULHPC Experience 2.0. In 6 th High Performance Computing and Cluster Technologies Conference (HPCCT 2022), July 08-10, 2022, Fuzhou, China. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3560442.3560445","title":"What is ULHPC ?"},{"location":"getting-started/#supercomputing-and-storage-resources-at-a-glance","text":"ULHPC is a strategic asset of the university and an important factor for the scientific and therefore also economic competitiveness of the Grand Duchy of Luxembourg. We provide a key research infrastructure featuring state-of-the-art computing and storage resources serving the UL HPC community primarily composed by UL researchers. The UL HPC platform has kept growing over time thanks to the continuous efforts of the core HPC / Digital Platform team - contact: hpc-team@uni.lu , recently completed with the EuroHPC Competence Center Task force (A. Vandeventer (Project Manager), L. Koutsantonis). ULHPC Computing and Storage Capacity (2022) Installed in the premises of the University\u2019s Centre de Calcul (CDC), the UL HPC facilities provides a total computing capacity of 2.76 PetaFlops and a shared storage capacity of around 10 PetaBytes . How big is 1 PetaFlops? 1 PetaByte? 1 PetaFlops = 10 15 floating-point operations per second (PFlops or PF for short), corresponds to the cumulative performance of more than 3510 Macbook Pro 13\" laptops 1 , or 7420 iPhone XS 2 1 PetaByte = 10 15 bytes = 8*10 15 bits, corresponding to the cumulative raw capacity of more than 1950 SSDs 512GB. This places the HPC center of the University of Luxembourg as one of the major actors in HPC and Big Data for the Greater Region Saar-Lor-Lux. In practice, the UL HPC Facility features 3 types of computing resources: \" regular \" nodes: Dual CPU, no accelerators, 128 to 256 GB of RAM \" gpu \" nodes: Dual CPU, 4 Nvidia accelerators, 768 GB RAM \" bigmem \" nodes: Quad-CPU, no accelerators, 3072 GB RAM These resources can be reserved and allocated for the execution of jobs scheduled on the platform thanks to a Resource and Job Management Systems (RJMS) - Slurm in practice. This tool allows for a fine-grain analysis and accounting of the used resources, facilitating the generation of activity reports for a given time period.","title":"Supercomputing and Storage Resources at a glance"},{"location":"getting-started/#iris","text":"iris , in production since June 2017, is a Dell/Intel supercomputer with a theoretical peak performance of 1082 TFlop/s , featuring 196 computing nodes (totalling 5824 computing cores) and 96 GPU accelerators (NVidia V100). Iris Detailed system specifications","title":"Iris"},{"location":"getting-started/#aion","text":"aion , in production since October 2020, is a Bull Sequana XH2000 /AMD supercomputer offering a peak performance of 1692 TFlop/s , featuring 318 compute nodes (totalling 40704 computing cores). Aion Detailed system specifications","title":"Aion"},{"location":"getting-started/#gpfsspectrumscale-file-system-home-project","text":"IBM Spectrum Scale , formerly known as the General Parallel File System (GPFS), is global high -performance clustered file system available on all ULHPC computational systems through a DDN GridScaler/GS7K system. It allows sharing homedirs and project data between users, systems, and eventually (i.e. if needed) with the \"outside world\". GPFS/Spectrumscale Detailed specifications","title":"GPFS/SpectrumScale File System ($HOME, project)"},{"location":"getting-started/#lustre-file-system-scratch","text":"The Lustre file system is an open-source, parallel file system that supports many requirements of leadership class HPC simulation environments. It is available as a global high -performance file system on all ULHPC computational systems through a DDN ExaScaler and is meant to host temporary scratch data . Lustre Detailed specifications","title":"Lustre File System ($SCRATCH)"},{"location":"getting-started/#onefs-file-system-project-backup-archival","text":"In 2014, the SIU, the UL HPC and the LCSB join their forces (and their funding) to acquire a scalable and modular NAS solution able to sustain the need for an internal big data storage, i.e. provides space for centralized data and backups of all devices used by the UL staff and all research-related data, including the one proceed on the UL HPC platform. A global low -performance Dell/EMC Isilon system is available on all ULHPC computational systems. It is intended for long term storage of data that is not frequently accessed. For more details, see Isilon specifications .","title":"OneFS File System (project, backup, archival)"},{"location":"getting-started/#fast-infiniband-network","text":"High Performance Computing (HPC) encompasses advanced computation over parallel processing, enabling faster execution of highly compute intensive tasks. The execution time of a given simulation depends upon many factors, such as the number of CPU/GPU cores and their utilisation factor and the interconnect performance, efficiency, and scalability. InfiniBand is the fast interconnect technology implemented within all ULHPC supercomputers , more specifically: Iris relies on a EDR Infiniband (IB) Fabric in a Fat-Tree Topology Aion relies on a HDR100 Infiniband (IB) Fabric in a Fat-Tree Topology For more details, see ULHPC IB Network Detailed specifications .","title":"Fast Infiniband Network"},{"location":"getting-started/#acceptable-use-policy-aup","text":"There are a number of policies which apply to ULHPC users. UL HPC Acceptable Use Policy (AUP) [pdf] Important All users of UL HPC resources and PIs must abide by the UL HPC Acceptable Use Policy (AUP) . You should read and keep a signed copy of this document before using the facility. Access and/or usage of any ULHPC system assumes the tacit acknowledgement to this policy.","title":"Acceptable Use Policy (AUP)"},{"location":"getting-started/#ulhpc-accounts","text":"In order to use the ULHPC facilities, you need to have a user account with an associated user login name (also called username) placed under an account hierarchy. Get a ULHPC account Understanding Slurm account hierarchy and accounting rules ULHPC Identity Management (IPA portal) Password policy Usage Charging Policy","title":"ULHPC Accounts"},{"location":"getting-started/#connecting-to-ulhpc-supercomputers","text":"MFA is strongly encouraged for all ULHPC users It will be soon become mandatory - detailed instructions will be provided soon. SSH Open On Demand Portal ULHPC Login/Access servers Troubleshooting connection problems","title":"Connecting to ULHPC supercomputers"},{"location":"getting-started/#data-management","text":"Global Directory Structure Transferring data : Tools and recommendations to transfer data both inside and outside of ULHPC. Quotas and Purging Understanding Unix File Permissions","title":"Data Management"},{"location":"getting-started/#user-environment","text":"Info $HOME , Project and $SCRATCH directories are shared across all ULHPC systems, meaning that every file/directory pushed or created on the front-end is available on the computing nodes every file/directory pushed or created on the computing nodes is available on the front-end ULHPC User Environment","title":"User Environment"},{"location":"getting-started/#computing-software-environment","text":"The ULHPC Team supplies a large variety of HPC utilities, scientific applications and programming libraries to its user community. The user software environment is generated using Easybuild (EB) and is made available as environment modules through LMod . ULHPC Modules Environment ULHPC Supported Software List . Available modules are reachable from the compute nodes only via module avail ULHPC Easybuild Configuration Running Containers Software building support If you need help to build / develop software, we encourage you to first try using Easybuild as a recipe probably exist for the software you consider. You can then open a ticket on HPC Help Desk Portal and we will evaluate the cost and effort required. You may also ask the help of other ULHPC users using the HPC User community mailing list: (moderated): `hpc-users@uni.lu .","title":"Computing Software Environment"},{"location":"getting-started/#running-jobs","text":"Typical usage of the ULHPC supercomputers involves the reservation and allocation of computing resources for the execution of jobs (submitted via launcher scripts ) and scheduled on the platform thanks to a Resource and Job Management Systems (RJMS) - Slurm in our case. Slurm on ULHPC clusters Convenient Slurm Commands Rich set of launcher scripts examples Fairshare Job Priority and Backfilling Job Accounting and Billing","title":"Running Jobs"},{"location":"getting-started/#interactive-computing","text":"ULHPC also supports interactive computing. Interactive jobs Jupyter Notebook","title":"Interactive Computing"},{"location":"getting-started/#getting-help","text":"ULHPC places a very strong emphasis on enabling science and providing user-oriented systems and services.","title":"Getting Help"},{"location":"getting-started/#documentation","text":"We have always maintained an extensive documentation and HPC tutorials available online, which aims at being the most up-to-date and comprehensive while covering many (many) topics. ULHPC Technical Documentation ULHPC Tutorials The ULHPC Team welcomes your contributions These pages are hosted from a git repository and contributions are welcome! Fork this repo","title":"Documentation"},{"location":"getting-started/#support","text":"ULHPC Support Overview Service Now HPC Support Portal Availability and Response Time HPC support is provided on a volunteer basis by UL HPC staff and associated UL experts working at normal business hours. We offer no guarantee on response time except with paid support contracts.","title":"Support"},{"location":"getting-started/#consulting","text":"Expert-level service and support can be provided by the University HPC staff in the form of pools of 4-hour specialized help, at 480\u20ac VAT excluded (equivalent to 120\u20ac per hour expert rate EA). Service and support activities include but are not limited to HPC training on facilities utilization, software installation, HPC workflow development. For more details: see ULHPC Consulting Services The best MacBook Pro 13\" in 2020 is equiped with Ice Lake 2 GHz Intel Quad-Core i5 processors with an estimated computing performance of 284.3 Gflops as measured by the Geekbench 4 multi-core benchmarks platform, with SGEMM \u21a9 Apple A12 Bionic, the 64-bit ARM-based system on a chip (SoC) proposed on the iPhone XS has an estimated performance of 134.7 GFlops as measured by the Geekbench 4 multi-core benchmarks platform, with SGEMM \u21a9","title":"Consulting"},{"location":"layout/","text":"This repository is organized as follows (use tree -L 2 to complete): . \u251c\u2500\u2500 Makefile # GNU Make configuration \u251c\u2500\u2500 README.md # Project README \u251c\u2500\u2500 VERSION # /!\\ DO NOT EDIT. Current repository version \u251c\u2500\u2500 docs/ # [MkDocs](mkdocs.org) main directory \u251c\u2500\u2500 mkdocs.yml # [MkDocs](mkdocs.org) configuration \u251c\u2500\u2500 .envrc # Local direnv configuration -- see https://direnv.net/ \u2502 # Assumes you have installed in ~/.config/direnv/direnvrc \u2502 # the version proposed on \u2502 # https://raw.githubusercontent.com/Falkor/dotfiles/master/direnv/direnvrc \u251c\u2500\u2500 .python- { version,virtualenv } # Pyenv/Virtualenv configuration","title":"Layout"},{"location":"setup/","text":"Pre-Requisites and Laptop Setup \u00b6 You should follow the instructions provided on the ULHPC Tutorials: Pre-requisites page. For those not familiar with Linux Shell, kindly refer to the \"Introducing the UNIX/Linux Shell\" tutorial.","title":"Pre-Requisites and Laptop Setup"},{"location":"setup/#pre-requisites-and-laptop-setup","text":"You should follow the instructions provided on the ULHPC Tutorials: Pre-requisites page. For those not familiar with Linux Shell, kindly refer to the \"Introducing the UNIX/Linux Shell\" tutorial.","title":"Pre-Requisites and Laptop Setup"},{"location":"teaching-with-the-ulhpc/","text":"Teaching with the ULHPC \u00b6 If you plan to use the ULHPC to teach for groups of students, we highly recommend that you contact us (the HPC team) for the following reasons: When possible, we can plan our maintenance sessions outside of your planned teaching / training dates. We can help with the reservation of HPC ressources (e.g., GPU or big memory nodes) as some are highly booked and may not be available on-demand the day of your teaching or training session. We can provide temporary ULHPC account for your students / attendees. Resource reservation \u00b6 The ULHPC offers different types of computing nodes and their availability can vary greatly throughout the year. In particular, GPU and big memory nodes are rare and intensively used. If you plan to use them for a teaching session, please contact our team at hpc-team@uni.lu . Temporary student accounts \u00b6 For hands-on sessions involving students or trainees who don't necessarily have an ULHPC account, we can provide temporary accesses. As a teacher / trainer, your account will also have access to all the students / trainees accounts to simplify interactions and troubleshooting during your sessions. Please contact our team at hpc-team@uni.lu to help you in the preparation of your teaching / training session.","title":"Teaching with the ULHPC"},{"location":"teaching-with-the-ulhpc/#teaching-with-the-ulhpc","text":"If you plan to use the ULHPC to teach for groups of students, we highly recommend that you contact us (the HPC team) for the following reasons: When possible, we can plan our maintenance sessions outside of your planned teaching / training dates. We can help with the reservation of HPC ressources (e.g., GPU or big memory nodes) as some are highly booked and may not be available on-demand the day of your teaching or training session. We can provide temporary ULHPC account for your students / attendees.","title":"Teaching with the ULHPC"},{"location":"teaching-with-the-ulhpc/#resource-reservation","text":"The ULHPC offers different types of computing nodes and their availability can vary greatly throughout the year. In particular, GPU and big memory nodes are rare and intensively used. If you plan to use them for a teaching session, please contact our team at hpc-team@uni.lu .","title":"Resource reservation"},{"location":"teaching-with-the-ulhpc/#temporary-student-accounts","text":"For hands-on sessions involving students or trainees who don't necessarily have an ULHPC account, we can provide temporary accesses. As a teacher / trainer, your account will also have access to all the students / trainees accounts to simplify interactions and troubleshooting during your sessions. Please contact our team at hpc-team@uni.lu to help you in the preparation of your teaching / training session.","title":"Temporary student accounts"},{"location":"accounts/","text":"Get an Account \u00b6 In order to use the ULHPC facilities, you need to have a user account with an associated user login name (also called username) placed under an account hierarchy. Conditions of acceptance \u00b6 Acceptable Use Policy (AUP) \u00b6 There are a number of policies which apply to ULHPC users. UL HPC Acceptable Use Policy (AUP) [pdf] Important All users of UL HPC resources and PIs must abide by the UL HPC Acceptable Use Policy (AUP) . You should read and keep a signed copy of this document before using the facility. Access and/or usage of any ULHPC system assumes the tacit acknowledgement to this policy. Remember that you are expected to acknowledge ULHPC in your publications . See Acceptable Use Policy for more details. ULHPC Platforms are meant ONLY for R&D! The ULHPC facility is made for Research and Development and it is NOT a full production computing center -- for such needs, consider using the National HPC center . In particular, we cannot make any guarantees of cluster availability or timely job completion even if we target a minimum compute node availability above 95% which is typically met - for instance, past KPI statistics in 2019 report a computing node availability above 97%. Resource allocation policies \u00b6 ULHPC Usage Charging and Resource allocation policy UL internal R&D and training \u00b6 ULHPC resources are free of charge for UL staff for their internal work and training activities . Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back. Any other activities will be reviewed with the rectorate and are a priori subjected to be billed. Research Projects \u00b6 Externals and private partners \u00b6 How to Get an New User account? \u00b6 Account Request Form You can submit a request for a new ULHPC account by using the Create a ULHPC account form form. Enter your personal data, your organization and your institutional contact information. Specify your direct line manager / project PI. Eventually, if you need to access a specific project directory, mention it in comment. This will be granted only after approval of the project PI. Your account will undergo user checks, in accordance with ULHPC policies, to verify your identity and the information proposed. Under some circumstances, there could be a delay while this vetting takes place. After vetting has completed, you will receive a welcome email with your login information, and a unique link to a PrivateBin 1 holding a random temporary password. That link will expire if not used within 24 hours. The PI and PI Proxies for the project will be notified when applicable. Finally, you will need to log into the HPC IPA Portal to set up your initial password and Multi-Factor Authentication (MFA) for your account. Your new password must adhere to ULHPC's password requirements see Password policy and guidelines ULHPC Identity Management (IPA portal) documentation UL HPC \\neq \\neq University credentials Be aware that the source of authentication for the HPC services ( including gitlab.uni.lu ), based on RedHat IdM/IPA DIFFERS from the University credentials (based on UL Active Directory). ULHPC credentials are maintained by the HPC team; associated portal: https://***REMOVED*** authentication service for: UL HPC, Gitlab University credentials are maintained by the IT team of the University authentication service for Service Now and all other UL services Managing User Accounts \u00b6 ULHPC user accounts are managed in through the HPC IPA web portal . Security Incidents \u00b6 If you think there has been a computer security incident you should contact the ULHPC Team and the University CISO team as soon as possible: To: hpc-team@uni.lu,niek.nigg@uni.lu Subject: Security Incident for HPC account ' <login> ' ( ADAPT accordingly ) Please save any evidence of the break-in and include as many details as possible in your communication with us. How to Get an New Project account? \u00b6 Projects are defined for accounting purposes and are associated to a set of user accounts allowed by the project PI to access its data and submit jobs on behalf of the project account. See Slurm Account Hierarchy . You can request (or be automatically added) to project accounts for accounting purposes. For more information, please see the Project Account documentation FAQ \u00b6 Can I share an account? \u2013 Account Security Policies \u00b6 Danger The sharing of passwords or login credentials is not allowed under UL HPC and University information security policies. Please bear in mind that this policy also protects the end-user. Sharing credentials removes the ability to audit and accountability for the account holder in case of account misuse. Accounts which are in violation of this policy may be disabled or otherwise limited. Accounts knowingly skirting this policy may be banned. If you find that you need to share resources among multiple individuals, shared projects are just the way to go, and remember that the University extends access to its HPC resources ( i.e. , facility and expert HPC consultants) to the scientific staff of national public organizations and external partners for the duration of joint research projects under the conditions defined above. When in doubt, please contact us and we will be happy to assist you with finding a safe and secure way to do so. PrivateBin is a minimalist, open source online pastebin where the server has zero knowledge of pasted data. Data is encrypted / decrypted in the browser using 256bit AES in Galois Counter mode. \u21a9","title":"Get an Account"},{"location":"accounts/#get-an-account","text":"In order to use the ULHPC facilities, you need to have a user account with an associated user login name (also called username) placed under an account hierarchy.","title":"Get an Account"},{"location":"accounts/#conditions-of-acceptance","text":"","title":"Conditions of acceptance"},{"location":"accounts/#acceptable-use-policy-aup","text":"There are a number of policies which apply to ULHPC users. UL HPC Acceptable Use Policy (AUP) [pdf] Important All users of UL HPC resources and PIs must abide by the UL HPC Acceptable Use Policy (AUP) . You should read and keep a signed copy of this document before using the facility. Access and/or usage of any ULHPC system assumes the tacit acknowledgement to this policy. Remember that you are expected to acknowledge ULHPC in your publications . See Acceptable Use Policy for more details. ULHPC Platforms are meant ONLY for R&D! The ULHPC facility is made for Research and Development and it is NOT a full production computing center -- for such needs, consider using the National HPC center . In particular, we cannot make any guarantees of cluster availability or timely job completion even if we target a minimum compute node availability above 95% which is typically met - for instance, past KPI statistics in 2019 report a computing node availability above 97%.","title":"Acceptable Use Policy (AUP)"},{"location":"accounts/#resource-allocation-policies","text":"ULHPC Usage Charging and Resource allocation policy","title":"Resource allocation policies"},{"location":"accounts/#ul-internal-rd-and-training","text":"ULHPC resources are free of charge for UL staff for their internal work and training activities . Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back. Any other activities will be reviewed with the rectorate and are a priori subjected to be billed.","title":"UL internal R&amp;D and training"},{"location":"accounts/#research-projects","text":"","title":"Research Projects"},{"location":"accounts/#externals-and-private-partners","text":"","title":"Externals and private partners"},{"location":"accounts/#how-to-get-an-new-user-account","text":"Account Request Form You can submit a request for a new ULHPC account by using the Create a ULHPC account form form. Enter your personal data, your organization and your institutional contact information. Specify your direct line manager / project PI. Eventually, if you need to access a specific project directory, mention it in comment. This will be granted only after approval of the project PI. Your account will undergo user checks, in accordance with ULHPC policies, to verify your identity and the information proposed. Under some circumstances, there could be a delay while this vetting takes place. After vetting has completed, you will receive a welcome email with your login information, and a unique link to a PrivateBin 1 holding a random temporary password. That link will expire if not used within 24 hours. The PI and PI Proxies for the project will be notified when applicable. Finally, you will need to log into the HPC IPA Portal to set up your initial password and Multi-Factor Authentication (MFA) for your account. Your new password must adhere to ULHPC's password requirements see Password policy and guidelines ULHPC Identity Management (IPA portal) documentation UL HPC \\neq \\neq University credentials Be aware that the source of authentication for the HPC services ( including gitlab.uni.lu ), based on RedHat IdM/IPA DIFFERS from the University credentials (based on UL Active Directory). ULHPC credentials are maintained by the HPC team; associated portal: https://***REMOVED*** authentication service for: UL HPC, Gitlab University credentials are maintained by the IT team of the University authentication service for Service Now and all other UL services","title":"How to Get an New User account?"},{"location":"accounts/#managing-user-accounts","text":"ULHPC user accounts are managed in through the HPC IPA web portal .","title":"Managing User Accounts"},{"location":"accounts/#security-incidents","text":"If you think there has been a computer security incident you should contact the ULHPC Team and the University CISO team as soon as possible: To: hpc-team@uni.lu,niek.nigg@uni.lu Subject: Security Incident for HPC account ' <login> ' ( ADAPT accordingly ) Please save any evidence of the break-in and include as many details as possible in your communication with us.","title":"Security Incidents"},{"location":"accounts/#how-to-get-an-new-project-account","text":"Projects are defined for accounting purposes and are associated to a set of user accounts allowed by the project PI to access its data and submit jobs on behalf of the project account. See Slurm Account Hierarchy . You can request (or be automatically added) to project accounts for accounting purposes. For more information, please see the Project Account documentation","title":"How to Get an New Project account?"},{"location":"accounts/#faq","text":"","title":"FAQ"},{"location":"accounts/#can-i-share-an-account-account-security-policies","text":"Danger The sharing of passwords or login credentials is not allowed under UL HPC and University information security policies. Please bear in mind that this policy also protects the end-user. Sharing credentials removes the ability to audit and accountability for the account holder in case of account misuse. Accounts which are in violation of this policy may be disabled or otherwise limited. Accounts knowingly skirting this policy may be banned. If you find that you need to share resources among multiple individuals, shared projects are just the way to go, and remember that the University extends access to its HPC resources ( i.e. , facility and expert HPC consultants) to the scientific staff of national public organizations and external partners for the duration of joint research projects under the conditions defined above. When in doubt, please contact us and we will be happy to assist you with finding a safe and secure way to do so. PrivateBin is a minimalist, open source online pastebin where the server has zero knowledge of pasted data. Data is encrypted / decrypted in the browser using 256bit AES in Galois Counter mode. \u21a9","title":"Can I share an account? \u2013 Account Security Policies"},{"location":"accounts/collaboration_accounts/","text":"All ULHPC login accounts are associated with specific individuals and must not be shared. In some HPC centers, you may be able to request Collaboration Accounts designed to handle the following use cases: Collaborative Data Management : Large scale experimental and simulation data are typically read or written by multiple collaborators and are kept on disk for long periods. Collaborative Software Management Collaborative Job Management Info By default, we DO NOT provide Collaboration Accounts and encourage the usage of shared research projects <name> stored on the Global project directory to enable the group members to manipulate project data with the appropriate use of unix groups and file permissions. For dedicated job billing and accounting purposes, you should also request the creation of a project account (this will be done for all accepted funded projects). For more details, see Project Accounts documentation . We are aware nevertheless that a problem that often arises is that the files are owned by the collaborator who did the work and if that collaborator changes roles the default unix file permissions usually are such that the files cannot be managed (deleted) by other members of the collaboration and system administrators must be contacted. Similarly, for some use cases, Collaboration Accounts would enable members of the team to manipulate jobs submitted by other team members as necessary. Justified and argued use cases can be submitted to the HPC team to find the appropriate solution by opening a ticket on the HPC Helpdesk Portal .","title":"Collaboration Accounts"},{"location":"accounts/projects/","text":"Projects Accounts \u00b6 Shared project in the Global project directory. \u00b6 We can setup for you a dedicated project directory on the GPFS/SpectrumScale Filesystem for sharing research data with other colleagues. Whether to create a new project directory or to add/remove members to the group set to access the project data, use the Service Now HPC Support Portal . Service Now HPC Support Portal Data Storage Charging \u00b6 Slurm Project Account \u00b6 As explained in the Slurm Account Hierarchy , projects account can be created at the L3 level of the association tree. To quickly list a given project accounts and the users attached to it, you can use the sassoc helper function : # /!\\ ADAPT project acronym/name <name>accordingly sassoc project_<name> Alternatively, you can rely on sacctmgr , typically coupled with the withassoc attribute: # /!\\ ADAPT project acronym/name <name>accordingly sacctmgr show account where name = project_<name> format = \"account%20,user%20,Share,QOS%50\" withassoc As per HPC Resource Allocations for Research Project , creation of such project accounts is mandatory for funded research projects , since usage charging may occur when a detailed reporting will be provided for auditing purposes. With the help of the University Research Support department, we will create automatically project accounts from the list of accepted project which acknowledge the need of computing resources. Feel free nethertheless to use the Service Now HPC Support Portal to request the creation of a new project account or to add/remove members to the group - this might be pertinent for internal research projects or specific collaboration with external partners requiring a separate usage monitoring . Important Project account is a natural way to access the higher priority QOS not granted by default to your personnal account on the ULHPC. For instance, the high QOS is automatically granted as soon as a contribution to the HPC budget line is performed by the project.","title":"Projects Accounts"},{"location":"accounts/projects/#projects-accounts","text":"","title":"Projects Accounts"},{"location":"accounts/projects/#shared-project-in-the-global-project-directory","text":"We can setup for you a dedicated project directory on the GPFS/SpectrumScale Filesystem for sharing research data with other colleagues. Whether to create a new project directory or to add/remove members to the group set to access the project data, use the Service Now HPC Support Portal . Service Now HPC Support Portal","title":"Shared project in the Global project directory."},{"location":"accounts/projects/#data-storage-charging","text":"","title":"Data Storage Charging"},{"location":"accounts/projects/#slurm-project-account","text":"As explained in the Slurm Account Hierarchy , projects account can be created at the L3 level of the association tree. To quickly list a given project accounts and the users attached to it, you can use the sassoc helper function : # /!\\ ADAPT project acronym/name <name>accordingly sassoc project_<name> Alternatively, you can rely on sacctmgr , typically coupled with the withassoc attribute: # /!\\ ADAPT project acronym/name <name>accordingly sacctmgr show account where name = project_<name> format = \"account%20,user%20,Share,QOS%50\" withassoc As per HPC Resource Allocations for Research Project , creation of such project accounts is mandatory for funded research projects , since usage charging may occur when a detailed reporting will be provided for auditing purposes. With the help of the University Research Support department, we will create automatically project accounts from the list of accepted project which acknowledge the need of computing resources. Feel free nethertheless to use the Service Now HPC Support Portal to request the creation of a new project account or to add/remove members to the group - this might be pertinent for internal research projects or specific collaboration with external partners requiring a separate usage monitoring . Important Project account is a natural way to access the higher priority QOS not granted by default to your personnal account on the ULHPC. For instance, the high QOS is automatically granted as soon as a contribution to the HPC budget line is performed by the project.","title":"Slurm Project Account"},{"location":"aws/connection/","text":"Connection to the AWS Cluster \u00b6 Access to the frontend \u00b6 The ULHPC team will create specific access for the AWS Cluster and send to all project members a ssh key in order to connect to the cluster frontend. Once your account has been enabled, you can connect to the cluster using ssh. Computers based on Linux or Mac usually have ssh installed by default. To create a direct connection, use the command below (using your specific cluster name if it differs from workshop-cluster). ssh -i sshkey.pem username@ec2-44-209-227-16.compute-1.amazonaws.com This will open a direct, non-graphical connection in the terminal. To exit the remote terminal session, use the standard Linux command \u201cexit\u201d. Alternatively, you may want to save the configuration of this connection (and create an alias for it) by editing the file ~/.ssh/config (create it if it does not already exist) and adding the following entries: Host aws-ulhpc-access User username Hostname ec2-44-209-227-16.compute-1.amazonaws.com IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes For additionnal information about ssh connection , please refer to the following page . Data storage HOME storage is limited to 500GB for all users. The ULHPC team will also create for you a project directory located at /shared/projects/<project_id> . All members of the project will have the possibility to read, write and execute only in this directory. We strongly advise you to use the project directory to store data and install softwares.","title":"Connect to cluster"},{"location":"aws/connection/#connection-to-the-aws-cluster","text":"","title":"Connection to the AWS Cluster"},{"location":"aws/connection/#access-to-the-frontend","text":"The ULHPC team will create specific access for the AWS Cluster and send to all project members a ssh key in order to connect to the cluster frontend. Once your account has been enabled, you can connect to the cluster using ssh. Computers based on Linux or Mac usually have ssh installed by default. To create a direct connection, use the command below (using your specific cluster name if it differs from workshop-cluster). ssh -i sshkey.pem username@ec2-44-209-227-16.compute-1.amazonaws.com This will open a direct, non-graphical connection in the terminal. To exit the remote terminal session, use the standard Linux command \u201cexit\u201d. Alternatively, you may want to save the configuration of this connection (and create an alias for it) by editing the file ~/.ssh/config (create it if it does not already exist) and adding the following entries: Host aws-ulhpc-access User username Hostname ec2-44-209-227-16.compute-1.amazonaws.com IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes For additionnal information about ssh connection , please refer to the following page . Data storage HOME storage is limited to 500GB for all users. The ULHPC team will also create for you a project directory located at /shared/projects/<project_id> . All members of the project will have the possibility to read, write and execute only in this directory. We strongly advise you to use the project directory to store data and install softwares.","title":"Access to the frontend"},{"location":"aws/overview/","text":"Context & System Overview \u00b6 Context \u00b6 The University of Luxembourg announced a collaboration with Amazon Web Services (AWS) to deploy Amazon Elastic Compute Cloud (Amazon EC2) cloud computing infrastructure in order to accelerate strategic high-performance computing (HPC) research and development in Europe. University of Luxembourg will be among the first European universities to provide research and development communities with access to compute environments that use an architecture similar to the European Processor Initiative (EPI), which will be the basis for Europe\u2019s future exascale computing architecture. Using Amazon EC2 instances powered by AWS Graviton3 , the University of Luxembourg will make simulation capacity available to University researchers. This autumn, research projects will be selected from proposals submitted by University R&D teams. As part of this project, AWS will provide cloud computing services to the University that will support the development, design, and testing of numerical codes (i.e., codes that use only digits, such as binary), which traditionally demands a lot of compute power. This will give researchers an accessible, easy-to-use, end-to-end environment in which they can validate their simulation codes on ARM64 architectures, including servers, personal computers, and Internet of Things (IoT). After initial project selection by a steering committee that includes representatives from the University of Luxembourg and AWS, additional projects will be selected each quarter. Selections will be based on the University\u2019s outlined research goals. Priority will be given to research carried out by the University of Luxembourg and its interdisciplinary research centers; however, based on available capacity and project qualifications, the initiative could extend to European industrial partners. System description and environment \u00b6 The AWS Parallel Cluster based on the new Graviton3 (all instances and storage located in US-EAST-1) will provide cloud computing services to Uni.lu that will support the development, design, and testing of numerical codes, which traditionally demands a lot of compute power. This will give researchers an accessible, easy-to-use, end-to-end environment in which they can validate their simulation codes on ARM64 architectures, including servers, personal computers, and Internet of Things (IoT). The cluster will consist in two main partitions and jobs will be submitted using the Slurm scheduler : PIs and their teams of the funded projects under this call will have the possibility to compile their code with the Arm compiler and using the Arm Performance Library(APL) . Support will be provided by the ULHPC team as well as training activities.","title":"Overview"},{"location":"aws/overview/#context-system-overview","text":"","title":"Context &amp; System Overview"},{"location":"aws/overview/#context","text":"The University of Luxembourg announced a collaboration with Amazon Web Services (AWS) to deploy Amazon Elastic Compute Cloud (Amazon EC2) cloud computing infrastructure in order to accelerate strategic high-performance computing (HPC) research and development in Europe. University of Luxembourg will be among the first European universities to provide research and development communities with access to compute environments that use an architecture similar to the European Processor Initiative (EPI), which will be the basis for Europe\u2019s future exascale computing architecture. Using Amazon EC2 instances powered by AWS Graviton3 , the University of Luxembourg will make simulation capacity available to University researchers. This autumn, research projects will be selected from proposals submitted by University R&D teams. As part of this project, AWS will provide cloud computing services to the University that will support the development, design, and testing of numerical codes (i.e., codes that use only digits, such as binary), which traditionally demands a lot of compute power. This will give researchers an accessible, easy-to-use, end-to-end environment in which they can validate their simulation codes on ARM64 architectures, including servers, personal computers, and Internet of Things (IoT). After initial project selection by a steering committee that includes representatives from the University of Luxembourg and AWS, additional projects will be selected each quarter. Selections will be based on the University\u2019s outlined research goals. Priority will be given to research carried out by the University of Luxembourg and its interdisciplinary research centers; however, based on available capacity and project qualifications, the initiative could extend to European industrial partners.","title":"Context"},{"location":"aws/overview/#system-description-and-environment","text":"The AWS Parallel Cluster based on the new Graviton3 (all instances and storage located in US-EAST-1) will provide cloud computing services to Uni.lu that will support the development, design, and testing of numerical codes, which traditionally demands a lot of compute power. This will give researchers an accessible, easy-to-use, end-to-end environment in which they can validate their simulation codes on ARM64 architectures, including servers, personal computers, and Internet of Things (IoT). The cluster will consist in two main partitions and jobs will be submitted using the Slurm scheduler : PIs and their teams of the funded projects under this call will have the possibility to compile their code with the Arm compiler and using the Arm Performance Library(APL) . Support will be provided by the ULHPC team as well as training activities.","title":"System description and environment"},{"location":"aws/setup/","text":"Environment Setup \u00b6 AWS suggest to use Spack to setup your software environment. There is no hard requirement that you must use Spack. However we have included it here, as it is a quick, simple way to setup a development environment. The official ULHPC swsets are not available on the AWS cluster. If you prefer to use EasyBuild or manually compile softwares, please refer to the ULHPC software documentation for this purpose. Install Spack \u00b6 To do this, please clone the Spack GitHub repository into a SPACK_ROOT which is defined to be on a your project directory, i.e., /shared/project/<project_id> . Then add the configuration to you ~/.bashrc file. You may wish to change the location of the SPACK_ROOT to fit your specific cluster configuration. Here, we consider the release v0.19 of Spack from the releases/v0.19 branch, however, you may wish to checkout the develop branch for the latest packages. git clone -c feature.manyFiles = true -b releases/v0.19 https://github.com/spack/spack $SPACK_ROOT Then, add the following lines in your .bashrc export PROJECT = \"/shared/projects/<project_id>\" export SPACK_ROOT = \" ${ PROJECT } /spack\" if [[ -f \" ${ SPACK_ROOT } /share/spack/setup-env.sh\" && -n ${ SLURM_JOB_ID } ]] ; then source ${ SPACK_ROOT } /share/spack/setup-env.sh \" fi Adapt accordingly Do NOT forget to replace <project_id> with your project name Spack Binary Cache \u00b6 At ISC'22 , in conjunction with the Spack v0.18 release, AWS announced a collaborative effort to host a Binary Cache . The binary cache stores prebuilt versions of common HPC packages, meaning that the installation process is reduced to relocation rather than compilation. To increase flexibility the binary cache contains package builds with different variants and built with different compilers. The purpose of the binary cache is to drastically speed up package installation, especially when long dependency chains exist. The binary cache is periodically updated with the latest versions of packages, and is released in conjunction with Spack releases. Thus you can use the v0.18 binary cache to have packages specifically from that Spack release. Alternatively, you can make use of the develop binary cache, which is kept up to date with the Spack develop branch. To add the develop binary cache, and trusting the associated gpg keys: spack mirror add binary_mirror https://binaries.spack.io/develop spack buildcache keys -it Installing packages \u00b6 The notation for installing packages, when the binary cache has been enabled is unchanged. Spack will first check to see if the package is installable from the binary cache, and only upon failure will it install from source. We see confirmation of this in the output: $ spack install bzip2 == > Installing bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k == > Fetching https://binaries.spack.io/develop/build_cache/linux-amzn2-x86_64_v4-gcc-7.3.1-bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k.spec.json.sig gpg: Signature made Fri 01 Jul 2022 04 :21:22 AM UTC using RSA key ID 3DB0C723 gpg: Good signature from \"Spack Project Official Binaries <maintainers@spack.io>\" == > Fetching https://binaries.spack.io/develop/build_cache/linux-amzn2-x86_64_v4/gcc-7.3.1/bzip2-1.0.8/linux-amzn2-x86_64_v4-gcc-7.3.1-bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k.spack == > Extracting bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k from binary cache [ + ] /shared/spack/opt/spack/linux-amzn2-x86_64_v4/gcc-7.3.1/bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k Bypassing the binary cache \u00b6 Sometimes we might want to install a specific package from source, and bypass the binary cache. To achieve this we can pass the --no-cache flag to the install command. We can use this notation to install cowsay. spack install --no-cache cowsay To compile any software we are going to need a compiler. Out of the box Spack does not know about any compilers on the system. To list your registered compilers, please use the following command: spack compiler list It will return an empty list the first time you used after installing Spack == > No compilers available. Run ` spack compiler find ` to autodetect compilers AWS ParallelCluster installs GCC by default, so you can ask Spack to discover compilers on the system: spack compiler find This should identify your GCC install. In your case a conmpiler should be found. == > Added 1 new compiler to /home/ec2-user/.spack/linux/compilers.yaml gcc@7.3.1 == > Compilers are defined in the following files: /home/ec2-user/.spack/linux/compilers.yaml Install other compilers \u00b6 This default GCC compiler may be sufficient for many applications, we may want to install a newer version of GCC or other compilers in general. Spack is able to install compilers like any other package. Newer GCC version \u00b6 For example we can install a version of GCC 11.2.0, complete with binutils, and then add it to the Spack compiler list. ```\u00b7bash spack install -j [num cores] gcc@11.2.0+binutils spack load gcc@11.2.0 spack compiler find spack unload As Spack is building GCC and all of the dependency packages this install can take a long time (>30 mins). ## Arm Compiler for Linux The Arm Compiler for Linux (ACfL) can be installed by Spack on Arm systems, like the Graviton2 (C6g) or Graviton3 (C7g).o ```bash spack install arm@22.0.1 spack load arm@22.0.1 spack compiler find spack unload Where to build softwares \u00b6 The cluster has quite a small headnode, this means that the compilation of complex software is prohibited. One simple solution is to use the compute nodes to perform the Spack installations, by submitting the command through Slurm. srun -N1 -c 36 spack install -j36 gcc@11.2.0+binutils AWS Environment \u00b6 The versions of these external packages may change and are included for reference. The Cluster comes pre-installed with Slurm , libfabric , PMIx , Intel MPI , and Open MPI . To use these packages, you need to tell spack where to find them. cat << EOF > $SPACK_ROOT/etc/spack/packages.yaml packages: libfabric: variants: fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm externals: - spec: libfabric@1.13.2 fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm prefix: /opt/amazon/efa buildable: False openmpi: variants: fabrics=ofi +legacylaunchers schedulers=slurm ^libfabric externals: - spec: openmpi@4.1.1 %gcc@7.3.1 prefix: /opt/amazon/openmpi pmix: externals: - spec: pmix@3.2.3 ~pmi_backwards_compatibility prefix: /opt/pmix slurm: variants: +pmix sysconfdir=/opt/slurm/etc externals: - spec: slurm@21.08.8-2 +pmix sysconfdir=/opt/slurm/etc prefix: /opt/slurm buildable: False armpl: externals: - spec: armpl@21.0.0%gcc@9.3.0 prefix: /opt/arm/armpl/21.0.0/armpl_21.0_gcc-9.3/ EOF Add the GCC 9.3 Compiler \u00b6 The Graviton image ships with an additional compiler within the ArmPL project. We can add this compiler to the Spack environment with the following command: spack compiler add /opt/arm/armpl/gcc/9.3.0/bin/ Open MPI \u00b6 For Open MPI we have already made the definition to set libfabric as a dependency of Open MPI. So by default it will configure it correctly. spack install openmpi%gcc@11.2.0 Additional resources \u00b6 Job submission relies on the Slurm scheduler. Please refer to the following page for more details. Spack tutorial on AWS ParallelCluster","title":"Environment Setup"},{"location":"aws/setup/#environment-setup","text":"AWS suggest to use Spack to setup your software environment. There is no hard requirement that you must use Spack. However we have included it here, as it is a quick, simple way to setup a development environment. The official ULHPC swsets are not available on the AWS cluster. If you prefer to use EasyBuild or manually compile softwares, please refer to the ULHPC software documentation for this purpose.","title":"Environment Setup"},{"location":"aws/setup/#install-spack","text":"To do this, please clone the Spack GitHub repository into a SPACK_ROOT which is defined to be on a your project directory, i.e., /shared/project/<project_id> . Then add the configuration to you ~/.bashrc file. You may wish to change the location of the SPACK_ROOT to fit your specific cluster configuration. Here, we consider the release v0.19 of Spack from the releases/v0.19 branch, however, you may wish to checkout the develop branch for the latest packages. git clone -c feature.manyFiles = true -b releases/v0.19 https://github.com/spack/spack $SPACK_ROOT Then, add the following lines in your .bashrc export PROJECT = \"/shared/projects/<project_id>\" export SPACK_ROOT = \" ${ PROJECT } /spack\" if [[ -f \" ${ SPACK_ROOT } /share/spack/setup-env.sh\" && -n ${ SLURM_JOB_ID } ]] ; then source ${ SPACK_ROOT } /share/spack/setup-env.sh \" fi Adapt accordingly Do NOT forget to replace <project_id> with your project name","title":"Install Spack"},{"location":"aws/setup/#spack-binary-cache","text":"At ISC'22 , in conjunction with the Spack v0.18 release, AWS announced a collaborative effort to host a Binary Cache . The binary cache stores prebuilt versions of common HPC packages, meaning that the installation process is reduced to relocation rather than compilation. To increase flexibility the binary cache contains package builds with different variants and built with different compilers. The purpose of the binary cache is to drastically speed up package installation, especially when long dependency chains exist. The binary cache is periodically updated with the latest versions of packages, and is released in conjunction with Spack releases. Thus you can use the v0.18 binary cache to have packages specifically from that Spack release. Alternatively, you can make use of the develop binary cache, which is kept up to date with the Spack develop branch. To add the develop binary cache, and trusting the associated gpg keys: spack mirror add binary_mirror https://binaries.spack.io/develop spack buildcache keys -it","title":"Spack Binary Cache"},{"location":"aws/setup/#installing-packages","text":"The notation for installing packages, when the binary cache has been enabled is unchanged. Spack will first check to see if the package is installable from the binary cache, and only upon failure will it install from source. We see confirmation of this in the output: $ spack install bzip2 == > Installing bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k == > Fetching https://binaries.spack.io/develop/build_cache/linux-amzn2-x86_64_v4-gcc-7.3.1-bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k.spec.json.sig gpg: Signature made Fri 01 Jul 2022 04 :21:22 AM UTC using RSA key ID 3DB0C723 gpg: Good signature from \"Spack Project Official Binaries <maintainers@spack.io>\" == > Fetching https://binaries.spack.io/develop/build_cache/linux-amzn2-x86_64_v4/gcc-7.3.1/bzip2-1.0.8/linux-amzn2-x86_64_v4-gcc-7.3.1-bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k.spack == > Extracting bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k from binary cache [ + ] /shared/spack/opt/spack/linux-amzn2-x86_64_v4/gcc-7.3.1/bzip2-1.0.8-paghlsmxrq7p26qna6ml6au4fj2bdw6k","title":"Installing packages"},{"location":"aws/setup/#bypassing-the-binary-cache","text":"Sometimes we might want to install a specific package from source, and bypass the binary cache. To achieve this we can pass the --no-cache flag to the install command. We can use this notation to install cowsay. spack install --no-cache cowsay To compile any software we are going to need a compiler. Out of the box Spack does not know about any compilers on the system. To list your registered compilers, please use the following command: spack compiler list It will return an empty list the first time you used after installing Spack == > No compilers available. Run ` spack compiler find ` to autodetect compilers AWS ParallelCluster installs GCC by default, so you can ask Spack to discover compilers on the system: spack compiler find This should identify your GCC install. In your case a conmpiler should be found. == > Added 1 new compiler to /home/ec2-user/.spack/linux/compilers.yaml gcc@7.3.1 == > Compilers are defined in the following files: /home/ec2-user/.spack/linux/compilers.yaml","title":"Bypassing the binary cache"},{"location":"aws/setup/#install-other-compilers","text":"This default GCC compiler may be sufficient for many applications, we may want to install a newer version of GCC or other compilers in general. Spack is able to install compilers like any other package.","title":"Install other compilers"},{"location":"aws/setup/#newer-gcc-version","text":"For example we can install a version of GCC 11.2.0, complete with binutils, and then add it to the Spack compiler list. ```\u00b7bash spack install -j [num cores] gcc@11.2.0+binutils spack load gcc@11.2.0 spack compiler find spack unload As Spack is building GCC and all of the dependency packages this install can take a long time (>30 mins). ## Arm Compiler for Linux The Arm Compiler for Linux (ACfL) can be installed by Spack on Arm systems, like the Graviton2 (C6g) or Graviton3 (C7g).o ```bash spack install arm@22.0.1 spack load arm@22.0.1 spack compiler find spack unload","title":"Newer GCC version"},{"location":"aws/setup/#where-to-build-softwares","text":"The cluster has quite a small headnode, this means that the compilation of complex software is prohibited. One simple solution is to use the compute nodes to perform the Spack installations, by submitting the command through Slurm. srun -N1 -c 36 spack install -j36 gcc@11.2.0+binutils","title":"Where to build softwares"},{"location":"aws/setup/#aws-environment","text":"The versions of these external packages may change and are included for reference. The Cluster comes pre-installed with Slurm , libfabric , PMIx , Intel MPI , and Open MPI . To use these packages, you need to tell spack where to find them. cat << EOF > $SPACK_ROOT/etc/spack/packages.yaml packages: libfabric: variants: fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm externals: - spec: libfabric@1.13.2 fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm prefix: /opt/amazon/efa buildable: False openmpi: variants: fabrics=ofi +legacylaunchers schedulers=slurm ^libfabric externals: - spec: openmpi@4.1.1 %gcc@7.3.1 prefix: /opt/amazon/openmpi pmix: externals: - spec: pmix@3.2.3 ~pmi_backwards_compatibility prefix: /opt/pmix slurm: variants: +pmix sysconfdir=/opt/slurm/etc externals: - spec: slurm@21.08.8-2 +pmix sysconfdir=/opt/slurm/etc prefix: /opt/slurm buildable: False armpl: externals: - spec: armpl@21.0.0%gcc@9.3.0 prefix: /opt/arm/armpl/21.0.0/armpl_21.0_gcc-9.3/ EOF","title":"AWS Environment"},{"location":"aws/setup/#add-the-gcc-93-compiler","text":"The Graviton image ships with an additional compiler within the ArmPL project. We can add this compiler to the Spack environment with the following command: spack compiler add /opt/arm/armpl/gcc/9.3.0/bin/","title":"Add the GCC 9.3 Compiler"},{"location":"aws/setup/#open-mpi","text":"For Open MPI we have already made the definition to set libfabric as a dependency of Open MPI. So by default it will configure it correctly. spack install openmpi%gcc@11.2.0","title":"Open MPI"},{"location":"aws/setup/#additional-resources","text":"Job submission relies on the Slurm scheduler. Please refer to the following page for more details. Spack tutorial on AWS ParallelCluster","title":"Additional resources"},{"location":"connect/access/","text":"Login Nodes \u00b6 Opening an SSH connection to ULHPC systems results in a connection to an access node. Iris ssh iris-cluster Aion ssh aion-cluster Iris (X11) To be able to further run GUI applications within your [interactive] jobs: ssh -X iris-cluster # OR on Mac OS: ssh -Y iris-cluster Aion (X11) To be able to further run GUI applications within your [interactive] jobs: ssh -X aion-cluster # OR on Mac OS: ssh -Y aion-cluster Important Recall that you SHOULD NOT run any HPC application on the login nodes. That's why the module command is NOT available on them. Usage \u00b6 On access nodes, typical user tasks include Transferring and managing files Editing files Submitting jobs Appropriate Use Do not run compute- or memory-intensive applications on access nodes. These nodes are a shared resource. ULHPC admins may terminate processes which are having negative impacts on other users or the systems. Avoid watch If you must use the watch command, please use a much longer interval such as 5 minutes (=300 sec), e.g., watch -n 300 <your_command> . Tips \u00b6 ULHPC provides a wide variety of qos's An interactive qos is available on Iris and Aion for compute- and memory-intensive interactive work. Please, use an interactive job for resource-intensive processes instead of running them on access nodes. Tip To help identify processes that make heavy use of resources, you can use: top -u $USER /usr/bin/time -v ./my_command Running GUI Application over X11 If you intend to run GUI applications (MATLAB, Stata, ParaView etc.), you MUST connect by SSH to the login nodes with the -X (or -Y on Mac OS) option: Iris ssh -X iris-cluster # OR on Mac OS: ssh -Y iris-cluster Aion ssh -X aion-cluster # OR on Mac OS: ssh -Y aion-cluster","title":"Access/Login Servers"},{"location":"connect/access/#login-nodes","text":"Opening an SSH connection to ULHPC systems results in a connection to an access node. Iris ssh iris-cluster Aion ssh aion-cluster Iris (X11) To be able to further run GUI applications within your [interactive] jobs: ssh -X iris-cluster # OR on Mac OS: ssh -Y iris-cluster Aion (X11) To be able to further run GUI applications within your [interactive] jobs: ssh -X aion-cluster # OR on Mac OS: ssh -Y aion-cluster Important Recall that you SHOULD NOT run any HPC application on the login nodes. That's why the module command is NOT available on them.","title":"Login Nodes"},{"location":"connect/access/#usage","text":"On access nodes, typical user tasks include Transferring and managing files Editing files Submitting jobs Appropriate Use Do not run compute- or memory-intensive applications on access nodes. These nodes are a shared resource. ULHPC admins may terminate processes which are having negative impacts on other users or the systems. Avoid watch If you must use the watch command, please use a much longer interval such as 5 minutes (=300 sec), e.g., watch -n 300 <your_command> .","title":"Usage"},{"location":"connect/access/#tips","text":"ULHPC provides a wide variety of qos's An interactive qos is available on Iris and Aion for compute- and memory-intensive interactive work. Please, use an interactive job for resource-intensive processes instead of running them on access nodes. Tip To help identify processes that make heavy use of resources, you can use: top -u $USER /usr/bin/time -v ./my_command Running GUI Application over X11 If you intend to run GUI applications (MATLAB, Stata, ParaView etc.), you MUST connect by SSH to the login nodes with the -X (or -Y on Mac OS) option: Iris ssh -X iris-cluster # OR on Mac OS: ssh -Y iris-cluster Aion ssh -X aion-cluster # OR on Mac OS: ssh -Y aion-cluster","title":"Tips"},{"location":"connect/ipa/","text":"ULHPC Identity Management Portal (IdM/IPA) \u00b6 Reference Redhat 7 Documentation Red Hat Identity Management (IdM), formally referred to as IPA (\"Identity, Policy, and Audit\" -- see also https://www.freeipa.org ), provides a centralized and unified way to manage identity stores, authentication, policies, and authorization policies in a Linux-based domain. IdM significantly reduces the administrative overhead of managing different services individually and using different tools on different machines. All services (HPC and complementary ones) managed by the ULHPC team rely on a highly redundant setup involving several Redhat IdM/IPA server. SSH Key Management You are responsible for uploading and managing your authorized public SSH keys for your account, under the terms of the Acceptable Use Policy . Be aware that the ULHPC team review on a periodical basis the compliance to the policy, as well as the security of your keys. See also the note on deprecated/weak DSA/RSA keys Upload your SSH key on the ULHPC Identity Management Portal \u00b6 You should upload your public SSH key(s) *.pub to your user entry on the ULHPC Identity Management Portal. For that, connect to the ULHPC IdM portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail) and enter your ULHPC credentials. First copy the content of the key you want to add # Example with ED25519 **public** key ( laptop ) $> cat ~/.ssh/id_ed25519.pub ssh-ed25519 AAAA [ ... ] # OR the RSA **public** key ( laptop ) $> cat ~/.ssh/id_rsa.pub ssh-rsa AAAA [ ... ] Then on the portal: Select Identity / Users. Select your login entry Under the Settings tab in the Account Settings area, click SSH public keys: Add . Paste in the Base 64-encoded public key string, and click Set . Click Save at the top of the page. Your key fingerprint should be listed now. Listing SSH keys attached to your account through SSSD SSSD is a system daemon used on ULHPC computational resources. Its primary function is to provide access to local or remote identity and authentication resources through a common framework that can provide caching and offline support to the system. To easily access the authorized keys configured for your account from the command-line (i.e. without login on the ULHPC IPA portal), you can use: sss_ssh_authorizedkeys $(whoami) Change Your Password \u00b6 connect to the ULHPC IdM portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail) and enter your ULHPC credentials. On the top right under your name, select the entry \"Change Password\" In the dialog window that appears, enter the current password, and your new password. Your password should meet the password requirements explained in the next section below, and must be 'safe' or 'very safe' according to the provided password strength meter.","title":"Identity Management (IdM/IPA)"},{"location":"connect/ipa/#ulhpc-identity-management-portal-idmipa","text":"Reference Redhat 7 Documentation Red Hat Identity Management (IdM), formally referred to as IPA (\"Identity, Policy, and Audit\" -- see also https://www.freeipa.org ), provides a centralized and unified way to manage identity stores, authentication, policies, and authorization policies in a Linux-based domain. IdM significantly reduces the administrative overhead of managing different services individually and using different tools on different machines. All services (HPC and complementary ones) managed by the ULHPC team rely on a highly redundant setup involving several Redhat IdM/IPA server. SSH Key Management You are responsible for uploading and managing your authorized public SSH keys for your account, under the terms of the Acceptable Use Policy . Be aware that the ULHPC team review on a periodical basis the compliance to the policy, as well as the security of your keys. See also the note on deprecated/weak DSA/RSA keys","title":"ULHPC Identity Management Portal (IdM/IPA)"},{"location":"connect/ipa/#upload-your-ssh-key-on-the-ulhpc-identity-management-portal","text":"You should upload your public SSH key(s) *.pub to your user entry on the ULHPC Identity Management Portal. For that, connect to the ULHPC IdM portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail) and enter your ULHPC credentials. First copy the content of the key you want to add # Example with ED25519 **public** key ( laptop ) $> cat ~/.ssh/id_ed25519.pub ssh-ed25519 AAAA [ ... ] # OR the RSA **public** key ( laptop ) $> cat ~/.ssh/id_rsa.pub ssh-rsa AAAA [ ... ] Then on the portal: Select Identity / Users. Select your login entry Under the Settings tab in the Account Settings area, click SSH public keys: Add . Paste in the Base 64-encoded public key string, and click Set . Click Save at the top of the page. Your key fingerprint should be listed now. Listing SSH keys attached to your account through SSSD SSSD is a system daemon used on ULHPC computational resources. Its primary function is to provide access to local or remote identity and authentication resources through a common framework that can provide caching and offline support to the system. To easily access the authorized keys configured for your account from the command-line (i.e. without login on the ULHPC IPA portal), you can use: sss_ssh_authorizedkeys $(whoami)","title":"Upload your SSH key on the ULHPC Identity Management Portal"},{"location":"connect/ipa/#change-your-password","text":"connect to the ULHPC IdM portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail) and enter your ULHPC credentials. On the top right under your name, select the entry \"Change Password\" In the dialog window that appears, enter the current password, and your new password. Your password should meet the password requirements explained in the next section below, and must be 'safe' or 'very safe' according to the provided password strength meter.","title":"Change Your Password"},{"location":"connect/linux/","text":"Installation notes \u00b6 Normally, SSH is installed natively on your machine and the ssh command should be accessible from the command line (or a Terminal) through the ssh command: (your_workstation)$> ssh -V OpenSSH_8.4p1, OpenSSL 1.1.1h 22 Sep 2020 If that's not the case, consider installing the package openssh-client (Debian-like systems) or ssh (Redhat-like systems). Your local SSH configuration is located in the ~/.ssh/ directory and consists of: ~/.ssh/id_rsa.pub : your SSH public key. This one is the only one SAFE to distribute. ~/.ssh/id_rsa : the associated private key. NEVER EVER TRANSMIT THIS FILE (eventually) the configuration of the SSH client ~/.ssh/config ~/.ssh/known_hosts : Contains a list of host keys for all hosts you have logged into that are not already in the system-wide list of known host keys. This permits to detect man-in-the-middle attacks. SSH Key Management \u00b6 To generate an SSH keys, just use the ssh-keygen command, typically as follows: (your_workstation)$> ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/home/user/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/user/.ssh/id_rsa. Your public key has been saved in /home/user/.ssh/id_rsa.pub. The key fingerprint is: fe:e8:26:df:38:49:3a:99:d7:85:4e:c3:85:c8:24:5b username@yourworkstation The key's randomart image is: +---[RSA 4096]----+ | | | . E | | * . . | | . o . . | | S. o | | .. = . | | =.= o | | * ==o | | B=.o | +-----------------+ Warning To ensure the security of the platform and your data stored on it, you must protect your SSH keys with a passphrase! Additionally, your private key and passphrase should never be transmitted to anybody. After the execution of ssh-keygen command, the keys are generated and stored in the following files: SSH RSA Private key: ~/.ssh/id_rsa . Again, NEVER EVER TRANSMIT THIS FILE SSH RSA Public key: ~/.ssh/id_rsa.pub . This file is the ONLY one SAFE to distribute Ensure the access rights are correct on the generated keys using the ' ls -l ' command. The private key should be readable only by you: (your_workstation)$> ls -l ~/.ssh/id_* -rw------- 1 git git 751 Mar 1 20:16 /home/username/.ssh/id_rsa -rw-r--r-- 1 git git 603 Mar 1 20:16 /home/username/.ssh/id_rsa.pub Configuration \u00b6 In order to be able to login to the clusters, you will have to add this public key (i.e. id_rsa.pub ) into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail). The port on which the SSH servers are listening is not the default one ( i.e. 22) but 8022 . Consequently, if you want to connect to the Iris cluster, open a terminal and run (substituting yourlogin with the login name you received from us): (your_workstation)$> ssh -p 8022 yourlogin@access-iris.uni.lu For the Aion cluster, the access server host name is access-aion.uni.lu : (your_workstation)$> ssh -p 8022 yourlogin@access-aion.uni.lu Alternatively, you may want to save the configuration of this connection (and create an alias for it) by editing the file ~/.ssh/config (create it if it does not already exist) and adding the following entries: Host iris-cluster Hostname access-iris.uni.lu Host aion-cluster Hostname access-aion.uni.lu Host *-cluster User yourlogin Port 8022 ForwardAgent no Now you'll be able to issue the following (simpler) command to connect to the cluster and obtain the welcome banner: (your_workstation)$> ssh iris-cluster ================================================================================== Welcome to access2.iris-cluster.uni.lux ================================================================================== _ ____ / \\ ___ ___ ___ ___ ___|___ \\ / _ \\ / __/ __/ _ \\/ __/ __| __) | / ___ \\ (_| (_| __/\\__ \\__ \\/ __/ /_/ \\_\\___\\___\\___||___/___/_____| _____ _ ____ _ _ __ / /_ _|_ __(_)___ / ___| |_ _ ___| |_ ___ _ _\\ \\ | | | || '__| / __| | | | | | | / __| __/ _ \\ '__| | | | | || | | \\__ \\ | |___| | |_| \\__ \\ || __/ | | | | ||___|_| |_|___/ \\____|_|\\__,_|___/\\__\\___|_| | | \\_\\ /_/ ================================================================================== === Computing Nodes ========================================= #RAM/n === #Cores == iris-[001-108] 108 Dell C6320 (2 Xeon E5-2680v4@2.4GHz [14c/120W]) 128GB 3024 iris-[109-168] 60 Dell C6420 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 128GB 1680 iris-[169-186] 18 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 504 +72 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 16GB +368640 iris-[187-190] 4 Dell R840 (4 Xeon Platin.8180M@2.5GHz [28c/205W]) 3TB 448 iris-[191-196] 6 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 168 +24 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 32GB +122880 ================================================================================== *** TOTAL: 196 nodes, 5824 cores + 491520 CUDA cores + 61440 Tensor cores *** Fast interconnect using InfiniBand EDR 100 Gb/s technology Shared Storage (raw capacity): 2180 TB (GPFS) + 1300 TB (Lustre) = 3480 TB Support (in this order!) Platform notifications - User DOC ........ https://hpc.uni.lu/docs - Twitter: @ULHPC - FAQ ............. https://hpc.uni.lu/faq - Mailing-list .... hpc-users@uni.lu - Bug reports .NEW. https://hpc.uni.lu/support (Service Now) - Admins .......... hpc-team@uni.lu (OPEN TICKETS) ================================================================================== /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND ! First reserve your nodes (using srun/sbatch(1)) Linux access2.iris-cluster.uni.lux 3.10.0-957.21.3.el7.x86_64 x86_64 15:51:56 up 6 days, 2:32, 39 users, load average: 0.59, 0.68, 0.54 [yourlogin@access2 ~]$ Activate the SSH agent \u00b6 To be able to use your SSH key in a public-key authentication scheme, it must be loaded by an SSH agent . Mac OS X (>= 10.5) , this will be handled automatically; you will be asked to fill in the passphrase on the first connection. Linux , this will be handled automatically; you will be asked to fill the passphrase on the first connection. However if you get a message similar to the following: (your_workstation)$> ssh -vv iris-cluster [...] Agent admitted failure to sign using the key. Permission denied (publickey). This means that you have to manually load your key in the SSH agent by running: $> ssh-add ~/.ssh/id_rsa SSH Resources \u00b6 Mac OS X : Cyberduck is a free Cocoa FTP and SFTP client. Linux : OpenSSH is available in every good linux distro, and every *BSD, and Mac OS X. SSH Advanced Tips \u00b6 Bash completion : The bash-completion package eases the ssh command usage by providing completion for hostnames and more (assuming you set the directive HashKnownHost to no in your ~/etc/ssh_config ) Forwarding a local port : You can forward a local port to a host behind a firewall. This is useful if you run a server on one of the cluster nodes (let's say listening on port 2222) and you want to access it via the local port 1111 on your machine. Then you'll run: (your_workstation)$> ssh iris-cluster -L 1111:iris-014:2222 Forwarding a remote port : You can forward a remote port back to a host protected by your firewall. Tunnelling for others : By using the -g parameter, you allow connections from other hosts than localhost to use your SSH tunnels. Be warned that anybody within your network may access the tunnelized host this way, which may be a security issue. Using OpenSSH SOCKS proxy feature (with Firefox for instance) : the OpenSSH ssh client also embeds a SOCKS proxy. You may activate it by using the -D parameter and a value for a port ( e.g. 3128), then configuring your application (Firefox for instance) to use localhost:port (i.e. \"localhost:3128\") as a SOCKS proxy. The FoxyProxy module is typically useful for that. One very nice feature of FoxyProxy is that you can use the host resolution on the remote server. This permits you to access your local machine within the university for instance with the same name you would use within the UL network. To summarize, that's better than the VPN proxy ;) Once you setup a SSH SOCKS proxy, you can also use tsocks , a Shell wrapper to simplify the use of the tsocks(8) library to transparently allow an application (not aware of SOCKS) to transparently use a SOCKS proxy. For instance, assuming you create a VNC server on a given remote server as follows: (remote_server)$> vncserver -geometry 1366x768 New 'X' desktop is remote_server:1 Starting applications specified in /home/username/.vnc/xstartup Log file is /home/username/.vnc/remote_server:1.log Then you can make the VNC client on your workstation use this tunnel to access the VNS server as follows: (your_workstation)$> tsocks vncviewer <IP_of_remote_server>:1 Escape character : use ~. to disconnect, even if your remote command hangs.","title":"Installation notes"},{"location":"connect/linux/#installation-notes","text":"Normally, SSH is installed natively on your machine and the ssh command should be accessible from the command line (or a Terminal) through the ssh command: (your_workstation)$> ssh -V OpenSSH_8.4p1, OpenSSL 1.1.1h 22 Sep 2020 If that's not the case, consider installing the package openssh-client (Debian-like systems) or ssh (Redhat-like systems). Your local SSH configuration is located in the ~/.ssh/ directory and consists of: ~/.ssh/id_rsa.pub : your SSH public key. This one is the only one SAFE to distribute. ~/.ssh/id_rsa : the associated private key. NEVER EVER TRANSMIT THIS FILE (eventually) the configuration of the SSH client ~/.ssh/config ~/.ssh/known_hosts : Contains a list of host keys for all hosts you have logged into that are not already in the system-wide list of known host keys. This permits to detect man-in-the-middle attacks.","title":"Installation notes"},{"location":"connect/linux/#ssh-key-management","text":"To generate an SSH keys, just use the ssh-keygen command, typically as follows: (your_workstation)$> ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/home/user/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/user/.ssh/id_rsa. Your public key has been saved in /home/user/.ssh/id_rsa.pub. The key fingerprint is: fe:e8:26:df:38:49:3a:99:d7:85:4e:c3:85:c8:24:5b username@yourworkstation The key's randomart image is: +---[RSA 4096]----+ | | | . E | | * . . | | . o . . | | S. o | | .. = . | | =.= o | | * ==o | | B=.o | +-----------------+ Warning To ensure the security of the platform and your data stored on it, you must protect your SSH keys with a passphrase! Additionally, your private key and passphrase should never be transmitted to anybody. After the execution of ssh-keygen command, the keys are generated and stored in the following files: SSH RSA Private key: ~/.ssh/id_rsa . Again, NEVER EVER TRANSMIT THIS FILE SSH RSA Public key: ~/.ssh/id_rsa.pub . This file is the ONLY one SAFE to distribute Ensure the access rights are correct on the generated keys using the ' ls -l ' command. The private key should be readable only by you: (your_workstation)$> ls -l ~/.ssh/id_* -rw------- 1 git git 751 Mar 1 20:16 /home/username/.ssh/id_rsa -rw-r--r-- 1 git git 603 Mar 1 20:16 /home/username/.ssh/id_rsa.pub","title":"SSH Key Management"},{"location":"connect/linux/#configuration","text":"In order to be able to login to the clusters, you will have to add this public key (i.e. id_rsa.pub ) into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail). The port on which the SSH servers are listening is not the default one ( i.e. 22) but 8022 . Consequently, if you want to connect to the Iris cluster, open a terminal and run (substituting yourlogin with the login name you received from us): (your_workstation)$> ssh -p 8022 yourlogin@access-iris.uni.lu For the Aion cluster, the access server host name is access-aion.uni.lu : (your_workstation)$> ssh -p 8022 yourlogin@access-aion.uni.lu Alternatively, you may want to save the configuration of this connection (and create an alias for it) by editing the file ~/.ssh/config (create it if it does not already exist) and adding the following entries: Host iris-cluster Hostname access-iris.uni.lu Host aion-cluster Hostname access-aion.uni.lu Host *-cluster User yourlogin Port 8022 ForwardAgent no Now you'll be able to issue the following (simpler) command to connect to the cluster and obtain the welcome banner: (your_workstation)$> ssh iris-cluster ================================================================================== Welcome to access2.iris-cluster.uni.lux ================================================================================== _ ____ / \\ ___ ___ ___ ___ ___|___ \\ / _ \\ / __/ __/ _ \\/ __/ __| __) | / ___ \\ (_| (_| __/\\__ \\__ \\/ __/ /_/ \\_\\___\\___\\___||___/___/_____| _____ _ ____ _ _ __ / /_ _|_ __(_)___ / ___| |_ _ ___| |_ ___ _ _\\ \\ | | | || '__| / __| | | | | | | / __| __/ _ \\ '__| | | | | || | | \\__ \\ | |___| | |_| \\__ \\ || __/ | | | | ||___|_| |_|___/ \\____|_|\\__,_|___/\\__\\___|_| | | \\_\\ /_/ ================================================================================== === Computing Nodes ========================================= #RAM/n === #Cores == iris-[001-108] 108 Dell C6320 (2 Xeon E5-2680v4@2.4GHz [14c/120W]) 128GB 3024 iris-[109-168] 60 Dell C6420 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 128GB 1680 iris-[169-186] 18 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 504 +72 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 16GB +368640 iris-[187-190] 4 Dell R840 (4 Xeon Platin.8180M@2.5GHz [28c/205W]) 3TB 448 iris-[191-196] 6 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 168 +24 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 32GB +122880 ================================================================================== *** TOTAL: 196 nodes, 5824 cores + 491520 CUDA cores + 61440 Tensor cores *** Fast interconnect using InfiniBand EDR 100 Gb/s technology Shared Storage (raw capacity): 2180 TB (GPFS) + 1300 TB (Lustre) = 3480 TB Support (in this order!) Platform notifications - User DOC ........ https://hpc.uni.lu/docs - Twitter: @ULHPC - FAQ ............. https://hpc.uni.lu/faq - Mailing-list .... hpc-users@uni.lu - Bug reports .NEW. https://hpc.uni.lu/support (Service Now) - Admins .......... hpc-team@uni.lu (OPEN TICKETS) ================================================================================== /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND ! First reserve your nodes (using srun/sbatch(1)) Linux access2.iris-cluster.uni.lux 3.10.0-957.21.3.el7.x86_64 x86_64 15:51:56 up 6 days, 2:32, 39 users, load average: 0.59, 0.68, 0.54 [yourlogin@access2 ~]$","title":"Configuration"},{"location":"connect/linux/#activate-the-ssh-agent","text":"To be able to use your SSH key in a public-key authentication scheme, it must be loaded by an SSH agent . Mac OS X (>= 10.5) , this will be handled automatically; you will be asked to fill in the passphrase on the first connection. Linux , this will be handled automatically; you will be asked to fill the passphrase on the first connection. However if you get a message similar to the following: (your_workstation)$> ssh -vv iris-cluster [...] Agent admitted failure to sign using the key. Permission denied (publickey). This means that you have to manually load your key in the SSH agent by running: $> ssh-add ~/.ssh/id_rsa","title":"Activate the SSH agent"},{"location":"connect/linux/#ssh-resources","text":"Mac OS X : Cyberduck is a free Cocoa FTP and SFTP client. Linux : OpenSSH is available in every good linux distro, and every *BSD, and Mac OS X.","title":"SSH Resources"},{"location":"connect/linux/#ssh-advanced-tips","text":"Bash completion : The bash-completion package eases the ssh command usage by providing completion for hostnames and more (assuming you set the directive HashKnownHost to no in your ~/etc/ssh_config ) Forwarding a local port : You can forward a local port to a host behind a firewall. This is useful if you run a server on one of the cluster nodes (let's say listening on port 2222) and you want to access it via the local port 1111 on your machine. Then you'll run: (your_workstation)$> ssh iris-cluster -L 1111:iris-014:2222 Forwarding a remote port : You can forward a remote port back to a host protected by your firewall. Tunnelling for others : By using the -g parameter, you allow connections from other hosts than localhost to use your SSH tunnels. Be warned that anybody within your network may access the tunnelized host this way, which may be a security issue. Using OpenSSH SOCKS proxy feature (with Firefox for instance) : the OpenSSH ssh client also embeds a SOCKS proxy. You may activate it by using the -D parameter and a value for a port ( e.g. 3128), then configuring your application (Firefox for instance) to use localhost:port (i.e. \"localhost:3128\") as a SOCKS proxy. The FoxyProxy module is typically useful for that. One very nice feature of FoxyProxy is that you can use the host resolution on the remote server. This permits you to access your local machine within the university for instance with the same name you would use within the UL network. To summarize, that's better than the VPN proxy ;) Once you setup a SSH SOCKS proxy, you can also use tsocks , a Shell wrapper to simplify the use of the tsocks(8) library to transparently allow an application (not aware of SOCKS) to transparently use a SOCKS proxy. For instance, assuming you create a VNC server on a given remote server as follows: (remote_server)$> vncserver -geometry 1366x768 New 'X' desktop is remote_server:1 Starting applications specified in /home/username/.vnc/xstartup Log file is /home/username/.vnc/remote_server:1.log Then you can make the VNC client on your workstation use this tunnel to access the VNS server as follows: (your_workstation)$> tsocks vncviewer <IP_of_remote_server>:1 Escape character : use ~. to disconnect, even if your remote command hangs.","title":"SSH Advanced Tips"},{"location":"connect/ood/","text":"ULHPC Open On Demand (OOD) Portal \u00b6 Open OnDemand (OOD) is a Web portal compatible with Windows, Linux and MacOS. You should login with your ULHPC credential using the URL communicated to you by the UL HPC team. OOD provides a convenient web access to the HPC resources and integrates a file management system a job management system (job composer, monitoring your submitted jobs, ...) an interactive command-line shell access interactive apps with graphical desktop environments ULHPC OOD Portal limitations The ULHPC OOD portal is NOT accessible outside the UniLu network. If you want to use it, you will need to setup a VPN to access the UniLu network Note : The portal is in _still under active development state: missing features and bugs can be reported to the ULHPC team via the support portal Live tests and demo are proposed during the ULHPC Tutorial: Preliminaries / OOD . Below are illustrations of OOD capabilities on the ULHPC facility. File management \u00b6 Job composer and Job List \u00b6 Shell access \u00b6 Interactive sessions \u00b6 Graphical Desktop Environment \u00b6","title":"Open On Demand Portal"},{"location":"connect/ood/#ulhpc-open-on-demand-ood-portal","text":"Open OnDemand (OOD) is a Web portal compatible with Windows, Linux and MacOS. You should login with your ULHPC credential using the URL communicated to you by the UL HPC team. OOD provides a convenient web access to the HPC resources and integrates a file management system a job management system (job composer, monitoring your submitted jobs, ...) an interactive command-line shell access interactive apps with graphical desktop environments ULHPC OOD Portal limitations The ULHPC OOD portal is NOT accessible outside the UniLu network. If you want to use it, you will need to setup a VPN to access the UniLu network Note : The portal is in _still under active development state: missing features and bugs can be reported to the ULHPC team via the support portal Live tests and demo are proposed during the ULHPC Tutorial: Preliminaries / OOD . Below are illustrations of OOD capabilities on the ULHPC facility.","title":"ULHPC Open On Demand (OOD) Portal"},{"location":"connect/ood/#file-management","text":"","title":"File management"},{"location":"connect/ood/#job-composer-and-job-list","text":"","title":"Job composer and Job List"},{"location":"connect/ood/#shell-access","text":"","title":"Shell access"},{"location":"connect/ood/#interactive-sessions","text":"","title":"Interactive sessions"},{"location":"connect/ood/#graphical-desktop-environment","text":"","title":"Graphical Desktop Environment"},{"location":"connect/ssh/","text":"SSH \u00b6 All ULHPC servers are reached using either the Secure Shell (SSH) communication and encryption protocol (version 2). Developed by SSH Communications Security Ltd. , Secure Shell is a an encrypted network protocol used to log into another computer over an unsecured network, to execute commands in a remote machine, and to move files from one machine to another in a secure way. On UNIX/LINUX/BSD type systems, SSH is also the name of a suite of software applications for connecting via the SSH protocol. The SSH applications can execute commands on a remote machine and transfer files from one machine to another. All communications are automatically and transparently encrypted, including passwords. Most versions of SSH provide login ( ssh , slogin ), a remote copy operation ( scp ), and many also provide a secure ftp client ( sftp ). Additionally, SSH allows secure X Window connections. To use SSH, you have to generate a pair of keys, one public and the other private . The public key authentication is the most secure and flexible approach to ensure a multi-purpose transparent connection to a remote server. This approach is enforced on the ULHPC platforms and assumes that the public key is known by the system in order to perform an authentication based on a challenge/response protocol instead of the classical password-based protocol. The way SSH handles the keys and the configuration files is illustrated in the following figure: Installation \u00b6 OpenSSH is natively supported on Linux / Mac OS / Unix / WSL (see below) On Windows, you are thus encouraged to install Windows Subsystem for Linux (WSL) and setup an Ubuntu subsystem from Microsoft Store . You probably want to also install Windows Terminal and MobaXterm Better performances of your Linux subsystem can be obtained by migrating to WSL 2 Follow the ULHPC Tutorial: Setup Pre-Requisites / Windows for detailed instructions. SSH Key Generation \u00b6 To generate an RSA SSH keys of 4096-bit length , just use the ssh-keygen command as follows : ssh-keygen -t rsa -b 4096 -o -a 100 After the execution of this command, the generated keys are stored in the following files: SSH RSA Private key: ~/.ssh/id_rsa . NEVER EVER TRANSMIT THIS FILE SSH RSA Public key: ~/.ssh/id_rsa.pub . This file is the ONLY one SAFE to distribute To passphrase or not to passphrase To ensure the security of your SSH key-pair on your laptop, you MUST protect your SSH keys with a passphrase! Note however that while possible, this passphrase is purely private and has a priori nothing to do with your University or your ULHPC credentials. Nevertheless, a strong passphrase follows the same recommendations as for strong passwords (for instance: see password requirements and guidelines . Finally, just like encryption keys, passphrases need to be kept safe and protected from unauthorised access. A Password Manager can help you to store all your passwords safely. The University is currently not offering a university wide password manger but there are many free and paid ones you can use, for example: KeePassX , PWSafe , Dashlane , 1Password or LastPass . You may want to generate also ED25519 Key Pairs (which is the most recommended public-key algorithm available today) -- see explaination ssh-keygen -t ed25519 -o -a 100 Your key pairs will be located under ~/.ssh/ and follow the following format -- the .pub extension indicated the public key part and is the ONLY one SAFE to distribute : $ ls -l ~/.ssh/id_* -rw------- username groupname ~/.ssh/id_rsa -rw-r--r-- username groupname ~/.ssh/id_rsa.pub # Public RSA key -rw------- username groupname ~/.ssh/id_ed25519 -rw-r--r-- username groupname ~/.ssh/id_ed25519.pub # Public ED25519 key Ensure the access rights are correct on the generated keys using the ' ls -l ' command. In particular, the private key should be readable only by you: For more details, follow the ULHPC Tutorials: Preliminaries / SSH . (deprecated - Windows only): SSH key management with MobaKeyGen tool On Windows with MobaXterm , a tool exists and can be used to generate an SSH key pair. While not recommended (we encourage you to run WSL), here are the instructions to follow to generate these keys: Open the application Start > Program Files > MobaXterm . Change the default home directory for a persistent home directory instead of the default Temp directory. Go onto Settings > Configuration > General > Persistent home directory . choose a location for your home directory. your local SSH configuration will be located under HOME/.ssh/ Go onto Tools > Network > MobaKeyGen (SSH key generator) . Choose RSA as the type of key to generate and change \"Number of bits in a generated key\" to 4096. Click on the Generate button. Move your mouse to generate some randomness. Select a strong passphrase in the Key passphrase field for your key. Save the public and private keys as respectively id_rsa.pub and id_rsa.ppk . Please keep a copy of the public key, you will have to add this public key into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail). (deprecated - Windows only): SSH key management with PuTTY While no longer recommended, you may still want to use Putty and the associated tools, more precisely: PuTTY , the free SSH client Pageant , an SSH authentication agent for PuTTY tools PuTTYgen , an RSA key generation utility PSCP , an SCP (file transfer) client, i.e. command-line secure file copy WinSCP , SCP/SFTP (file transfer) client with easy-to-use graphical interface The different steps involved in the installation process are illustrated below ( REMEMBER to tick the option \"Associate .PPK files (PuTTY Private Key) with Pageant and PuTTYGen\" ): Now you can use the PuTTYgen utility to generate an RSA key pair. The main steps for the generation of the keys are illustrated below (yet with 4096 bits instead of 2048): Save the public and private keys as respectively id_rsa.pub and id_rsa.ppk . Please keep a copy of the public key, you will have to add this public key into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail). Password-less logins and transfers \u00b6 Password based authentication is disabled on all ULHPC servers. You can only use public-key authentication . This assumes that you upload your public SSH keys *.pub to your user entry on the ULHPC Identity Management Portal . Consult the associated documentation to discover how to do it. Once done, you can connect by SSH to the ULHPC clusters. Note that the port on which the SSH servers are listening is not the default SSH one ( i.e. 22) but 8022 . Consequently, if you want to connect to the Iris cluster, open a terminal and run (substituting yourlogin with the login name you received from us): Iris # ADAPT 'yourlogin' accordingly ssh -p 8022 yourlogin@access-iris.uni.lu Aion # ADAPT 'yourlogin' accordingly ssh -p 8022 yourlogin@access-aion.uni.lu Of course, we advise you to setup your SSH configuration to avoid typing this detailed command. This is explained in the next section. SSH Configuration \u00b6 On Linux / Mac OS / Unix / WSL, your SSH configuration is defined in ~/.ssh/config . As recommended in the ULHPC Tutorials: Preliminaries / SSH , you probably want to create the following configuration to easiest further access and data transfers: # ~/.ssh/config -- SSH Configuration # Common options Host * Compression yes ConnectTimeout 15 # ULHPC Clusters Host iris-cluster Hostname access-iris.uni.lu Host aion-cluster Hostname access-aion.uni.lu # /!\\ ADAPT 'yourlogin' accordingly Host *-cluster User yourlogin Port 8022 ForwardAgent no You should now be able to connect as follows Iris ssh iris-cluster Aion ssh aion-cluster (Windows only) Remote session configuration with MobaXterm This part of the documentation comes from MobaXterm documentation page MobaXterm allows you to launch remote sessions. You just have to click on the \"Sessions\" button to start a new session. Select SSH session on the second screen. Enter the following parameters: Remote host: access-iris.uni.lu (repeat with access-aion.uni.lu ) Check the Specify username box Username: yourlogin Adapt to match the one that was sent to you in the Welcome e-mail once your HPC account was created Port: 8022 Go in Advanced SSH settings and check the Use private key box. Select your previously generated key id_rsa.ppk . You can now click on Connect and enjoy. (deprecated - Windows only) - Remote session configuration with PuTTY If you want to connect to one of the ULHPC cluster, open Putty and enter the following settings: In Category:Session : Host Name: access-iris.uni.lu (or access-aion.uni.lu if you want to access Aion) Port: 8022 Connection Type: SSH (leave as default) In Category:Connection:Data : Auto-login username: yourlogin Adapt to match the one that was sent to you in the Welcome e-mail once your HPC account was created In Category:SSH:Auth : Upload your private key: Options controlling SSH authentication Click on Open button. If this is the first time connecting to the server from this computer a Putty Security Alert will appear. Accept the connection by clicking Yes . You should now be logged into the selected ULHPC login node . Now you probably want want to save the configuration of this connection : Go onto the Session category. Enter the settings you want to save. Enter a name in the Saved session field (for example Iris for access to Iris cluster). Click on the Save button. Next time you want to connect to the cluster, click on Load button and Open to open a new connection. SSH Agent \u00b6 On your laptop \u00b6 To be able to use your SSH key in a public-key authentication scheme, it must be loaded by an SSH agent . Mac OS X (>= 10.5) , this will be handled automatically; you will be asked to fill in the passphrase on the first connection. Linux , this will be handled automatically; you will be asked to fill the passphrase on the first connection. However if you get a message similar to the following: ( laptop ) $> ssh -vv iris-cluster [ ... ] Agent admitted failure to sign using the key. Permission denied ( publickey ) . This means that you have to manually load your key in the SSH agent by running: ( laptop ) $> ssh-add ~/.ssh/id_rsa Enter passphrase for ~/.ssh/id_rsa: # <-- enter your passphrase here Identity added: ~/.ssh/id_rsa ( <login>@<hostname> ) ( laptop ) $> ssh-add ~/.ssh/id_ed25519 Enter passphrase for ~/.ssh/id_ed25519: # <-- enter your passphrase here Identity added: ~/.ssh/id_ed25519 ( <login>@<hostname> ) On Ubuntu/WSL , if you experience issues when using ssh-add , you should install the keychain package and use it as follows (eventually add it to your ~/.profile ): # Installation ( laptop ) $> sudo apt install keychain # Save your passphrase /usr/bin/keychain --nogui ~/.ssh/id_ed25519 # (eventually) repeat with ~/.ssh/id_rsa # Load the agent in your shell source ~/.keychain/ $( hostname ) -sh (Windows only) SSH Agent within MobaXterm Go in Settings > SSH Tab In SSH agents section, check Use internal SSH agent \"MobAgent\" Click on the + button on the right Select your private key file. If you have several keys, you can add them by doing steps above again. Click on \"Show keys currently loaded in MobAgent\". An advertisement window may appear asking if you want to run MobAgent. Click on \"Yes\". Check that your key(s) appears in the window. Close the window. Click on OK . Restart MobaXterm. (deprecated - Windows only) - SSH Agent with PuTTY Pageant To be able to use your PuTTY key in a public-key authentication scheme, it must be loaded by an SSH agent . You should run Pageant for that. To load your SSH key in Pageant: Right-click on the pageant icon in the system tray, click on the Add key menu item select the private key file you saved while running puttygen.exe i.e. `` click on the Open button: a new dialog will pop up and ask for your passphrase. Once your passphrase is entered, your key will be loaded in pageant, enabling you to connect with Putty. On ULHPC clusters \u00b6 For security reason, SSH agent forwarding is prohibited and explicitly disabled (see ForwardAgent no configuration by default in the above configuration , you may need to manually load an agent once connected on the ULHPC facility, for instance if you are tired of typing the passphrase of a SSH key generated on the cluster to access a remote (private) service. You need to proceed as follows: $ eval \" $( ssh-agent ) \" # export the SSH_AUTH_SOCK and SSH_AGENT_PID variables $ ssh-add ~/.ssh/id_rsa # [...] Enter passphrase for [ ... ] Identity added: ~/.ssh/id_rsa ( <login>@<hostname> ) You can then enjoy it. Be aware however that this exposes your private key. So you MUST properly kill your agent when you don't need it any mode, using $ eval \" $( ssh-agent -k ) \" Agent pid <PID> killed Key fingerprints \u00b6 ULHPC may occasionally update the host keys on the major systems. Check here to confirm the current fingerprints. Iris With regards access-iris.uni.lu : 256 SHA256:tkhRD9IVo04NPw4OV/s2LSKEwe54LAEphm7yx8nq1pE /etc/ssh/ssh_host_ed25519_key.pub (ED25519) 2048 SHA256:WDWb2hh5uPU6RgaSotxzUe567F3scioJWy+9iftVmhI /etc/ssh/ssh_host_rsa_key.pub (RSA) Aion With regards access-aion.uni.lu : 256 SHA256:jwbW8pkfCzXrh1Xhf9n0UI+7hd/YGi4FlyOE92yxxe0 [access-aion.uni.lu]:8022 (ED25519) 3072 SHA256:L9n2gT6aV9KGy0Xdh1ks2DciE9wFz7MDRBPGWPFwFK4 [access-aion.uni.lu]:8022 (RSA) Get SSH key fingerprint The ssh fingerprints can be obtained via: ssh-keygen -lf <(ssh-keyscan -t rsa,ed25519 $(hostname) 2>/dev/null) Putty key fingerprint format Depending on the ssh client you use to connect to ULHPC systems, you may see different key fingerprints. For example, Putty uses different format of fingerprints as follows: access-iris.uni.lu ssh-ed25519 255 4096 07:6a:5f:11:df:d4:3f:d4:97:98:12:69:3a:63:70:2f You may see the following warning when connecting to Cori with Putty, but it is safe to ingore. PuTTY Security Alert The server's host key is not cached in the registry. You have no guarantee that the server is the computer you think it is. The server's ssh-ed25519 key fingerprint is: ssh-ed25519 255 4096 07:6a:5f:11:df:d4:3f:d4:97:98:12:69:3a:63:70:2f If you trust this host, hit Yes to add the key to PuTTY's cache and carry on connecting. If you want to carry on connecting just once, without adding the key to the cache, hit No. If you do not trust this host, hit Cancel to abandon the connection. Host Keys \u00b6 These are the entries in ~/.ssh/known_hosts . Iris The known host SSH entry for the Iris cluster should be as follows: [access-iris.uni.lu]:8022 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOP1eF8uJ37h5jFQQShn/NHRGD/d8KsMMUTHkoPRANLn Aion The known host SSH entry for the Aion cluster should be as follows: [access-aion.uni.lu]:8022 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFmcYJ7T6A1wOvIQaohgwVCrKLqIrzpQZAZrlEKx8Vsy Troubleshooting \u00b6 See the corresponding section . Advanced SSH Tips and Tricks \u00b6 CLI Completion \u00b6 The bash-completion package eases the ssh command usage by providing completion for hostnames and more (assuming you set the directive HashKnownHost to no in your ~/etc/ssh_config ) SOCKS 5 Proxy plugin \u00b6 Many Data Analytics framework involves a web interface (at the level of the master and/or the workers) you probably want to access in a relative transparent way. For that, a convenient way is to rely on a SOCKS proxy, which is basically an SSH tunnel in which specific applications forward their traffic down the tunnel to the server, and then on the server end, the proxy forwards the traffic out to the general Internet. Unlike a VPN, a SOCKS proxy has to be configured on an app by app basis on the client machine, but can be set up without any specialty client agents. The general principle is depicted below. Setting Up the Tunnel \u00b6 To initiate such a SOCKS proxy using SSH (listening on localhost:1080 for instance), you simply need to use the -D 1080 command line option when connecting to a remote server: Iris ssh -D 1080 -C iris-cluster Aion ssh -D 1080 -C aion-cluster -D : Tells SSH that we want a SOCKS tunnel on the specified port number (you can choose a number between 1025-65536) -C : Compresses the data before sending it FoxyProxy [Firefox] Extension \u00b6 Now that you have an SSH tunnel, it's time to configure your web browser (recommended: Firefox) to use that tunnel. In particular, install the Foxy Proxy extension for Firefox and configure it to use your SOCKS proxy: Right click on the fox icon, Select Options Add a new proxy button Name: ULHPC proxy Informations > Manual configuration Host IP: 127.0.0.1 Port: 1080 Check the Proxy SOCKS Option Click on OK Close Open a new tab Click on the Fox Choose the ULHPC proxy disable it when you no longer need it. You can now access any web interface deployed on any service reachable from the SSH jump host i.e. the ULHPC login node. Using tsock \u00b6 Once you setup a SSH SOCKS proxy, you can also use tsocks , a Shell wrapper to simplify the use of the tsocks(8) library to transparently allow an application (not aware of SOCKS) to transparently use a SOCKS proxy. For instance, assuming you create a VNC server on a given remote server as follows: (remote_server) $ > vncserver -geometry 1366x768 New 'X' desktop is remote_server:1 Starting applications specified in /home/username/.vnc/xstartup Log file is /home/username/.vnc/remote_server:1.log Then you can make the VNC client on your workstation use this tunnel to access the VNS server as follows: (laptop) $ > tsocks vncviewer <IP_of_remote_server>:1 tsock Escape character Use ~. to disconnect, even if your remote command hangs. SSH Port Forwarding \u00b6 Forwarding a local port \u00b6 You can forward a local port to a host behind a firewall. This is useful if you run a server on one of the cluster nodes (let's say listening on port 2222) and you want to access it via the local port 1111 on your machine. Then you'll run: # Here targeting iris cluster ( laptop ) $ ssh iris-cluster -L 1111 :iris-014:2222 Forwarding a remote port \u00b6 You can forward a remote port back to a host protected by your firewall. Tunnelling for others \u00b6 By using the -g parameter, you allow connections from other hosts than localhost to use your SSH tunnels. Be warned that anybody within your network may access the tunnelized host this way, which may be a security issue. Extras Tools around SSH \u00b6 Assh - Advanced SSH config is a transparent wrapper that make ~/.ssh/config easier to manage support for templates , aliases , defaults , inheritance etc. gateways : transparent ssh connection chaining more flexible command-line. Ex : Connect to hosta using hostb as a gateway $ ssh hosta/hostb drastically simplify your SSH config Linux / Mac OS only ClusterShell : clush , nodeset (or cluset), light, unified, robust command execution framework well-suited to ease daily administrative tasks of Linux clusters. using tools like clush and nodeset efficient, parallel, scalable command execution engine \\hfill{\\tiny in Python} provides an unified node groups syntax and external group access see nodeset and the NodeSet class DSH - Distributed / Dancer's Shell sshutle , \" where transparent proxy meets VPN meets ssh \"","title":"SSH"},{"location":"connect/ssh/#ssh","text":"All ULHPC servers are reached using either the Secure Shell (SSH) communication and encryption protocol (version 2). Developed by SSH Communications Security Ltd. , Secure Shell is a an encrypted network protocol used to log into another computer over an unsecured network, to execute commands in a remote machine, and to move files from one machine to another in a secure way. On UNIX/LINUX/BSD type systems, SSH is also the name of a suite of software applications for connecting via the SSH protocol. The SSH applications can execute commands on a remote machine and transfer files from one machine to another. All communications are automatically and transparently encrypted, including passwords. Most versions of SSH provide login ( ssh , slogin ), a remote copy operation ( scp ), and many also provide a secure ftp client ( sftp ). Additionally, SSH allows secure X Window connections. To use SSH, you have to generate a pair of keys, one public and the other private . The public key authentication is the most secure and flexible approach to ensure a multi-purpose transparent connection to a remote server. This approach is enforced on the ULHPC platforms and assumes that the public key is known by the system in order to perform an authentication based on a challenge/response protocol instead of the classical password-based protocol. The way SSH handles the keys and the configuration files is illustrated in the following figure:","title":"SSH"},{"location":"connect/ssh/#installation","text":"OpenSSH is natively supported on Linux / Mac OS / Unix / WSL (see below) On Windows, you are thus encouraged to install Windows Subsystem for Linux (WSL) and setup an Ubuntu subsystem from Microsoft Store . You probably want to also install Windows Terminal and MobaXterm Better performances of your Linux subsystem can be obtained by migrating to WSL 2 Follow the ULHPC Tutorial: Setup Pre-Requisites / Windows for detailed instructions.","title":"Installation"},{"location":"connect/ssh/#ssh-key-generation","text":"To generate an RSA SSH keys of 4096-bit length , just use the ssh-keygen command as follows : ssh-keygen -t rsa -b 4096 -o -a 100 After the execution of this command, the generated keys are stored in the following files: SSH RSA Private key: ~/.ssh/id_rsa . NEVER EVER TRANSMIT THIS FILE SSH RSA Public key: ~/.ssh/id_rsa.pub . This file is the ONLY one SAFE to distribute To passphrase or not to passphrase To ensure the security of your SSH key-pair on your laptop, you MUST protect your SSH keys with a passphrase! Note however that while possible, this passphrase is purely private and has a priori nothing to do with your University or your ULHPC credentials. Nevertheless, a strong passphrase follows the same recommendations as for strong passwords (for instance: see password requirements and guidelines . Finally, just like encryption keys, passphrases need to be kept safe and protected from unauthorised access. A Password Manager can help you to store all your passwords safely. The University is currently not offering a university wide password manger but there are many free and paid ones you can use, for example: KeePassX , PWSafe , Dashlane , 1Password or LastPass . You may want to generate also ED25519 Key Pairs (which is the most recommended public-key algorithm available today) -- see explaination ssh-keygen -t ed25519 -o -a 100 Your key pairs will be located under ~/.ssh/ and follow the following format -- the .pub extension indicated the public key part and is the ONLY one SAFE to distribute : $ ls -l ~/.ssh/id_* -rw------- username groupname ~/.ssh/id_rsa -rw-r--r-- username groupname ~/.ssh/id_rsa.pub # Public RSA key -rw------- username groupname ~/.ssh/id_ed25519 -rw-r--r-- username groupname ~/.ssh/id_ed25519.pub # Public ED25519 key Ensure the access rights are correct on the generated keys using the ' ls -l ' command. In particular, the private key should be readable only by you: For more details, follow the ULHPC Tutorials: Preliminaries / SSH . (deprecated - Windows only): SSH key management with MobaKeyGen tool On Windows with MobaXterm , a tool exists and can be used to generate an SSH key pair. While not recommended (we encourage you to run WSL), here are the instructions to follow to generate these keys: Open the application Start > Program Files > MobaXterm . Change the default home directory for a persistent home directory instead of the default Temp directory. Go onto Settings > Configuration > General > Persistent home directory . choose a location for your home directory. your local SSH configuration will be located under HOME/.ssh/ Go onto Tools > Network > MobaKeyGen (SSH key generator) . Choose RSA as the type of key to generate and change \"Number of bits in a generated key\" to 4096. Click on the Generate button. Move your mouse to generate some randomness. Select a strong passphrase in the Key passphrase field for your key. Save the public and private keys as respectively id_rsa.pub and id_rsa.ppk . Please keep a copy of the public key, you will have to add this public key into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail). (deprecated - Windows only): SSH key management with PuTTY While no longer recommended, you may still want to use Putty and the associated tools, more precisely: PuTTY , the free SSH client Pageant , an SSH authentication agent for PuTTY tools PuTTYgen , an RSA key generation utility PSCP , an SCP (file transfer) client, i.e. command-line secure file copy WinSCP , SCP/SFTP (file transfer) client with easy-to-use graphical interface The different steps involved in the installation process are illustrated below ( REMEMBER to tick the option \"Associate .PPK files (PuTTY Private Key) with Pageant and PuTTYGen\" ): Now you can use the PuTTYgen utility to generate an RSA key pair. The main steps for the generation of the keys are illustrated below (yet with 4096 bits instead of 2048): Save the public and private keys as respectively id_rsa.pub and id_rsa.ppk . Please keep a copy of the public key, you will have to add this public key into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail).","title":"SSH Key Generation"},{"location":"connect/ssh/#password-less-logins-and-transfers","text":"Password based authentication is disabled on all ULHPC servers. You can only use public-key authentication . This assumes that you upload your public SSH keys *.pub to your user entry on the ULHPC Identity Management Portal . Consult the associated documentation to discover how to do it. Once done, you can connect by SSH to the ULHPC clusters. Note that the port on which the SSH servers are listening is not the default SSH one ( i.e. 22) but 8022 . Consequently, if you want to connect to the Iris cluster, open a terminal and run (substituting yourlogin with the login name you received from us): Iris # ADAPT 'yourlogin' accordingly ssh -p 8022 yourlogin@access-iris.uni.lu Aion # ADAPT 'yourlogin' accordingly ssh -p 8022 yourlogin@access-aion.uni.lu Of course, we advise you to setup your SSH configuration to avoid typing this detailed command. This is explained in the next section.","title":"Password-less logins and transfers"},{"location":"connect/ssh/#ssh-configuration","text":"On Linux / Mac OS / Unix / WSL, your SSH configuration is defined in ~/.ssh/config . As recommended in the ULHPC Tutorials: Preliminaries / SSH , you probably want to create the following configuration to easiest further access and data transfers: # ~/.ssh/config -- SSH Configuration # Common options Host * Compression yes ConnectTimeout 15 # ULHPC Clusters Host iris-cluster Hostname access-iris.uni.lu Host aion-cluster Hostname access-aion.uni.lu # /!\\ ADAPT 'yourlogin' accordingly Host *-cluster User yourlogin Port 8022 ForwardAgent no You should now be able to connect as follows Iris ssh iris-cluster Aion ssh aion-cluster (Windows only) Remote session configuration with MobaXterm This part of the documentation comes from MobaXterm documentation page MobaXterm allows you to launch remote sessions. You just have to click on the \"Sessions\" button to start a new session. Select SSH session on the second screen. Enter the following parameters: Remote host: access-iris.uni.lu (repeat with access-aion.uni.lu ) Check the Specify username box Username: yourlogin Adapt to match the one that was sent to you in the Welcome e-mail once your HPC account was created Port: 8022 Go in Advanced SSH settings and check the Use private key box. Select your previously generated key id_rsa.ppk . You can now click on Connect and enjoy. (deprecated - Windows only) - Remote session configuration with PuTTY If you want to connect to one of the ULHPC cluster, open Putty and enter the following settings: In Category:Session : Host Name: access-iris.uni.lu (or access-aion.uni.lu if you want to access Aion) Port: 8022 Connection Type: SSH (leave as default) In Category:Connection:Data : Auto-login username: yourlogin Adapt to match the one that was sent to you in the Welcome e-mail once your HPC account was created In Category:SSH:Auth : Upload your private key: Options controlling SSH authentication Click on Open button. If this is the first time connecting to the server from this computer a Putty Security Alert will appear. Accept the connection by clicking Yes . You should now be logged into the selected ULHPC login node . Now you probably want want to save the configuration of this connection : Go onto the Session category. Enter the settings you want to save. Enter a name in the Saved session field (for example Iris for access to Iris cluster). Click on the Save button. Next time you want to connect to the cluster, click on Load button and Open to open a new connection.","title":"SSH Configuration"},{"location":"connect/ssh/#ssh-agent","text":"","title":"SSH Agent"},{"location":"connect/ssh/#on-your-laptop","text":"To be able to use your SSH key in a public-key authentication scheme, it must be loaded by an SSH agent . Mac OS X (>= 10.5) , this will be handled automatically; you will be asked to fill in the passphrase on the first connection. Linux , this will be handled automatically; you will be asked to fill the passphrase on the first connection. However if you get a message similar to the following: ( laptop ) $> ssh -vv iris-cluster [ ... ] Agent admitted failure to sign using the key. Permission denied ( publickey ) . This means that you have to manually load your key in the SSH agent by running: ( laptop ) $> ssh-add ~/.ssh/id_rsa Enter passphrase for ~/.ssh/id_rsa: # <-- enter your passphrase here Identity added: ~/.ssh/id_rsa ( <login>@<hostname> ) ( laptop ) $> ssh-add ~/.ssh/id_ed25519 Enter passphrase for ~/.ssh/id_ed25519: # <-- enter your passphrase here Identity added: ~/.ssh/id_ed25519 ( <login>@<hostname> ) On Ubuntu/WSL , if you experience issues when using ssh-add , you should install the keychain package and use it as follows (eventually add it to your ~/.profile ): # Installation ( laptop ) $> sudo apt install keychain # Save your passphrase /usr/bin/keychain --nogui ~/.ssh/id_ed25519 # (eventually) repeat with ~/.ssh/id_rsa # Load the agent in your shell source ~/.keychain/ $( hostname ) -sh (Windows only) SSH Agent within MobaXterm Go in Settings > SSH Tab In SSH agents section, check Use internal SSH agent \"MobAgent\" Click on the + button on the right Select your private key file. If you have several keys, you can add them by doing steps above again. Click on \"Show keys currently loaded in MobAgent\". An advertisement window may appear asking if you want to run MobAgent. Click on \"Yes\". Check that your key(s) appears in the window. Close the window. Click on OK . Restart MobaXterm. (deprecated - Windows only) - SSH Agent with PuTTY Pageant To be able to use your PuTTY key in a public-key authentication scheme, it must be loaded by an SSH agent . You should run Pageant for that. To load your SSH key in Pageant: Right-click on the pageant icon in the system tray, click on the Add key menu item select the private key file you saved while running puttygen.exe i.e. `` click on the Open button: a new dialog will pop up and ask for your passphrase. Once your passphrase is entered, your key will be loaded in pageant, enabling you to connect with Putty.","title":"On your laptop"},{"location":"connect/ssh/#on-ulhpc-clusters","text":"For security reason, SSH agent forwarding is prohibited and explicitly disabled (see ForwardAgent no configuration by default in the above configuration , you may need to manually load an agent once connected on the ULHPC facility, for instance if you are tired of typing the passphrase of a SSH key generated on the cluster to access a remote (private) service. You need to proceed as follows: $ eval \" $( ssh-agent ) \" # export the SSH_AUTH_SOCK and SSH_AGENT_PID variables $ ssh-add ~/.ssh/id_rsa # [...] Enter passphrase for [ ... ] Identity added: ~/.ssh/id_rsa ( <login>@<hostname> ) You can then enjoy it. Be aware however that this exposes your private key. So you MUST properly kill your agent when you don't need it any mode, using $ eval \" $( ssh-agent -k ) \" Agent pid <PID> killed","title":"On ULHPC clusters"},{"location":"connect/ssh/#key-fingerprints","text":"ULHPC may occasionally update the host keys on the major systems. Check here to confirm the current fingerprints. Iris With regards access-iris.uni.lu : 256 SHA256:tkhRD9IVo04NPw4OV/s2LSKEwe54LAEphm7yx8nq1pE /etc/ssh/ssh_host_ed25519_key.pub (ED25519) 2048 SHA256:WDWb2hh5uPU6RgaSotxzUe567F3scioJWy+9iftVmhI /etc/ssh/ssh_host_rsa_key.pub (RSA) Aion With regards access-aion.uni.lu : 256 SHA256:jwbW8pkfCzXrh1Xhf9n0UI+7hd/YGi4FlyOE92yxxe0 [access-aion.uni.lu]:8022 (ED25519) 3072 SHA256:L9n2gT6aV9KGy0Xdh1ks2DciE9wFz7MDRBPGWPFwFK4 [access-aion.uni.lu]:8022 (RSA) Get SSH key fingerprint The ssh fingerprints can be obtained via: ssh-keygen -lf <(ssh-keyscan -t rsa,ed25519 $(hostname) 2>/dev/null) Putty key fingerprint format Depending on the ssh client you use to connect to ULHPC systems, you may see different key fingerprints. For example, Putty uses different format of fingerprints as follows: access-iris.uni.lu ssh-ed25519 255 4096 07:6a:5f:11:df:d4:3f:d4:97:98:12:69:3a:63:70:2f You may see the following warning when connecting to Cori with Putty, but it is safe to ingore. PuTTY Security Alert The server's host key is not cached in the registry. You have no guarantee that the server is the computer you think it is. The server's ssh-ed25519 key fingerprint is: ssh-ed25519 255 4096 07:6a:5f:11:df:d4:3f:d4:97:98:12:69:3a:63:70:2f If you trust this host, hit Yes to add the key to PuTTY's cache and carry on connecting. If you want to carry on connecting just once, without adding the key to the cache, hit No. If you do not trust this host, hit Cancel to abandon the connection.","title":"Key fingerprints"},{"location":"connect/ssh/#host-keys","text":"These are the entries in ~/.ssh/known_hosts . Iris The known host SSH entry for the Iris cluster should be as follows: [access-iris.uni.lu]:8022 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOP1eF8uJ37h5jFQQShn/NHRGD/d8KsMMUTHkoPRANLn Aion The known host SSH entry for the Aion cluster should be as follows: [access-aion.uni.lu]:8022 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFmcYJ7T6A1wOvIQaohgwVCrKLqIrzpQZAZrlEKx8Vsy","title":"Host Keys"},{"location":"connect/ssh/#troubleshooting","text":"See the corresponding section .","title":"Troubleshooting"},{"location":"connect/ssh/#advanced-ssh-tips-and-tricks","text":"","title":"Advanced SSH Tips and Tricks"},{"location":"connect/ssh/#cli-completion","text":"The bash-completion package eases the ssh command usage by providing completion for hostnames and more (assuming you set the directive HashKnownHost to no in your ~/etc/ssh_config )","title":"CLI Completion"},{"location":"connect/ssh/#socks-5-proxy-plugin","text":"Many Data Analytics framework involves a web interface (at the level of the master and/or the workers) you probably want to access in a relative transparent way. For that, a convenient way is to rely on a SOCKS proxy, which is basically an SSH tunnel in which specific applications forward their traffic down the tunnel to the server, and then on the server end, the proxy forwards the traffic out to the general Internet. Unlike a VPN, a SOCKS proxy has to be configured on an app by app basis on the client machine, but can be set up without any specialty client agents. The general principle is depicted below.","title":"SOCKS 5 Proxy plugin"},{"location":"connect/ssh/#setting-up-the-tunnel","text":"To initiate such a SOCKS proxy using SSH (listening on localhost:1080 for instance), you simply need to use the -D 1080 command line option when connecting to a remote server: Iris ssh -D 1080 -C iris-cluster Aion ssh -D 1080 -C aion-cluster -D : Tells SSH that we want a SOCKS tunnel on the specified port number (you can choose a number between 1025-65536) -C : Compresses the data before sending it","title":"Setting Up the Tunnel"},{"location":"connect/ssh/#foxyproxy-firefox-extension","text":"Now that you have an SSH tunnel, it's time to configure your web browser (recommended: Firefox) to use that tunnel. In particular, install the Foxy Proxy extension for Firefox and configure it to use your SOCKS proxy: Right click on the fox icon, Select Options Add a new proxy button Name: ULHPC proxy Informations > Manual configuration Host IP: 127.0.0.1 Port: 1080 Check the Proxy SOCKS Option Click on OK Close Open a new tab Click on the Fox Choose the ULHPC proxy disable it when you no longer need it. You can now access any web interface deployed on any service reachable from the SSH jump host i.e. the ULHPC login node.","title":"FoxyProxy [Firefox] Extension"},{"location":"connect/ssh/#using-tsock","text":"Once you setup a SSH SOCKS proxy, you can also use tsocks , a Shell wrapper to simplify the use of the tsocks(8) library to transparently allow an application (not aware of SOCKS) to transparently use a SOCKS proxy. For instance, assuming you create a VNC server on a given remote server as follows: (remote_server) $ > vncserver -geometry 1366x768 New 'X' desktop is remote_server:1 Starting applications specified in /home/username/.vnc/xstartup Log file is /home/username/.vnc/remote_server:1.log Then you can make the VNC client on your workstation use this tunnel to access the VNS server as follows: (laptop) $ > tsocks vncviewer <IP_of_remote_server>:1 tsock Escape character Use ~. to disconnect, even if your remote command hangs.","title":"Using tsock"},{"location":"connect/ssh/#ssh-port-forwarding","text":"","title":"SSH Port Forwarding"},{"location":"connect/ssh/#forwarding-a-local-port","text":"You can forward a local port to a host behind a firewall. This is useful if you run a server on one of the cluster nodes (let's say listening on port 2222) and you want to access it via the local port 1111 on your machine. Then you'll run: # Here targeting iris cluster ( laptop ) $ ssh iris-cluster -L 1111 :iris-014:2222","title":"Forwarding a local port"},{"location":"connect/ssh/#forwarding-a-remote-port","text":"You can forward a remote port back to a host protected by your firewall.","title":"Forwarding a remote port"},{"location":"connect/ssh/#tunnelling-for-others","text":"By using the -g parameter, you allow connections from other hosts than localhost to use your SSH tunnels. Be warned that anybody within your network may access the tunnelized host this way, which may be a security issue.","title":"Tunnelling for others"},{"location":"connect/ssh/#extras-tools-around-ssh","text":"Assh - Advanced SSH config is a transparent wrapper that make ~/.ssh/config easier to manage support for templates , aliases , defaults , inheritance etc. gateways : transparent ssh connection chaining more flexible command-line. Ex : Connect to hosta using hostb as a gateway $ ssh hosta/hostb drastically simplify your SSH config Linux / Mac OS only ClusterShell : clush , nodeset (or cluset), light, unified, robust command execution framework well-suited to ease daily administrative tasks of Linux clusters. using tools like clush and nodeset efficient, parallel, scalable command execution engine \\hfill{\\tiny in Python} provides an unified node groups syntax and external group access see nodeset and the NodeSet class DSH - Distributed / Dancer's Shell sshutle , \" where transparent proxy meets VPN meets ssh \"","title":"Extras Tools around SSH"},{"location":"connect/troubleshooting/","text":"There are several possibilities and usually the error message can give you some hints. Your account has expired \u00b6 Please open a ticket on ServiceNow (HPC \u2192 User access & accounts \u2192 Report issue with cluster access) or send us an email to hpc-team@uni.lu with the current end date of your contract and we will extend your account accordingly. \"Access Denied\" or \"Permission denied (publickey)\" \u00b6 Basically, you are NOT able to connect to the access servers until your SSH public key is configured. There can be several reason that explain the denied connection message: Make sure you are using the proper ULHPC user name (and not your local username or University/Eduroam login). Check your mail entitled \" [HPC@Uni.lu] Welcome - Account information \" to get your ULHPC login Log into IPA and double check your SSH public key settings. Ensure you have run your SSH agent If you have a new computer or for some other reason you have generated new ssh key , please update your ssh keys on the IPA user portal. See IPA for more details You are using (deprecated) DSA/RSA keys . As per the OpenSSH website : \"OpenSSH 7.0 and greater similarly disable the ssh-dss (DSA) public key algorithm. It too is weak and we recommend against its use\". Solution: generate a new RSA keypair (3092 bit or more) and re-upload it on the IPA web portal (use the URL communicated to you by the UL HPC team in your \u201cwelcome\u201d mail). For more information on keys, see this website . Your public key is corrupted, please verify and re-upload it on the IPA web portal. We have taken the cluster down for maintenance and we forgot to activate the banner message mentioning this. Please check the calendar, the latest Twitter messages (box on the right of this page) and the messages sent on the hpc-users mailing list. If the above steps did not permit to solve your issue, please open a ticket on ServiceNow (HPC \u2192 User access & accounts \u2192 Report issue with cluster access) or send us an email to hpc-team@uni.lu . Host identification changed \u00b6 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. ... Ensure that your ~/.ssh/known_hosts file contains the correct entries for the ULHPC clusters and confirm the fingerprints using the posted fingerprints Open ~/.ssh/known_hosts Remove any lines referring Iris and Aion and save the file Paste the specified host key entries (for all clusters) OR retry connecting to the host and accept the new host key after verify that you have the correct \"fingerprint\" from the reference list . Be careful with permission changes to your $HOME \u00b6 If you change your home directory to be writeable by the group, ssh will not let you connect anymore. It requires drwxr-xr-x or 755 (or less) on your $HOME and ~/.ssh , and -rw-r--r-- or 644 (or less) on ~/.ssh/authorized_keys . File and folder permissions can be verified at any time using stat $path , e.g.: $> stat $HOME $> stat $HOME/.ssh $> stat $HOME/.ssh/authorized_keys Check out the description of the notation of file permissions in both symbolic and numeric mode. On your local machine, you also need to to have read/write permissions to ~/.ssh/config for your user only. This can be ensured with the following command: chmod 600 ~/.ssh/config Open a ticket \u00b6 If you cannot solve your problem, do not hesitate to open a ticket on the Service Now portal .","title":"Troubleshooting"},{"location":"connect/troubleshooting/#your-account-has-expired","text":"Please open a ticket on ServiceNow (HPC \u2192 User access & accounts \u2192 Report issue with cluster access) or send us an email to hpc-team@uni.lu with the current end date of your contract and we will extend your account accordingly.","title":"Your account has expired"},{"location":"connect/troubleshooting/#access-denied-or-permission-denied-publickey","text":"Basically, you are NOT able to connect to the access servers until your SSH public key is configured. There can be several reason that explain the denied connection message: Make sure you are using the proper ULHPC user name (and not your local username or University/Eduroam login). Check your mail entitled \" [HPC@Uni.lu] Welcome - Account information \" to get your ULHPC login Log into IPA and double check your SSH public key settings. Ensure you have run your SSH agent If you have a new computer or for some other reason you have generated new ssh key , please update your ssh keys on the IPA user portal. See IPA for more details You are using (deprecated) DSA/RSA keys . As per the OpenSSH website : \"OpenSSH 7.0 and greater similarly disable the ssh-dss (DSA) public key algorithm. It too is weak and we recommend against its use\". Solution: generate a new RSA keypair (3092 bit or more) and re-upload it on the IPA web portal (use the URL communicated to you by the UL HPC team in your \u201cwelcome\u201d mail). For more information on keys, see this website . Your public key is corrupted, please verify and re-upload it on the IPA web portal. We have taken the cluster down for maintenance and we forgot to activate the banner message mentioning this. Please check the calendar, the latest Twitter messages (box on the right of this page) and the messages sent on the hpc-users mailing list. If the above steps did not permit to solve your issue, please open a ticket on ServiceNow (HPC \u2192 User access & accounts \u2192 Report issue with cluster access) or send us an email to hpc-team@uni.lu .","title":"\"Access Denied\" or \"Permission denied (publickey)\""},{"location":"connect/troubleshooting/#host-identification-changed","text":"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. ... Ensure that your ~/.ssh/known_hosts file contains the correct entries for the ULHPC clusters and confirm the fingerprints using the posted fingerprints Open ~/.ssh/known_hosts Remove any lines referring Iris and Aion and save the file Paste the specified host key entries (for all clusters) OR retry connecting to the host and accept the new host key after verify that you have the correct \"fingerprint\" from the reference list .","title":"Host identification changed"},{"location":"connect/troubleshooting/#be-careful-with-permission-changes-to-your-home","text":"If you change your home directory to be writeable by the group, ssh will not let you connect anymore. It requires drwxr-xr-x or 755 (or less) on your $HOME and ~/.ssh , and -rw-r--r-- or 644 (or less) on ~/.ssh/authorized_keys . File and folder permissions can be verified at any time using stat $path , e.g.: $> stat $HOME $> stat $HOME/.ssh $> stat $HOME/.ssh/authorized_keys Check out the description of the notation of file permissions in both symbolic and numeric mode. On your local machine, you also need to to have read/write permissions to ~/.ssh/config for your user only. This can be ensured with the following command: chmod 600 ~/.ssh/config","title":"Be careful with permission changes to your $HOME"},{"location":"connect/troubleshooting/#open-a-ticket","text":"If you cannot solve your problem, do not hesitate to open a ticket on the Service Now portal .","title":"Open a ticket"},{"location":"connect/windows/","text":"In this page, we cover two different SSH client software: MobaXterm and Putty. Choose your preferred tool. MobaXterm \u00b6 Installation notes \u00b6 The following steps will help you to configure MobaXterm to access the UL HPC clusters. You can also check out the MobaXterm demo which shows an overview of its features. First, download and install MobaXterm. Open the application Start > Program Files > MobaXterm . Change the default home directory for a persistent home directory instead of the default Temp directory. Go onto Settings > Configuration > General > Persistent home directory . Choose a location for your home directory. Your local SSH configuration is located in the HOME/.ssh/ directory and consists of: HOME/.ssh/id_rsa.pub : your SSH public key. This one is the only one SAFE to distribute. HOME/.ssh/id_rsa : the associated private key. NEVER EVER TRANSMIT THIS FILE (eventually) the configuration of the SSH client HOME/.ssh/config HOME/.ssh/known_hosts : Contains a list of host keys for all hosts you have logged into that are not already in the system-wide list of known host keys. This permits to detect man-in-the-middle attacks. SSH Key Management \u00b6 Choose the method you prefer: either the graphical interface MobaKeyGen or command line generation of the ssh key. With MobaKeyGen tool \u00b6 Go onto Tools > Network > MobaKeyGen (SSH key generator) . Choose RSA as the type of key to generate and change \"Number of bits in a generated key\" to 4096. Click on the Generate button. Move your mouse to generate some randomness. Warning To ensure the security of the platform and your data stored on it, you must protect your SSH keys with a passphrase! Additionally, your private key and passphrase should never be transmitted to anybody. Select a strong passphrase in the Key passphrase field for your key. Save the public and private keys as respectively id_rsa.pub and id_rsa.ppk . Please keep a copy of the public key, you will have to add this public key into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail). With local terminal \u00b6 Click on Start local terminal . To generate an SSH keys, just use the ssh-keygen command, typically as follows: $> ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/home/user/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/user/.ssh/id_rsa. Your public key has been saved in /home/user/.ssh/id_rsa.pub. The key fingerprint is: fe:e8:26:df:38:49:3a:99:d7:85:4e:c3:85:c8:24:5b username@yourworkstation The key's randomart image is: +---[RSA 4096]----+ | | | . E | | * . . | | . o . . | | S. o | | .. = . | | =.= o | | * ==o | | B=.o | +-----------------+ Warning To ensure the security of the platform and your data stored on it, you must protect your SSH keys with a passphrase! After the execution of ssh-keygen command, the keys are generated and stored in the following files: SSH RSA Private key: HOME/.ssh/id_rsa . Again, NEVER EVER TRANSMIT THIS FILE SSH RSA Public key: HOME/.ssh/id_rsa.pub . This file is the ONLY one SAFE to distribute Configuration \u00b6 This part of the documentation comes from MobaXterm documentation page MobaXterm allows you to launch remote sessions. You just have to click on the \"Sessions\" button to start a new session. Select SSH session on the second screen. Enter the following parameters: Remote host: access-iris.uni.lu or access-aion.uni.lu Check the Specify username box Username: yourlogin as was sent to you in the Welcome e-mail once your HPC account was created Port: 8022 Go in Advanced SSH settings and check the Use private key box. Select your previously generated key id_rsa.ppk . Click on Connect . The following text appears. ================================================================================== Welcome to access2.iris-cluster.uni.lux ================================================================================== _ ____ / \\ ___ ___ ___ ___ ___|___ \\ / _ \\ / __/ __/ _ \\/ __/ __| __) | / ___ \\ (_| (_| __/\\__ \\__ \\/ __/ /_/ \\_\\___\\___\\___||___/___/_____| _____ _ ____ _ _ __ / /_ _|_ __(_)___ / ___| |_ _ ___| |_ ___ _ _\\ \\ | | | || '__| / __| | | | | | | / __| __/ _ \\ '__| | | | | || | | \\__ \\ | |___| | |_| \\__ \\ || __/ | | | | ||___|_| |_|___/ \\____|_|\\__,_|___/\\__\\___|_| | | \\_\\ /_/ ================================================================================== === Computing Nodes ========================================= #RAM/n === #Cores == iris-[001-108] 108 Dell C6320 (2 Xeon E5-2680v4@2.4GHz [14c/120W]) 128GB 3024 iris-[109-168] 60 Dell C6420 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 128GB 1680 iris-[169-186] 18 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 504 +72 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 16GB +368640 iris-[187-190] 4 Dell R840 (4 Xeon Platin.8180M@2.5GHz [28c/205W]) 3TB 448 iris-[191-196] 6 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 168 +24 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 32GB +122880 ================================================================================== *** TOTAL: 196 nodes, 5824 cores + 491520 CUDA cores + 61440 Tensor cores *** Fast interconnect using InfiniBand EDR 100 Gb/s technology Shared Storage (raw capacity): 2180 TB (GPFS) + 1300 TB (Lustre) = 3480 TB Support (in this order!) Platform notifications - User DOC ........ https://hpc.uni.lu/docs - Twitter: @ULHPC - FAQ ............. https://hpc.uni.lu/faq - Mailing-list .... hpc-users@uni.lu - Bug reports .NEW. https://hpc.uni.lu/support (Service Now) - Admins .......... hpc-team@uni.lu (OPEN TICKETS) ================================================================================== /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND ! First reserve your nodes (using srun/sbatch(1)) Linux access2.iris-cluster.uni.lux 3.10.0-957.21.3.el7.x86_64 x86_64 15:51:56 up 6 days, 2:32, 39 users, load average: 0.59, 0.68, 0.54 [yourlogin@access2 ~]$ Putty \u00b6 Installation notes \u00b6 You need to install Putty and the associated tools, more precisely: PuTTY , the free SSH client Pageant , an SSH authentication agent for PuTTY tools PuTTYgen , an RSA key generation utility PSCP , an SCP (file transfer) client, i.e. command-line secure file copy WinSCP , SCP/SFTP (file transfer) client with easy-to-use graphical interface The simplest method is probably to download and run the latest Putty installer (does not include WinSCP). The different steps involved in the installation process are illustrated below ( REMEMBER to tick the option \"Associate .PPK files (PuTTY Private Key) with Pageant and PuTTYGen\" ): Now you should have all the Putty programs available in Start / All Programs / Putty . SSH Key Management \u00b6 Here you can use the PuTTYgen utility, an RSA key generation utility. The main steps for the generation of the keys are illustrated below: Configuration \u00b6 In order to be able to login to the clusters, you will have to add this public key into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail). The port on which the SSH servers are listening is not the default one ( i.e. 22) but 8022 . Consequently, if you want to connect to the Iris cluster, open Putty and enter the following settings: In Category:Session : Host Name: access-iris.uni.lu or access-aion.uni.lu Port: 8022 Connection Type: SSH (leave as default) In Category:Connection:Data : Auto-login username: yourlogin In Category:SSH:Auth : Upload your private key: Options controlling SSH authentication Click on Open button. If this is the first time connecting to the server from this computer a Putty Security Alert will appear. Accept the connection by clicking Yes . Enter your login (username of your HPC account). You are now logged into Iris access server with SSH. Alternatively, you may want to save the configuration of this connection. Go onto the Session category. Enter the settings you want to save. Enter a name in the Saved session field (for example Iris for access to Iris cluster). Click on the Save button. Next time you want to connect to the cluster, click on Load button and Open to open a new connexion. Now you'll be able to obtain the welcome banner: ================================================================================== Welcome to access2.iris-cluster.uni.lux ================================================================================== _ ____ / \\ ___ ___ ___ ___ ___|___ \\ / _ \\ / __/ __/ _ \\/ __/ __| __) | / ___ \\ (_| (_| __/\\__ \\__ \\/ __/ /_/ \\_\\___\\___\\___||___/___/_____| _____ _ ____ _ _ __ / /_ _|_ __(_)___ / ___| |_ _ ___| |_ ___ _ _\\ \\ | | | || '__| / __| | | | | | | / __| __/ _ \\ '__| | | | | || | | \\__ \\ | |___| | |_| \\__ \\ || __/ | | | | ||___|_| |_|___/ \\____|_|\\__,_|___/\\__\\___|_| | | \\_\\ /_/ ================================================================================== === Computing Nodes ========================================= #RAM/n === #Cores == iris-[001-108] 108 Dell C6320 (2 Xeon E5-2680v4@2.4GHz [14c/120W]) 128GB 3024 iris-[109-168] 60 Dell C6420 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 128GB 1680 iris-[169-186] 18 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 504 +72 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 16GB +368640 iris-[187-190] 4 Dell R840 (4 Xeon Platin.8180M@2.5GHz [28c/205W]) 3TB 448 iris-[191-196] 6 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 168 +24 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 32GB +122880 ================================================================================== *** TOTAL: 196 nodes, 5824 cores + 491520 CUDA cores + 61440 Tensor cores *** Fast interconnect using InfiniBand EDR 100 Gb/s technology Shared Storage (raw capacity): 2180 TB (GPFS) + 1300 TB (Lustre) = 3480 TB Support (in this order!) Platform notifications - User DOC ........ https://hpc.uni.lu/docs - Twitter: @ULHPC - FAQ ............. https://hpc.uni.lu/faq - Mailing-list .... hpc-users@uni.lu - Bug reports .NEW. https://hpc.uni.lu/support (Service Now) - Admins .......... hpc-team@uni.lu (OPEN TICKETS) ================================================================================== /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND ! First reserve your nodes (using srun/sbatch(1)) Activate the SSH agent \u00b6 To be able to use your SSH key in a public-key authentication scheme, it must be loaded by an SSH agent . You should run Pageant . To load your SSH key in Pageant, just right-click on the pageant icon in the system tray, click on the Add key menu item and select the private key file you saved while running puttygen.exe and click on the Open button: a new dialog will pop up and ask for your passphrase. Once your passphrase is entered, your key will be loaded in pageant, enabling you to connect with Putty. Open Putty.exe (connection type: SSH ) In _Category:Session_: Host Name: access-iris.uni.lu or access-aion.uni.lu Port: 8022 Saved session: Iris In Category:Connection:Data : Auto-login username: yourlogin Go back to Category:Session and click on Save Click on Open SSH Resources \u00b6 OpenSSH/ Cygwin : OpenSSH is available with Cygwin. You may then find the same features in your SSH client even if you run Windows. Furthermore, Cygwin also embeds many other GNU Un*x like tools, and even a FREE X server for windows. Putty : Free windowish SSH client ssh.com Free for non commercial use windows client","title":"Windows"},{"location":"connect/windows/#mobaxterm","text":"","title":"MobaXterm"},{"location":"connect/windows/#installation-notes","text":"The following steps will help you to configure MobaXterm to access the UL HPC clusters. You can also check out the MobaXterm demo which shows an overview of its features. First, download and install MobaXterm. Open the application Start > Program Files > MobaXterm . Change the default home directory for a persistent home directory instead of the default Temp directory. Go onto Settings > Configuration > General > Persistent home directory . Choose a location for your home directory. Your local SSH configuration is located in the HOME/.ssh/ directory and consists of: HOME/.ssh/id_rsa.pub : your SSH public key. This one is the only one SAFE to distribute. HOME/.ssh/id_rsa : the associated private key. NEVER EVER TRANSMIT THIS FILE (eventually) the configuration of the SSH client HOME/.ssh/config HOME/.ssh/known_hosts : Contains a list of host keys for all hosts you have logged into that are not already in the system-wide list of known host keys. This permits to detect man-in-the-middle attacks.","title":"Installation notes"},{"location":"connect/windows/#ssh-key-management","text":"Choose the method you prefer: either the graphical interface MobaKeyGen or command line generation of the ssh key.","title":"SSH Key Management"},{"location":"connect/windows/#with-mobakeygen-tool","text":"Go onto Tools > Network > MobaKeyGen (SSH key generator) . Choose RSA as the type of key to generate and change \"Number of bits in a generated key\" to 4096. Click on the Generate button. Move your mouse to generate some randomness. Warning To ensure the security of the platform and your data stored on it, you must protect your SSH keys with a passphrase! Additionally, your private key and passphrase should never be transmitted to anybody. Select a strong passphrase in the Key passphrase field for your key. Save the public and private keys as respectively id_rsa.pub and id_rsa.ppk . Please keep a copy of the public key, you will have to add this public key into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail).","title":"With MobaKeyGen tool"},{"location":"connect/windows/#with-local-terminal","text":"Click on Start local terminal . To generate an SSH keys, just use the ssh-keygen command, typically as follows: $> ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/home/user/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/user/.ssh/id_rsa. Your public key has been saved in /home/user/.ssh/id_rsa.pub. The key fingerprint is: fe:e8:26:df:38:49:3a:99:d7:85:4e:c3:85:c8:24:5b username@yourworkstation The key's randomart image is: +---[RSA 4096]----+ | | | . E | | * . . | | . o . . | | S. o | | .. = . | | =.= o | | * ==o | | B=.o | +-----------------+ Warning To ensure the security of the platform and your data stored on it, you must protect your SSH keys with a passphrase! After the execution of ssh-keygen command, the keys are generated and stored in the following files: SSH RSA Private key: HOME/.ssh/id_rsa . Again, NEVER EVER TRANSMIT THIS FILE SSH RSA Public key: HOME/.ssh/id_rsa.pub . This file is the ONLY one SAFE to distribute","title":"With local terminal"},{"location":"connect/windows/#configuration","text":"This part of the documentation comes from MobaXterm documentation page MobaXterm allows you to launch remote sessions. You just have to click on the \"Sessions\" button to start a new session. Select SSH session on the second screen. Enter the following parameters: Remote host: access-iris.uni.lu or access-aion.uni.lu Check the Specify username box Username: yourlogin as was sent to you in the Welcome e-mail once your HPC account was created Port: 8022 Go in Advanced SSH settings and check the Use private key box. Select your previously generated key id_rsa.ppk . Click on Connect . The following text appears. ================================================================================== Welcome to access2.iris-cluster.uni.lux ================================================================================== _ ____ / \\ ___ ___ ___ ___ ___|___ \\ / _ \\ / __/ __/ _ \\/ __/ __| __) | / ___ \\ (_| (_| __/\\__ \\__ \\/ __/ /_/ \\_\\___\\___\\___||___/___/_____| _____ _ ____ _ _ __ / /_ _|_ __(_)___ / ___| |_ _ ___| |_ ___ _ _\\ \\ | | | || '__| / __| | | | | | | / __| __/ _ \\ '__| | | | | || | | \\__ \\ | |___| | |_| \\__ \\ || __/ | | | | ||___|_| |_|___/ \\____|_|\\__,_|___/\\__\\___|_| | | \\_\\ /_/ ================================================================================== === Computing Nodes ========================================= #RAM/n === #Cores == iris-[001-108] 108 Dell C6320 (2 Xeon E5-2680v4@2.4GHz [14c/120W]) 128GB 3024 iris-[109-168] 60 Dell C6420 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 128GB 1680 iris-[169-186] 18 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 504 +72 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 16GB +368640 iris-[187-190] 4 Dell R840 (4 Xeon Platin.8180M@2.5GHz [28c/205W]) 3TB 448 iris-[191-196] 6 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 168 +24 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 32GB +122880 ================================================================================== *** TOTAL: 196 nodes, 5824 cores + 491520 CUDA cores + 61440 Tensor cores *** Fast interconnect using InfiniBand EDR 100 Gb/s technology Shared Storage (raw capacity): 2180 TB (GPFS) + 1300 TB (Lustre) = 3480 TB Support (in this order!) Platform notifications - User DOC ........ https://hpc.uni.lu/docs - Twitter: @ULHPC - FAQ ............. https://hpc.uni.lu/faq - Mailing-list .... hpc-users@uni.lu - Bug reports .NEW. https://hpc.uni.lu/support (Service Now) - Admins .......... hpc-team@uni.lu (OPEN TICKETS) ================================================================================== /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND ! First reserve your nodes (using srun/sbatch(1)) Linux access2.iris-cluster.uni.lux 3.10.0-957.21.3.el7.x86_64 x86_64 15:51:56 up 6 days, 2:32, 39 users, load average: 0.59, 0.68, 0.54 [yourlogin@access2 ~]$","title":"Configuration"},{"location":"connect/windows/#putty","text":"","title":"Putty"},{"location":"connect/windows/#installation-notes_1","text":"You need to install Putty and the associated tools, more precisely: PuTTY , the free SSH client Pageant , an SSH authentication agent for PuTTY tools PuTTYgen , an RSA key generation utility PSCP , an SCP (file transfer) client, i.e. command-line secure file copy WinSCP , SCP/SFTP (file transfer) client with easy-to-use graphical interface The simplest method is probably to download and run the latest Putty installer (does not include WinSCP). The different steps involved in the installation process are illustrated below ( REMEMBER to tick the option \"Associate .PPK files (PuTTY Private Key) with Pageant and PuTTYGen\" ): Now you should have all the Putty programs available in Start / All Programs / Putty .","title":"Installation notes"},{"location":"connect/windows/#ssh-key-management_1","text":"Here you can use the PuTTYgen utility, an RSA key generation utility. The main steps for the generation of the keys are illustrated below:","title":"SSH Key Management"},{"location":"connect/windows/#configuration_1","text":"In order to be able to login to the clusters, you will have to add this public key into your account, using the IPA user portal (use the URL communicated to you by the UL HPC team in your \"welcome\" mail). The port on which the SSH servers are listening is not the default one ( i.e. 22) but 8022 . Consequently, if you want to connect to the Iris cluster, open Putty and enter the following settings: In Category:Session : Host Name: access-iris.uni.lu or access-aion.uni.lu Port: 8022 Connection Type: SSH (leave as default) In Category:Connection:Data : Auto-login username: yourlogin In Category:SSH:Auth : Upload your private key: Options controlling SSH authentication Click on Open button. If this is the first time connecting to the server from this computer a Putty Security Alert will appear. Accept the connection by clicking Yes . Enter your login (username of your HPC account). You are now logged into Iris access server with SSH. Alternatively, you may want to save the configuration of this connection. Go onto the Session category. Enter the settings you want to save. Enter a name in the Saved session field (for example Iris for access to Iris cluster). Click on the Save button. Next time you want to connect to the cluster, click on Load button and Open to open a new connexion. Now you'll be able to obtain the welcome banner: ================================================================================== Welcome to access2.iris-cluster.uni.lux ================================================================================== _ ____ / \\ ___ ___ ___ ___ ___|___ \\ / _ \\ / __/ __/ _ \\/ __/ __| __) | / ___ \\ (_| (_| __/\\__ \\__ \\/ __/ /_/ \\_\\___\\___\\___||___/___/_____| _____ _ ____ _ _ __ / /_ _|_ __(_)___ / ___| |_ _ ___| |_ ___ _ _\\ \\ | | | || '__| / __| | | | | | | / __| __/ _ \\ '__| | | | | || | | \\__ \\ | |___| | |_| \\__ \\ || __/ | | | | ||___|_| |_|___/ \\____|_|\\__,_|___/\\__\\___|_| | | \\_\\ /_/ ================================================================================== === Computing Nodes ========================================= #RAM/n === #Cores == iris-[001-108] 108 Dell C6320 (2 Xeon E5-2680v4@2.4GHz [14c/120W]) 128GB 3024 iris-[109-168] 60 Dell C6420 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 128GB 1680 iris-[169-186] 18 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 504 +72 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 16GB +368640 iris-[187-190] 4 Dell R840 (4 Xeon Platin.8180M@2.5GHz [28c/205W]) 3TB 448 iris-[191-196] 6 Dell C4140 (2 Xeon Gold 6132@2.6GHz [14c/140W]) 768GB 168 +24 GPU (4 Tesla V100 [5120c CUDA + 640c Tensor]) 32GB +122880 ================================================================================== *** TOTAL: 196 nodes, 5824 cores + 491520 CUDA cores + 61440 Tensor cores *** Fast interconnect using InfiniBand EDR 100 Gb/s technology Shared Storage (raw capacity): 2180 TB (GPFS) + 1300 TB (Lustre) = 3480 TB Support (in this order!) Platform notifications - User DOC ........ https://hpc.uni.lu/docs - Twitter: @ULHPC - FAQ ............. https://hpc.uni.lu/faq - Mailing-list .... hpc-users@uni.lu - Bug reports .NEW. https://hpc.uni.lu/support (Service Now) - Admins .......... hpc-team@uni.lu (OPEN TICKETS) ================================================================================== /!\\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND ! First reserve your nodes (using srun/sbatch(1))","title":"Configuration"},{"location":"connect/windows/#activate-the-ssh-agent","text":"To be able to use your SSH key in a public-key authentication scheme, it must be loaded by an SSH agent . You should run Pageant . To load your SSH key in Pageant, just right-click on the pageant icon in the system tray, click on the Add key menu item and select the private key file you saved while running puttygen.exe and click on the Open button: a new dialog will pop up and ask for your passphrase. Once your passphrase is entered, your key will be loaded in pageant, enabling you to connect with Putty. Open Putty.exe (connection type: SSH ) In _Category:Session_: Host Name: access-iris.uni.lu or access-aion.uni.lu Port: 8022 Saved session: Iris In Category:Connection:Data : Auto-login username: yourlogin Go back to Category:Session and click on Save Click on Open","title":"Activate the SSH agent"},{"location":"connect/windows/#ssh-resources","text":"OpenSSH/ Cygwin : OpenSSH is available with Cygwin. You may then find the same features in your SSH client even if you run Windows. Furthermore, Cygwin also embeds many other GNU Un*x like tools, and even a FREE X server for windows. Putty : Free windowish SSH client ssh.com Free for non commercial use windows client","title":"SSH Resources"},{"location":"containers/","text":"Containers \u00b6 Many applications and libraries can also be used through container systems, with the updated Singularity tool providing many new features of which we can especially highlight support for Open Containers Initiative - OCI containers (including Docker OCI), and support for secure containers - building and running encrypted containers with RSA keys and passphrases. Singularity \u00b6 The ULHPC offers the possibilty to run Singularity containers . Singularity is an open source container platform designed to be simple, fast, and secure. Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Loading Singularity \u00b6 To use Singularity, you need to load the corresponding Lmod module. >$ module load tools/Singularity Warning Modules are not allowed on the access servers. To test interactively Singularity, rememerber to ask for an interactive job first. salloc -p interactive --pty bash Pulling container images \u00b6 Like Docker , Singularity provide a way to pull images from a Hubs such as DockerHub and Singuarity Hub . >$ singularity pull docker://ubuntu:latest You should see the following output: Output INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob d72e567cc804 done Copying blob 0f3630e5ff08 done Copying blob b6a83d81d1f4 done Copying config bbea2a0436 done Writing manifest to image destination Storing signatures ... INFO: Creating SIF file... You may now test the container by executing some inner commands: >$ singularity exec ubuntu_latest.sif cat /etc/os-release Output NAME=\"Ubuntu\" VERSION=\"20.04.1 LTS (Focal Fossa)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 20.04.1 LTS\" VERSION_ID=\"20.04\" HOME_URL=\" https://www.ubuntu.com/&quot ; SUPPORT_URL=\" https://help.ubuntu.com/&quot ; BUG_REPORT_URL=\" https://bugs.launchpad.net/ubuntu/&quot ; PRIVACY_POLICY_URL=\" https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot ; VERSION_CODENAME=focal UBUNTU_CODENAME=focal Building container images \u00b6 Building container images requires to have root privileges. Therefore, users have to build images on their local machine before transfering them to the platform. Please refer to the Data transfer section for this purpose. Note Singularity 3 introduces the ability to build your containers in the cloud, so you can easily and securely create containers for your applications without speci al privileges or setup on your local system. The Remote Builder can securely build a container for you from a definition file entered here or via the Singularity CLI (see https://cloud.sylabs.io/builder for more details). GPU-enabled Singularity containers \u00b6 This section relies on the very excellent documentation from CSCS . In the following example, a container with CUDA features is build, transfered and tested on the ULHPC platform. This example will pull a CUDA container from DockrHub and setup CUDA examples . For this purpose, a singularity definition file, i.e., cuda_samples.def needs to be created with the following content: Bootstrap: docker From: nvidia/cuda:10.1-devel %post apt-get update apt-get install -y git git clone https://github.com/NVIDIA/cuda-samples.git /usr/local/cuda_samples cd /usr/local/cuda_samples git fetch origin --tags git checkout 10 .1.1 make %runscript /usr/local/cuda_samples/Samples/deviceQuery/deviceQuery On a local machine having singularity installed, we can build the container image, i.e., cuda_samples.sif using the definition file using the follwing singularity command: sudo singularity build cuda_samples.sif cuda_samples.def Warning You should have root privileges on this machine. Without this condition, you will not be able to built the definition file. Once the container is built and transfered to your dedicated storage on the ULHPC plaform, the container can be executed with the following command: # Inside an interactive job on a gpu-enabled node singularity run --nv cuda_samples.sif Warning In order to run a CUDA-enabled container, the --nv option has to be passed to singularity run. According to this option, singularity is going to setup the container environment to use the NVIDIA GPU and the basic CUDA libraries. Output CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"Tesla V100-SXM2-16GB\" CUDA Driver Version / Runtime Version 10.2 / 10.1 CUDA Capability Major/Minor version number: 7.0 Total amount of global memory: 16160 MBytes (16945512448 bytes) (80) Multiprocessors, ( 64) CUDA Cores/MP: 5120 CUDA Cores GPU Max Clock rate: 1530 MHz (1.53 GHz) Memory Clock rate: 877 Mhz Memory Bus Width: 4096-bit L2 Cache Size: 6291456 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 5 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Enabled Device supports Unified Addressing (UVA): Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 30 / 0 Compute Mode: < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) > deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 10.1, NumDevs = 1 Result = PASS MPI and Singularity containers \u00b6 This section relies on the very excellent documentation from CSCS . The following singularity definition file mpi_osu.def can be used to build a container with the osu benchmarks using mpi: bootstrap: docker from: debian:jessie %post # Install software apt-get update apt-get install -y file g++ gcc gfortran make gdb strace realpath wget curl --no-install-recommends # Install mpich curl -kO https://www.mpich.org/static/downloads/3.1.4/mpich-3.1.4.tar.gz tar -zxvf mpich-3.1.4.tar.gz cd mpich-3.1.4 ./configure --disable-fortran --enable-fast = all,O3 --prefix = /usr make -j $( nproc ) make install ldconfig # Build osu benchmarks wget -q http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.3.2.tar.gz tar xf osu-micro-benchmarks-5.3.2.tar.gz cd osu-micro-benchmarks-5.3.2 ./configure --prefix = /usr/local CC = $( which mpicc ) CFLAGS = -O3 make make install cd .. rm -rf osu-micro-benchmarks-5.3.2 rm osu-micro-benchmarks-5.3.2.tar.gz %runscript /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw sudo singularity build mpi_osu.sif mpi_osu.def Once the container image is ready, you can use it for example inside the following slurm launcher to start a best-effort job: #!/bin/bash -l #SBATCH -J ParallelJob #SBATCH -N 2 #SBATCH --ntasks-per-node=1 #SBATCH --time=05:00 #SBATCH -p batch #SBATCH --qos=qos-besteffort module load tools/Singularity srun -n $SLURM_NTASKS singularity run mpi_osu.sif The content of the output file: Output # OSU MPI Bandwidth Test v5.3.2 # Size Bandwidth (MB/s) 1 0.35 2 0.78 4 1.70 8 3.66 16 7.68 32 16.38 64 32.86 128 66.61 256 80.12 512 97.68 1024 151.57 2048 274.60 4096 408.71 8192 456.51 16384 565.84 32768 582.62 65536 587.17 131072 630.64 262144 656.45 524288 682.37 1048576 712.19 2097152 714.55","title":"About"},{"location":"containers/#containers","text":"Many applications and libraries can also be used through container systems, with the updated Singularity tool providing many new features of which we can especially highlight support for Open Containers Initiative - OCI containers (including Docker OCI), and support for secure containers - building and running encrypted containers with RSA keys and passphrases.","title":"Containers"},{"location":"containers/#singularity","text":"The ULHPC offers the possibilty to run Singularity containers . Singularity is an open source container platform designed to be simple, fast, and secure. Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way.","title":"Singularity"},{"location":"containers/#loading-singularity","text":"To use Singularity, you need to load the corresponding Lmod module. >$ module load tools/Singularity Warning Modules are not allowed on the access servers. To test interactively Singularity, rememerber to ask for an interactive job first. salloc -p interactive --pty bash","title":"Loading Singularity"},{"location":"containers/#pulling-container-images","text":"Like Docker , Singularity provide a way to pull images from a Hubs such as DockerHub and Singuarity Hub . >$ singularity pull docker://ubuntu:latest You should see the following output: Output INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob d72e567cc804 done Copying blob 0f3630e5ff08 done Copying blob b6a83d81d1f4 done Copying config bbea2a0436 done Writing manifest to image destination Storing signatures ... INFO: Creating SIF file... You may now test the container by executing some inner commands: >$ singularity exec ubuntu_latest.sif cat /etc/os-release Output NAME=\"Ubuntu\" VERSION=\"20.04.1 LTS (Focal Fossa)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 20.04.1 LTS\" VERSION_ID=\"20.04\" HOME_URL=\" https://www.ubuntu.com/&quot ; SUPPORT_URL=\" https://help.ubuntu.com/&quot ; BUG_REPORT_URL=\" https://bugs.launchpad.net/ubuntu/&quot ; PRIVACY_POLICY_URL=\" https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot ; VERSION_CODENAME=focal UBUNTU_CODENAME=focal","title":"Pulling container images"},{"location":"containers/#building-container-images","text":"Building container images requires to have root privileges. Therefore, users have to build images on their local machine before transfering them to the platform. Please refer to the Data transfer section for this purpose. Note Singularity 3 introduces the ability to build your containers in the cloud, so you can easily and securely create containers for your applications without speci al privileges or setup on your local system. The Remote Builder can securely build a container for you from a definition file entered here or via the Singularity CLI (see https://cloud.sylabs.io/builder for more details).","title":"Building container images"},{"location":"containers/#gpu-enabled-singularity-containers","text":"This section relies on the very excellent documentation from CSCS . In the following example, a container with CUDA features is build, transfered and tested on the ULHPC platform. This example will pull a CUDA container from DockrHub and setup CUDA examples . For this purpose, a singularity definition file, i.e., cuda_samples.def needs to be created with the following content: Bootstrap: docker From: nvidia/cuda:10.1-devel %post apt-get update apt-get install -y git git clone https://github.com/NVIDIA/cuda-samples.git /usr/local/cuda_samples cd /usr/local/cuda_samples git fetch origin --tags git checkout 10 .1.1 make %runscript /usr/local/cuda_samples/Samples/deviceQuery/deviceQuery On a local machine having singularity installed, we can build the container image, i.e., cuda_samples.sif using the definition file using the follwing singularity command: sudo singularity build cuda_samples.sif cuda_samples.def Warning You should have root privileges on this machine. Without this condition, you will not be able to built the definition file. Once the container is built and transfered to your dedicated storage on the ULHPC plaform, the container can be executed with the following command: # Inside an interactive job on a gpu-enabled node singularity run --nv cuda_samples.sif Warning In order to run a CUDA-enabled container, the --nv option has to be passed to singularity run. According to this option, singularity is going to setup the container environment to use the NVIDIA GPU and the basic CUDA libraries. Output CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"Tesla V100-SXM2-16GB\" CUDA Driver Version / Runtime Version 10.2 / 10.1 CUDA Capability Major/Minor version number: 7.0 Total amount of global memory: 16160 MBytes (16945512448 bytes) (80) Multiprocessors, ( 64) CUDA Cores/MP: 5120 CUDA Cores GPU Max Clock rate: 1530 MHz (1.53 GHz) Memory Clock rate: 877 Mhz Memory Bus Width: 4096-bit L2 Cache Size: 6291456 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 5 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Enabled Device supports Unified Addressing (UVA): Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 30 / 0 Compute Mode: < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) > deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 10.1, NumDevs = 1 Result = PASS","title":"GPU-enabled Singularity containers"},{"location":"containers/#mpi-and-singularity-containers","text":"This section relies on the very excellent documentation from CSCS . The following singularity definition file mpi_osu.def can be used to build a container with the osu benchmarks using mpi: bootstrap: docker from: debian:jessie %post # Install software apt-get update apt-get install -y file g++ gcc gfortran make gdb strace realpath wget curl --no-install-recommends # Install mpich curl -kO https://www.mpich.org/static/downloads/3.1.4/mpich-3.1.4.tar.gz tar -zxvf mpich-3.1.4.tar.gz cd mpich-3.1.4 ./configure --disable-fortran --enable-fast = all,O3 --prefix = /usr make -j $( nproc ) make install ldconfig # Build osu benchmarks wget -q http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.3.2.tar.gz tar xf osu-micro-benchmarks-5.3.2.tar.gz cd osu-micro-benchmarks-5.3.2 ./configure --prefix = /usr/local CC = $( which mpicc ) CFLAGS = -O3 make make install cd .. rm -rf osu-micro-benchmarks-5.3.2 rm osu-micro-benchmarks-5.3.2.tar.gz %runscript /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw sudo singularity build mpi_osu.sif mpi_osu.def Once the container image is ready, you can use it for example inside the following slurm launcher to start a best-effort job: #!/bin/bash -l #SBATCH -J ParallelJob #SBATCH -N 2 #SBATCH --ntasks-per-node=1 #SBATCH --time=05:00 #SBATCH -p batch #SBATCH --qos=qos-besteffort module load tools/Singularity srun -n $SLURM_NTASKS singularity run mpi_osu.sif The content of the output file: Output # OSU MPI Bandwidth Test v5.3.2 # Size Bandwidth (MB/s) 1 0.35 2 0.78 4 1.70 8 3.66 16 7.68 32 16.38 64 32.86 128 66.61 256 80.12 512 97.68 1024 151.57 2048 274.60 4096 408.71 8192 456.51 16384 565.84 32768 582.62 65536 587.17 131072 630.64 262144 656.45 524288 682.37 1048576 712.19 2097152 714.55","title":"MPI and Singularity containers"},{"location":"contributing/","text":"You are more than welcome to contribute to the development of this project. You are however expected to follow the model of Github Flow for your contributions. What is a [good] Git Workflow? A Git Workflow is a recipe or recommendation for how to use Git to accomplish work in a consistent and productive manner. Indeed, Git offers a lot of flexibility in how changes can be managed, yet there is no standardized process on how to interact with Git. The following questions are expected to be addressed by a successful workflow: Q1 : Does this workflow scale with team size? Q2 : Is it possible to prevent/limit mistakes and errors ? Q3 : Is it easy to undo mistakes and errors with this workflow? Q4 : Does this workflow permits to easily test new feature/functionnalities before production release ? Q5 : Does this workflow allow for Continuous Integration (even if not yet planned at the beginning) Q6 : Does this workflow permit to master the production release Q7 : Does this workflow impose any new unnecessary cognitive overhead to the team? Q8 : The workflow is easy to use/setup and maintain In particular, the default \" workflow \" centralizedgitl (where everybody just commit to the single master branch), while being the only one satisfying Q7, proved to be easily error-prone and can break production system relying on the underlying repository. For this reason, other more or less complex workflows have emerged -- all feature-branch-based , that supports teams and projects where production deployments are made regularly: Git-flow , the historical successful workflow featuring two main branches with an infinite lifetime ( production and {master | devel} ) all operations are facilitated by the git-flow CLI extension maintaining both branches can be bothersome - make up the only one permitting to really control production release Github Flow , a lightweight version with a single branch ( master ) pull-request based - requires interaction with Gitlab/Github web interface ( git request-pull might help) The ULHPC team enforces an hydrid workflow detailed below , HOWEVER you can safely contribute to this documentation by following the Github Flow explained now. Default Git workflow for contributions \u00b6 We expect contributors to follow the Github Flow concept. This flow is ideal for organizations that need simplicity, and roll out frequently. If you are already using Git, you are probably using a version of the Github flow. Every unit of work, whether it be a bugfix or feature, is done through a branch that is created from master. After the work has been completed in the branch, it is reviewed and tested before being merged into master and pushed out to production. In details: As preliminaries (to be done only once), Fork the ULHPC/ulhpc-docs repository under <YOUR-USERNAME>/ulhpc-docs A fork is a copy of a repository placed under your Github namespace. Forking a repository allows you to freely experiment with changes without affecting the original project. In the top-right corner of the ULHPC/ulhpc-docs repository, click \"Fork\" button. Under Settings, change the repository name from docs to ulhpc-docs Once done, you can clone your copy (forked) repository: select the SSH url under the \"Code\" button: # (Recommended) Place your repo in a clean (and self-explicit) directory layout # /!\\ ADAPT 'YOUR-USERNAME' with your Github username $> mkdir -p ~/git/github.com/YOUR-USERNAME $> cd ~/git/github.com/YOUR-USERNAME # /!\\ ADAPT 'YOUR-USERNAME' with your Github username git clone git@github.com:YOUR-USERNAME/ulhpc-docs.git $> cd ulhpc-docs $> make setup Configure your working forked copy to sync with the original ULHPC/ulhpc-docs repository through a dedicated upstream remote # Check current remote: only 'origin' should be listed $> git remote -v origin git@github.com:YOUR-USERNAME/ulhpc-docs.git ( fetch ) origin git@github.com:YOUR-USERNAME/ulhpc-docs.git ( push ) # Add upstream $> make setup-upstream # OR, manually: $> git remote add upstream https://github.com/ULHPC/ulhpc-docs.git # Check the new remote $> git remote -v origin git@github.com:YOUR-USERNAME/ulhpc-docs.git ( fetch ) origin git@github.com:YOUR-USERNAME/ulhpc-docs.git ( push ) upstream https://github.com/ULHPC/ulhpc-docs.git ( fetch ) upstream https://github.com/ULHPC/ulhpc-docs.git ( push ) At this level, you probably want to follow the setup instructions to configure your ulhpc-docs python virtualenv and deploy locally the documentation with make doc access the local documentation with your favorite browser by visiting the URL http://localhost:8000 Then, to bring your contributions: Pull the latest changes from the upstream remote using: make sync-upstream Create your own feature branch with appropriate name <name> : # IF you have installed git-flow: {brew | apt | yum |...} install gitflow git-flow # /!\\ ADAPT <name> with appropriate name: this will create and checkout to branch feature/<name> git-flow feature start <name> # OR git checkout -b feature/<name> Commit your changes once satisfied with them git add [ ... ] git commit -s -m 'Added some feature' Push to the feature branch and publish it # IF you have installed git-flow # /!\\ ADAPT <name> accordingly git-flow feature publish <name> # OR git push -u origin feature/<name> Create a new Pull Request to submit your changes to the ULHPC team. Commit first! # check what would be put in the pull request git request-pull master ./ # Open Pull Request from web interface # Github: Open 'new pull request' # Base = feature/<name>, compare = master Pull request will be reviewed, eventually with comments/suggestion for modifications -- see official doc you may need to apply new commits to resolve the comments -- remember to mention the pull request in the commit message with the prefix ' [PR#<ID>] ' (Ex: [PR#5] ) in your commit message cd /path/to/ulhpc-docs git checkout feature/<name> git pull # [...] git add [ ... ] # /!\\ ADAPT Pull Request ID accordingly git commit -s -m '[PR#<ID>] ...' After your pull request has been reviewed and merged , you can safely delete the feature branch. # Adapt <name> accordingly git checkout feature/<name> # Eventually, if needed make sync-upstream git-flow feature finish <name> # feature branch 'feature/<name>' will be merged into 'devel' # # feature branch 'feature/<name>' will be locally deleted # # you will checkout back to the 'master' branch git push origin --delete feature/<name> # /!\\ WARNING: Ensure you delete the CORRECT remote branch git push # sync master branch ULHPC Git Workflow \u00b6 Throughout all its projects, the ULHPC team has enforced a stricter workflow for Git repository summarized in the below figure: The main concepts inherited from both advanced workflows ( Git-flow and Github Flow ) are listed below: The central repository holds two main branches with an infinite lifetime: production : the production-ready branch, used for the deployed version of the documentation. devel | master | main ( master in this case): the main (master) branch where the latest developments intervene (name depends on repository purpose). This is the default branch you get when you clone the repository. You should always setup your local copy of the repository with make setup ensure also you have installed the gitflow extension ensure you are properly made the initial configuration of git -- see also sample .gitconfig In compliment to the Github Flow described above, several additional operations are facilitated by the root Makefile : Initial setup of the repository with make setup Release of a new version of this repository with make start_bump_{patch,minor,major} and make release this action is managed by the ULHPC team according to the semantic versioning scheme implemented within this this project.","title":"Overview"},{"location":"contributing/#default-git-workflow-for-contributions","text":"We expect contributors to follow the Github Flow concept. This flow is ideal for organizations that need simplicity, and roll out frequently. If you are already using Git, you are probably using a version of the Github flow. Every unit of work, whether it be a bugfix or feature, is done through a branch that is created from master. After the work has been completed in the branch, it is reviewed and tested before being merged into master and pushed out to production. In details: As preliminaries (to be done only once), Fork the ULHPC/ulhpc-docs repository under <YOUR-USERNAME>/ulhpc-docs A fork is a copy of a repository placed under your Github namespace. Forking a repository allows you to freely experiment with changes without affecting the original project. In the top-right corner of the ULHPC/ulhpc-docs repository, click \"Fork\" button. Under Settings, change the repository name from docs to ulhpc-docs Once done, you can clone your copy (forked) repository: select the SSH url under the \"Code\" button: # (Recommended) Place your repo in a clean (and self-explicit) directory layout # /!\\ ADAPT 'YOUR-USERNAME' with your Github username $> mkdir -p ~/git/github.com/YOUR-USERNAME $> cd ~/git/github.com/YOUR-USERNAME # /!\\ ADAPT 'YOUR-USERNAME' with your Github username git clone git@github.com:YOUR-USERNAME/ulhpc-docs.git $> cd ulhpc-docs $> make setup Configure your working forked copy to sync with the original ULHPC/ulhpc-docs repository through a dedicated upstream remote # Check current remote: only 'origin' should be listed $> git remote -v origin git@github.com:YOUR-USERNAME/ulhpc-docs.git ( fetch ) origin git@github.com:YOUR-USERNAME/ulhpc-docs.git ( push ) # Add upstream $> make setup-upstream # OR, manually: $> git remote add upstream https://github.com/ULHPC/ulhpc-docs.git # Check the new remote $> git remote -v origin git@github.com:YOUR-USERNAME/ulhpc-docs.git ( fetch ) origin git@github.com:YOUR-USERNAME/ulhpc-docs.git ( push ) upstream https://github.com/ULHPC/ulhpc-docs.git ( fetch ) upstream https://github.com/ULHPC/ulhpc-docs.git ( push ) At this level, you probably want to follow the setup instructions to configure your ulhpc-docs python virtualenv and deploy locally the documentation with make doc access the local documentation with your favorite browser by visiting the URL http://localhost:8000 Then, to bring your contributions: Pull the latest changes from the upstream remote using: make sync-upstream Create your own feature branch with appropriate name <name> : # IF you have installed git-flow: {brew | apt | yum |...} install gitflow git-flow # /!\\ ADAPT <name> with appropriate name: this will create and checkout to branch feature/<name> git-flow feature start <name> # OR git checkout -b feature/<name> Commit your changes once satisfied with them git add [ ... ] git commit -s -m 'Added some feature' Push to the feature branch and publish it # IF you have installed git-flow # /!\\ ADAPT <name> accordingly git-flow feature publish <name> # OR git push -u origin feature/<name> Create a new Pull Request to submit your changes to the ULHPC team. Commit first! # check what would be put in the pull request git request-pull master ./ # Open Pull Request from web interface # Github: Open 'new pull request' # Base = feature/<name>, compare = master Pull request will be reviewed, eventually with comments/suggestion for modifications -- see official doc you may need to apply new commits to resolve the comments -- remember to mention the pull request in the commit message with the prefix ' [PR#<ID>] ' (Ex: [PR#5] ) in your commit message cd /path/to/ulhpc-docs git checkout feature/<name> git pull # [...] git add [ ... ] # /!\\ ADAPT Pull Request ID accordingly git commit -s -m '[PR#<ID>] ...' After your pull request has been reviewed and merged , you can safely delete the feature branch. # Adapt <name> accordingly git checkout feature/<name> # Eventually, if needed make sync-upstream git-flow feature finish <name> # feature branch 'feature/<name>' will be merged into 'devel' # # feature branch 'feature/<name>' will be locally deleted # # you will checkout back to the 'master' branch git push origin --delete feature/<name> # /!\\ WARNING: Ensure you delete the CORRECT remote branch git push # sync master branch","title":"Default Git workflow for contributions"},{"location":"contributing/#ulhpc-git-workflow","text":"Throughout all its projects, the ULHPC team has enforced a stricter workflow for Git repository summarized in the below figure: The main concepts inherited from both advanced workflows ( Git-flow and Github Flow ) are listed below: The central repository holds two main branches with an infinite lifetime: production : the production-ready branch, used for the deployed version of the documentation. devel | master | main ( master in this case): the main (master) branch where the latest developments intervene (name depends on repository purpose). This is the default branch you get when you clone the repository. You should always setup your local copy of the repository with make setup ensure also you have installed the gitflow extension ensure you are properly made the initial configuration of git -- see also sample .gitconfig In compliment to the Github Flow described above, several additional operations are facilitated by the root Makefile : Initial setup of the repository with make setup Release of a new version of this repository with make start_bump_{patch,minor,major} and make release this action is managed by the ULHPC team according to the semantic versioning scheme implemented within this this project.","title":"ULHPC Git Workflow"},{"location":"contributing/versioning/","text":"The operation consisting of releasing a new version of this repository is automated by a set of tasks within the root Makefile . In this context, a version number have the following format: <major>.<minor>.<patch>[-b<build>] where: < major > corresponds to the major version number < minor > corresponds to the minor version number < patch > corresponds to the patching version number (eventually) < build > states the build number i.e. the total number of commits within the devel branch. Example: `1.0.0-b28`. VERSION file The current version number is stored in the root file VERSION . /!\\ IMPORTANT: NEVER MAKE ANY MANUAL CHANGES TO THIS FILE ULHPC/docs repository release Only the ULHPC team is allowed to perform the releasing operations (and push to the production branch). By default, the main documentation website is built against the production branch. For more information on the version, run: $> make versioninfo ULHPC Team procedure for repository release If a new version number such be bumped, the following command is issued: make start_bump_ { major,minor,patch } This will start the release process for you using git-flow within the release/<new-version> branch - see also Git(hub) flow . Once the last changes are committed, the release becomes effective by running: make release It will finish the release using git-flow , create the appropriate tag in the production branch and merge all things the way they should be in the master branch.","title":"Semantic Versioning"},{"location":"data/backups/","text":"Backups \u00b6 Danger All ULHPC users should back up important files on a regular basis. Ultimately, it is your responsibility to protect yourself from data loss. ULHPC has 3 different backup targets, with different rotation policies and physical locations. The backups are only accessible by HPC staff, for disaster recovery purposes. More precisions can be requested via a support request. User directories on the ULHPC clusters \u00b6 Directory Path Backup location Frequency Retention home directories $HOME CDC, Belval Weekly last 7 backups, at least one per month for the last 2 months projects $PROJECTWORK CDC, Belval Weekly one backup per week of the backup directory ( $PROJECT/backup/ ) scratch $SCRATCH not backed up Isilon project directories snapshots \u00b6 Projects stored on the Isilon filesystem are snapshotted weekly, the snapshots are kept for 10 days. Danger Snapshots are not a real backup . It does not protect you against a system failure, it will only permit to recover some files in case of accidental deletion Each project directory, in /mnt/isilon/projects/ contains a hidden sub-directory .snapshot : .snapshot is invisible to ls , ls -a , find and similar commands can be browsed normally after cd .snapshot files cannot be created, deleted or edited in snapshots files can only be copied out of a snapshot Virtual machines \u00b6 Source Backup location Frequency Retention Gitlab infrastructure CS43, Kirchberg Weekly last 5 weekly snapshots Iris infrastructure CDC, Belval Weekly last 5 weekly snapshots Services \u00b6 Name Backup location Frequency Retention gitlab.uni.lu CDC, Belval Daily last 7 daily backups, one per month for the last 6 months hpc.uni.lu (pad, privatebin) CDC, Belval Daily last 7 daily backups, one per month for the last 6 months Restore \u00b6 If you require a restoration of lost data that cannot be accomplished via the snapshots capability, please create a new request on Service Now portal , with pathnames and timestamps of the missing data. Such restore requests may take a few days to complete. Purging \u00b6 Note See Filesystem Quotas and Purging for detailed information about inode, space quotas, and file system purge policies. Warning $SCRATCH directories are not backed up Backup Tools \u00b6 In practice, the ULHPC backup infrastructure is fully puppetized and make use of several tools facilitating the operations: backupninja , which allows you to coordinate system backup by dropping a few simple configuration files into /etc/backup.d/ a forked version of bontmia , which stands for \"Backup Over Network To Multiple Incremental Archives\" BorgBackup , a deduplicating backup program supporting compression and authenticated encryption. several internal scripts to pilot LVM snapshots/backup/restore operations","title":"Backups"},{"location":"data/backups/#backups","text":"Danger All ULHPC users should back up important files on a regular basis. Ultimately, it is your responsibility to protect yourself from data loss. ULHPC has 3 different backup targets, with different rotation policies and physical locations. The backups are only accessible by HPC staff, for disaster recovery purposes. More precisions can be requested via a support request.","title":"Backups"},{"location":"data/backups/#user-directories-on-the-ulhpc-clusters","text":"Directory Path Backup location Frequency Retention home directories $HOME CDC, Belval Weekly last 7 backups, at least one per month for the last 2 months projects $PROJECTWORK CDC, Belval Weekly one backup per week of the backup directory ( $PROJECT/backup/ ) scratch $SCRATCH not backed up","title":"User directories on the ULHPC clusters"},{"location":"data/backups/#isilon-project-directories-snapshots","text":"Projects stored on the Isilon filesystem are snapshotted weekly, the snapshots are kept for 10 days. Danger Snapshots are not a real backup . It does not protect you against a system failure, it will only permit to recover some files in case of accidental deletion Each project directory, in /mnt/isilon/projects/ contains a hidden sub-directory .snapshot : .snapshot is invisible to ls , ls -a , find and similar commands can be browsed normally after cd .snapshot files cannot be created, deleted or edited in snapshots files can only be copied out of a snapshot","title":"Isilon project directories snapshots"},{"location":"data/backups/#virtual-machines","text":"Source Backup location Frequency Retention Gitlab infrastructure CS43, Kirchberg Weekly last 5 weekly snapshots Iris infrastructure CDC, Belval Weekly last 5 weekly snapshots","title":"Virtual machines"},{"location":"data/backups/#services","text":"Name Backup location Frequency Retention gitlab.uni.lu CDC, Belval Daily last 7 daily backups, one per month for the last 6 months hpc.uni.lu (pad, privatebin) CDC, Belval Daily last 7 daily backups, one per month for the last 6 months","title":"Services"},{"location":"data/backups/#restore","text":"If you require a restoration of lost data that cannot be accomplished via the snapshots capability, please create a new request on Service Now portal , with pathnames and timestamps of the missing data. Such restore requests may take a few days to complete.","title":"Restore"},{"location":"data/backups/#purging","text":"Note See Filesystem Quotas and Purging for detailed information about inode, space quotas, and file system purge policies. Warning $SCRATCH directories are not backed up","title":"Purging"},{"location":"data/backups/#backup-tools","text":"In practice, the ULHPC backup infrastructure is fully puppetized and make use of several tools facilitating the operations: backupninja , which allows you to coordinate system backup by dropping a few simple configuration files into /etc/backup.d/ a forked version of bontmia , which stands for \"Backup Over Network To Multiple Incremental Archives\" BorgBackup , a deduplicating backup program supporting compression and authenticated encryption. several internal scripts to pilot LVM snapshots/backup/restore operations","title":"Backup Tools"},{"location":"data/encryption/","text":"Sensitive Data Protection \u00b6 The advent of the EU General Data Protection Regulation ( GDPR ) permitted to highlight the need to protect sensitive information from leakage. GPG \u00b6 A basic approach relies on GPG to encrypt single files -- see this tutorial for more details # File encryption $ gpg --encrypt [ -r <recipient> ] <file> # => produces <file>.gpg $ rm -f <file> # /!\\ WARNING: encryption DOES NOT delete the input (clear-text) file $ gpg --armor --detach-sign <file> # Generate signature file <file>.asc # Decryption $ gpg --verify <file>.asc # (eventually but STRONGLY encouraged) verify signature file $ gpg --decrypt <file>.gpg # Decrypt PGP encrypted file One drawback is that files need to be completely decrypted for processing Tutorial: Using GnuPG aka Gnu Privacy Guard aka GPG File Encryption Frameworks (EncFS, GoCryptFS...) \u00b6 In contrast to disk-encryption software that operate on whole disks (TrueCrypt, dm-crypt etc), file encryption operates on individual files that can be backed up or synchronised easily, especially within a Git repository. Comparison matrix gocryptfs , aspiring successor of EncFS written in Go EncFS , mature with known security issues eCryptFS , integrated into the Linux kernel Cryptomator , strong cross-platform support through Java and WebDAV securefs , a cross-platform project implemented in C++. CryFS , result of a master thesis at the KIT University that uses chunked storage to obfuscate file sizes. Assuming you are working from /path/to/my/project , your workflow (mentionned below for EncFS, but it can be adpated to all the other tools) operated on encrypted vaults and would be as follows: ( eventually ) if operating within a working copy of a git repository, you should ignore the mounting directory (ex: vault/* ) in the root .gitignore of the repository this ensures neither you nor a collaborator will commit any unencrypted version of a file by mistake you commit only the EncFS / GocryptFS / eCryptFS / Cryptomator / securefs / CryFS raw directory (ex: .crypt/ ) in your repository. Thus only encrypted form or your files are commited You create the EncFS / GocryptFS / eCryptFS / Cryptomator / securefs / CryFS encrypted vault You prepare macros/scripts/Makefile/Rakefile tasks to lock/unlock the vault on demand Here are for instance a few example of these operations in live to create a encrypted vault: EncFS $ cd /path/to/my/project $ rawdir = .crypt # /!\\ ADAPT accordingly $ mountdir = vault # /!\\ ADAPT accordingly # # (eventually) Ignore the mount dir $ echo $mountdir >> .gitignore ### EncFS: Creation of an EncFS vault (only once) $ encfs --standard $rawdir $mountdir GoCryptFS you SHOULD be on a computing node to use GoCryptFS . $ cd /path/to/my/project $ rawdir = .crypt # /!\\ ADAPT accordingly $ mountdir = vault # /!\\ ADAPT accordingly # # (eventually) Ignore the mount dir $ echo $mountdir >> .gitignore ### GoCryptFS: load the module - you SHOULD be on a computing node $ module load tools/gocryptfs # Creation of a GoCryptFS vault (only once) $> gocryptfs -init $rawdir Then you can mount/unmount the vault as follows: Tool OS Opening/Unlocking the vault Closing/locking the vault EncFS Linux encfs -o nonempty --idle=60 $rawdir $mountdir fusermount -u $mountdir EncFS Mac OS encfs --idle=60 $rawdir $mountdir umount $mountdir GocryptFS gocryptfs $rawdir $mountdir as above The fact that GoCryptFS is available as a module brings the advantage that it can be mounted in a view folder ( vault/ ) where you can read and write the unencrypted files, which is Automatically unmounted upon job termination. File Encryption using SSH [RSA] Key Pairs \u00b6 Man pages: openssl rsa , openssl rsautl and openssl enc Tutorial: Encryption with RSA Key Pairs Tutorial: How to encrypt a big file using OpenSSL and someone's public key OpenSSL Command-Line HOWTO , in particular the section 'How do I simply encrypt a file?' If you encrypt/decrypt files or messages on more than a one-off occasion, you should really use GnuPGP as that is a much better suited tool for this kind of operations. But if you already have someone's public SSH key, it can be convenient to use it, and it is safe. Warning The below instructions are NOT compliant with the new OpenSSH format which is used for storing encrypted (or unencrypted) RSA, EcDSA and Ed25519 keys (among others) when you use the -o option of ssh-keygen . You can recognize these keys by the fact that the private SSH key ~/.ssh/id_rsa starts with - ----BEGIN OPENSSH PRIVATE KEY----- Encrypt a file using a public SSH key \u00b6 (eventually) SSH RSA public key conversion to PEM PKCS8 OpenSSL encryption/decryption operations performed using the RSA algorithm relies on keys following the PEM format 1 (ideally in the PKCS#8 format). It is possible to convert OpenSSH public keys (private ones are already compliant) to the PEM PKCS8 format (a more secure format). For that one can either use the ssh-keygen or the openssl commands, the first one being recomm ended. # Convert the public key of your collaborator to the PEM PKCS8 format (a more secure format) $ ssh-keygen -f id_dst_rsa.pub -e -m pkcs8 > id_dst_rsa.pkcs8.pub # OR use OpenSSL for that... $ openssl rsa -in id_dst_rsa -pubout -outform PKCS8 > id_dst_rsa.pkcs8.pub Note that you don't actually need to save the PKCS#8 version of his public key file -- the below command will make this conversion on demand. Generate a 256 bit (32 byte) random symmetric key There is a limit to the maximum length of a message i.e. size of a file that can be encrypted using asymmetric RSA public key encryption keys (which is what SSH ke ys are). For this reason, you should better rely on a 256 bit key to use for symmetric AES encryption and then encrypt/decrypt that symmetric AES key with the asymmetric RSA k eys This is how encrypted connections usually work, by the way. Generate the unique symmetric key key.bin of 32 bytes ( i.e. 256 bit) as follows: openssl rand -base64 32 -out key.bin You should only use this key once . If you send something else to the recipient at another time, you should regenerate another key. Encrypt the (potentially big) file with the symmetric key openssl enc -aes-256-cbc -salt -in bigdata.dat -out bigdata.dat.enc -pass file:./key.bin Indicative performance of OpenSSL Encryption time You can quickly generate random files of 1 or 10 GiB size as follows: # Random generation of a 1GiB file $ dd if = /dev/urandom of = bigfile_1GiB.dat bs = 64M count = 16 iflag = fullblock # Random generation of a 1GiB file $ dd if = /dev/urandom of = bigfile_10GiB.dat bs = 64M count = 160 iflag = fullblock An indicated encryption time taken for above generated random file on a local laptop (Mac OS X, local filesystem over SSD) is proposed in the below table, using openssl enc -aes-256-cbc -salt -in bigfile_<N>GiB.dat -out bigfile_<N>GiB.dat.enc -pass file:./key.bin File size Encryption time bigfile_1GiB.dat 1 GiB 0m5.395s bigfile_10GiB.dat 10 GiB 2m50.214s Encrypt the symmetric key, using your collaborator public SSH key in PKCS8 format: $ openssl rsautl -encrypt -pubin -inkey < ( ssh-keygen -e -m PKCS8 -f id_dst_rsa.pub ) -in key.bin -out key.bin.enc # OR, if you have a copy of the PKCS#8 version of his public key $ openssl rsautl -encrypt -pubin -inkey id_dst_rsa.pkcs8.pub -in key.bin -out key.bin.enc Delete the unencrypted symmetric key as you don't need it any more (and you should not use it anymore) $> rm key.bin Now you can transfer the *.enc files i.e. send the (potentially big) encrypted file <file>.enc and the encrypted symmetric key ( i.e. key.bin.enc ) to the recipient _i.e. your collaborator. Note that you are encouraged to send the encrypted file and the encrypted key separately. Although it's not absolutely necessary, it's good practice to separate the two. If you're allowed to, transfer them by SSH to an agreed remote server. It is even safe to upload the files to a public file sharing service and tell the recipient to download them from there. Decrypt a file encrypted with a public SSH key \u00b6 First decrypt the symmetric key using the SSH private counterpart: # Decrypt the key -- /!\\ ADAPT the path to the private SSH key $ openssl rsautl -decrypt -inkey ~/.ssh/id_rsa -in key.bin.enc -out key.bin Enter pass phrase for ~/.ssh/id_rsa: Now the (potentially big) file can be decrypted, using the symmetric key: openssl enc -d -aes-256-cbc -in bigdata.dat.enc -out bigdata.dat -pass file:./key.bin Misc Q&D for small files \u00b6 For a 'quick and dirty' encryption/decryption of small files: # Encrypt $ openssl rsautl -encrypt -inkey < ( ssh-keygen -e -m PKCS8 -f ~/.ssh/id_rsa.pub ) -pubin -in <cleartext_file>.dat -out <encrypted_file>.dat.enc # Decrypt $ openssl rsautl -decrypt -inkey ~/.ssh/id_rsa -in <encrypted_file>.dat.enc -out <cleartext_file>.dat Data Encryption in Git Repository with git-crypt \u00b6 It is of course even more important in the context of git repositories , whether public or private, since the disposal of a working copy of the repository enable the access to the full history of commits, in particular the ones eventually done by mistake ( git commit -a ) that used to include sensitive files. That's where git-crypt comes for help. It is an open source, command line utility that empowers developers to protect specific files within a git repository. git-crypt enables transparent encryption and decryption of files in a git repository. Files which you choose to protect are encrypted when committed, and decrypted when checked out. git-crypt lets you freely share a repository containing a mix of public and private content. git-crypt gracefully degrades, so developers without the secret key can still clone and commit to a repository with encrypted files. This lets you store your secret material (such as keys or passwords) in the same repository as your code, without requiring you to lock down your entire repository. The biggest advantage of git-crypt is that private data and public data can live in the same location. Using Git-crypt to Protect Sensitive Data PetaSuite Protect \u00b6 PetaSuite is a compression suite for Next-Generation-Sequencing (NGS) data. It consists of a command-line tool and a user-mode library. The command line tool performs compression and decompression operations on files. The user-mode library allows other tools and pipelines to transparently access the NGS data in their original file formats. PetaSuite is used within LCSB and provides the following features: Encrypt and compress genomic data Encryption keys and access managed centrally Decryption and decompression on-the-fly using a library that intercepts all FS access This is a commercial software -- contact lcsb.software@uni.lu if you would like to use it Defined in RFCs 1421 through 1424 , is a container format for public/private keys or certificates used preferentially by open-source software such as OpenSSL . The name is from Privacy Enhanced Mail (PEM) (a failed method for secure email, but the container format it used lives on, and is a base64 translation of the x509 ASN.1 keys. \u21a9","title":"Sensitive Data Protection"},{"location":"data/encryption/#sensitive-data-protection","text":"The advent of the EU General Data Protection Regulation ( GDPR ) permitted to highlight the need to protect sensitive information from leakage.","title":"Sensitive Data Protection"},{"location":"data/encryption/#gpg","text":"A basic approach relies on GPG to encrypt single files -- see this tutorial for more details # File encryption $ gpg --encrypt [ -r <recipient> ] <file> # => produces <file>.gpg $ rm -f <file> # /!\\ WARNING: encryption DOES NOT delete the input (clear-text) file $ gpg --armor --detach-sign <file> # Generate signature file <file>.asc # Decryption $ gpg --verify <file>.asc # (eventually but STRONGLY encouraged) verify signature file $ gpg --decrypt <file>.gpg # Decrypt PGP encrypted file One drawback is that files need to be completely decrypted for processing Tutorial: Using GnuPG aka Gnu Privacy Guard aka GPG","title":"GPG"},{"location":"data/encryption/#file-encryption-frameworks-encfs-gocryptfs","text":"In contrast to disk-encryption software that operate on whole disks (TrueCrypt, dm-crypt etc), file encryption operates on individual files that can be backed up or synchronised easily, especially within a Git repository. Comparison matrix gocryptfs , aspiring successor of EncFS written in Go EncFS , mature with known security issues eCryptFS , integrated into the Linux kernel Cryptomator , strong cross-platform support through Java and WebDAV securefs , a cross-platform project implemented in C++. CryFS , result of a master thesis at the KIT University that uses chunked storage to obfuscate file sizes. Assuming you are working from /path/to/my/project , your workflow (mentionned below for EncFS, but it can be adpated to all the other tools) operated on encrypted vaults and would be as follows: ( eventually ) if operating within a working copy of a git repository, you should ignore the mounting directory (ex: vault/* ) in the root .gitignore of the repository this ensures neither you nor a collaborator will commit any unencrypted version of a file by mistake you commit only the EncFS / GocryptFS / eCryptFS / Cryptomator / securefs / CryFS raw directory (ex: .crypt/ ) in your repository. Thus only encrypted form or your files are commited You create the EncFS / GocryptFS / eCryptFS / Cryptomator / securefs / CryFS encrypted vault You prepare macros/scripts/Makefile/Rakefile tasks to lock/unlock the vault on demand Here are for instance a few example of these operations in live to create a encrypted vault: EncFS $ cd /path/to/my/project $ rawdir = .crypt # /!\\ ADAPT accordingly $ mountdir = vault # /!\\ ADAPT accordingly # # (eventually) Ignore the mount dir $ echo $mountdir >> .gitignore ### EncFS: Creation of an EncFS vault (only once) $ encfs --standard $rawdir $mountdir GoCryptFS you SHOULD be on a computing node to use GoCryptFS . $ cd /path/to/my/project $ rawdir = .crypt # /!\\ ADAPT accordingly $ mountdir = vault # /!\\ ADAPT accordingly # # (eventually) Ignore the mount dir $ echo $mountdir >> .gitignore ### GoCryptFS: load the module - you SHOULD be on a computing node $ module load tools/gocryptfs # Creation of a GoCryptFS vault (only once) $> gocryptfs -init $rawdir Then you can mount/unmount the vault as follows: Tool OS Opening/Unlocking the vault Closing/locking the vault EncFS Linux encfs -o nonempty --idle=60 $rawdir $mountdir fusermount -u $mountdir EncFS Mac OS encfs --idle=60 $rawdir $mountdir umount $mountdir GocryptFS gocryptfs $rawdir $mountdir as above The fact that GoCryptFS is available as a module brings the advantage that it can be mounted in a view folder ( vault/ ) where you can read and write the unencrypted files, which is Automatically unmounted upon job termination.","title":"File Encryption Frameworks (EncFS, GoCryptFS...)"},{"location":"data/encryption/#file-encryption-using-ssh-rsa-key-pairs","text":"Man pages: openssl rsa , openssl rsautl and openssl enc Tutorial: Encryption with RSA Key Pairs Tutorial: How to encrypt a big file using OpenSSL and someone's public key OpenSSL Command-Line HOWTO , in particular the section 'How do I simply encrypt a file?' If you encrypt/decrypt files or messages on more than a one-off occasion, you should really use GnuPGP as that is a much better suited tool for this kind of operations. But if you already have someone's public SSH key, it can be convenient to use it, and it is safe. Warning The below instructions are NOT compliant with the new OpenSSH format which is used for storing encrypted (or unencrypted) RSA, EcDSA and Ed25519 keys (among others) when you use the -o option of ssh-keygen . You can recognize these keys by the fact that the private SSH key ~/.ssh/id_rsa starts with - ----BEGIN OPENSSH PRIVATE KEY-----","title":"File Encryption using SSH [RSA] Key Pairs"},{"location":"data/encryption/#encrypt-a-file-using-a-public-ssh-key","text":"(eventually) SSH RSA public key conversion to PEM PKCS8 OpenSSL encryption/decryption operations performed using the RSA algorithm relies on keys following the PEM format 1 (ideally in the PKCS#8 format). It is possible to convert OpenSSH public keys (private ones are already compliant) to the PEM PKCS8 format (a more secure format). For that one can either use the ssh-keygen or the openssl commands, the first one being recomm ended. # Convert the public key of your collaborator to the PEM PKCS8 format (a more secure format) $ ssh-keygen -f id_dst_rsa.pub -e -m pkcs8 > id_dst_rsa.pkcs8.pub # OR use OpenSSL for that... $ openssl rsa -in id_dst_rsa -pubout -outform PKCS8 > id_dst_rsa.pkcs8.pub Note that you don't actually need to save the PKCS#8 version of his public key file -- the below command will make this conversion on demand. Generate a 256 bit (32 byte) random symmetric key There is a limit to the maximum length of a message i.e. size of a file that can be encrypted using asymmetric RSA public key encryption keys (which is what SSH ke ys are). For this reason, you should better rely on a 256 bit key to use for symmetric AES encryption and then encrypt/decrypt that symmetric AES key with the asymmetric RSA k eys This is how encrypted connections usually work, by the way. Generate the unique symmetric key key.bin of 32 bytes ( i.e. 256 bit) as follows: openssl rand -base64 32 -out key.bin You should only use this key once . If you send something else to the recipient at another time, you should regenerate another key. Encrypt the (potentially big) file with the symmetric key openssl enc -aes-256-cbc -salt -in bigdata.dat -out bigdata.dat.enc -pass file:./key.bin Indicative performance of OpenSSL Encryption time You can quickly generate random files of 1 or 10 GiB size as follows: # Random generation of a 1GiB file $ dd if = /dev/urandom of = bigfile_1GiB.dat bs = 64M count = 16 iflag = fullblock # Random generation of a 1GiB file $ dd if = /dev/urandom of = bigfile_10GiB.dat bs = 64M count = 160 iflag = fullblock An indicated encryption time taken for above generated random file on a local laptop (Mac OS X, local filesystem over SSD) is proposed in the below table, using openssl enc -aes-256-cbc -salt -in bigfile_<N>GiB.dat -out bigfile_<N>GiB.dat.enc -pass file:./key.bin File size Encryption time bigfile_1GiB.dat 1 GiB 0m5.395s bigfile_10GiB.dat 10 GiB 2m50.214s Encrypt the symmetric key, using your collaborator public SSH key in PKCS8 format: $ openssl rsautl -encrypt -pubin -inkey < ( ssh-keygen -e -m PKCS8 -f id_dst_rsa.pub ) -in key.bin -out key.bin.enc # OR, if you have a copy of the PKCS#8 version of his public key $ openssl rsautl -encrypt -pubin -inkey id_dst_rsa.pkcs8.pub -in key.bin -out key.bin.enc Delete the unencrypted symmetric key as you don't need it any more (and you should not use it anymore) $> rm key.bin Now you can transfer the *.enc files i.e. send the (potentially big) encrypted file <file>.enc and the encrypted symmetric key ( i.e. key.bin.enc ) to the recipient _i.e. your collaborator. Note that you are encouraged to send the encrypted file and the encrypted key separately. Although it's not absolutely necessary, it's good practice to separate the two. If you're allowed to, transfer them by SSH to an agreed remote server. It is even safe to upload the files to a public file sharing service and tell the recipient to download them from there.","title":"Encrypt a file using a public SSH key"},{"location":"data/encryption/#decrypt-a-file-encrypted-with-a-public-ssh-key","text":"First decrypt the symmetric key using the SSH private counterpart: # Decrypt the key -- /!\\ ADAPT the path to the private SSH key $ openssl rsautl -decrypt -inkey ~/.ssh/id_rsa -in key.bin.enc -out key.bin Enter pass phrase for ~/.ssh/id_rsa: Now the (potentially big) file can be decrypted, using the symmetric key: openssl enc -d -aes-256-cbc -in bigdata.dat.enc -out bigdata.dat -pass file:./key.bin","title":"Decrypt a file encrypted with a public SSH key"},{"location":"data/encryption/#misc-qd-for-small-files","text":"For a 'quick and dirty' encryption/decryption of small files: # Encrypt $ openssl rsautl -encrypt -inkey < ( ssh-keygen -e -m PKCS8 -f ~/.ssh/id_rsa.pub ) -pubin -in <cleartext_file>.dat -out <encrypted_file>.dat.enc # Decrypt $ openssl rsautl -decrypt -inkey ~/.ssh/id_rsa -in <encrypted_file>.dat.enc -out <cleartext_file>.dat","title":"Misc Q&amp;D for small files"},{"location":"data/encryption/#data-encryption-in-git-repository-with-git-crypt","text":"It is of course even more important in the context of git repositories , whether public or private, since the disposal of a working copy of the repository enable the access to the full history of commits, in particular the ones eventually done by mistake ( git commit -a ) that used to include sensitive files. That's where git-crypt comes for help. It is an open source, command line utility that empowers developers to protect specific files within a git repository. git-crypt enables transparent encryption and decryption of files in a git repository. Files which you choose to protect are encrypted when committed, and decrypted when checked out. git-crypt lets you freely share a repository containing a mix of public and private content. git-crypt gracefully degrades, so developers without the secret key can still clone and commit to a repository with encrypted files. This lets you store your secret material (such as keys or passwords) in the same repository as your code, without requiring you to lock down your entire repository. The biggest advantage of git-crypt is that private data and public data can live in the same location. Using Git-crypt to Protect Sensitive Data","title":"Data Encryption in Git Repository with git-crypt"},{"location":"data/encryption/#petasuite-protect","text":"PetaSuite is a compression suite for Next-Generation-Sequencing (NGS) data. It consists of a command-line tool and a user-mode library. The command line tool performs compression and decompression operations on files. The user-mode library allows other tools and pipelines to transparently access the NGS data in their original file formats. PetaSuite is used within LCSB and provides the following features: Encrypt and compress genomic data Encryption keys and access managed centrally Decryption and decompression on-the-fly using a library that intercepts all FS access This is a commercial software -- contact lcsb.software@uni.lu if you would like to use it Defined in RFCs 1421 through 1424 , is a container format for public/private keys or certificates used preferentially by open-source software such as OpenSSL . The name is from Privacy Enhanced Mail (PEM) (a failed method for secure email, but the container format it used lives on, and is a base64 translation of the x509 ASN.1 keys. \u21a9","title":"PetaSuite Protect"},{"location":"data/gdpr/","text":"UL HPC Acceptable Use Policy (AUP) (pdf) Warning Personal data is/may be visible, accessible or handled: directly on the HPC clusters through Resource and Job Management System (RJMS) tools (Slurm) and associated monitoring interfaces through service portals (like OpenOnDemand ) on code management portals such GitLab , GitHub on secondary storage systems used within the University such as Atlas, DropIT, etc. Data Use \u00b6 Use of UL HPC data storage resources (file systems, data storage tiers, backup, etc.) should be used only for work directly related to the projects for which the resources were requested and granted, and primarily to advance University\u2019s missions of education and research. Use of UL HPC data resources for personal activities is prohibited. The UL HPC Team maintains up-to-date documentation on its data storage resources and their proper use, and provides regular training and support to users. Users assume the responsibility for following the documentation, training sessions and best practice guides in order to understand the proper and considerate use of the UL HPC data storage resources. Authors/generators/owners of information or data are responsible for its correct categorization as sensitive or non-sensitive. Owners of sensitive information are responsible for its secure handling, transmission, processing, storage, and disposal on the UL HPC systems. The UL HPC Team recommends use of encryption to protect the data from unauthorized access. Data Protection inquiries, especially as regards sensitive information processing can be directed to the Data Protection Officer: https://wwwen.uni.lu/university/data_protection/data_protection_officer . Users are prohibited from intentionally accessing, modifying or deleting data they do not own or have not been granted explicit permission to access. Users are responsible to ensure the appropriate level of protection, backup and integrity checks on their critical data and applications. It is their responsibility to set appropriate access controls for the data they bring, process and generate on UL HPC facilities. In the event of system failure or malicious actions, UL HPC makes no guarantee against loss of data or that user or project data can be recovered nor that it cannot be accessed, changed, or deleted by another individual. Personal information agreement \u00b6 UL HPC retains the right to monitor all activities on its facilities. Users acknowledge that data regarding their activity on UL HPC facilities will be collected. The data is collected (e.g. by the Slurm workload manager) for utilization accounting and reporting purposes, and for the purpose of understanding typical patterns of user\u2019s behavior on the system in order to further improve the services provided by UL HPC. Another goal is to identify intrusions, misuse, security incidents or illegal actions in order to protect UL HPC users and facilities.. Users agree that this data may be processed to extract information contributing to the above stated purposes. Users agree that their name, surname, email address, affiliation, work place and phone numbers are processed by the UL HPC Team in order to provide HPC and associated services. Further information about Data Protection can be found at: https://wwwen.uni.lu/university/data_protection Data Protection inquiries can be directed to the Data Protection Officer: https://wwwen.uni.lu/university/data_protection/data_protection_officer","title":"GDPR Compliance"},{"location":"data/gdpr/#data-use","text":"Use of UL HPC data storage resources (file systems, data storage tiers, backup, etc.) should be used only for work directly related to the projects for which the resources were requested and granted, and primarily to advance University\u2019s missions of education and research. Use of UL HPC data resources for personal activities is prohibited. The UL HPC Team maintains up-to-date documentation on its data storage resources and their proper use, and provides regular training and support to users. Users assume the responsibility for following the documentation, training sessions and best practice guides in order to understand the proper and considerate use of the UL HPC data storage resources. Authors/generators/owners of information or data are responsible for its correct categorization as sensitive or non-sensitive. Owners of sensitive information are responsible for its secure handling, transmission, processing, storage, and disposal on the UL HPC systems. The UL HPC Team recommends use of encryption to protect the data from unauthorized access. Data Protection inquiries, especially as regards sensitive information processing can be directed to the Data Protection Officer: https://wwwen.uni.lu/university/data_protection/data_protection_officer . Users are prohibited from intentionally accessing, modifying or deleting data they do not own or have not been granted explicit permission to access. Users are responsible to ensure the appropriate level of protection, backup and integrity checks on their critical data and applications. It is their responsibility to set appropriate access controls for the data they bring, process and generate on UL HPC facilities. In the event of system failure or malicious actions, UL HPC makes no guarantee against loss of data or that user or project data can be recovered nor that it cannot be accessed, changed, or deleted by another individual.","title":"Data Use"},{"location":"data/gdpr/#personal-information-agreement","text":"UL HPC retains the right to monitor all activities on its facilities. Users acknowledge that data regarding their activity on UL HPC facilities will be collected. The data is collected (e.g. by the Slurm workload manager) for utilization accounting and reporting purposes, and for the purpose of understanding typical patterns of user\u2019s behavior on the system in order to further improve the services provided by UL HPC. Another goal is to identify intrusions, misuse, security incidents or illegal actions in order to protect UL HPC users and facilities.. Users agree that this data may be processed to extract information contributing to the above stated purposes. Users agree that their name, surname, email address, affiliation, work place and phone numbers are processed by the UL HPC Team in order to provide HPC and associated services. Further information about Data Protection can be found at: https://wwwen.uni.lu/university/data_protection Data Protection inquiries can be directed to the Data Protection Officer: https://wwwen.uni.lu/university/data_protection/data_protection_officer","title":"Personal information agreement"},{"location":"data/layout/","text":"Global Directory Structure \u00b6 ULHPC File Systems Overview \u00b6 Several File Systems co-exist on the ULHPC facility and are configured for different purposes. Each servers and computational resources has access to at least three different file systems with different levels of performance, permanence and available space summarized below Directory Env. file system backup purging /home/users/<login> $HOME GPFS/Spectrumscale yes no /work/projects/ <name> - GPFS/Spectrumscale yes no /scratch/users/<login> $SCRATCH Lustre no yes /mnt/isilon/projects/<name> - OneFS yes* no Global Home directory $HOME \u00b6 Home directories provide a convenient means for a user to have access to files such as dotfiles, source files, input files, configuration files regardless of the platform. Refer to your home directory using the environment variable $HOME whenever possible. The absolute path may change, but the value of $HOME will always be correct. Global Project directory $PROJECTHOME=/work/projects/ \u00b6 Project directories are intended for sharing data within a group of researchers, under /work/projects/<name> Refer to your project base home directory using the environment variable $PROJECTHOME=/work/projects whenever possible. Global Scratch directory $SCRATCH \u00b6 The scratch area is a Lustre -based file system designed for high performance temporary storage of large files. It is thus intended to support large I/O for jobs that are being actively computed on the ULHPC systems. We recommend that you run your jobs, especially data intensive ones, from the ULHPC scratch file system. Refer to your scratch directory using the environment variable $SCRATCH whenever possible (which expands to /scratch/users/$(whoami) ). The scratch file system is shared via the Infiniband network of the ULHPC facility and is available from all nodes while being tuned for high performance. Project Cold-Data and Archives \u00b6 OneFS, A global low -performance Dell/EMC Isilon solution is used to host project data, and serve for backup and archival purposes. You will find them mounted under /mnt/isilon/projects .","title":"Global Directory Structure"},{"location":"data/layout/#global-directory-structure","text":"","title":"Global Directory Structure"},{"location":"data/layout/#ulhpc-file-systems-overview","text":"Several File Systems co-exist on the ULHPC facility and are configured for different purposes. Each servers and computational resources has access to at least three different file systems with different levels of performance, permanence and available space summarized below Directory Env. file system backup purging /home/users/<login> $HOME GPFS/Spectrumscale yes no /work/projects/ <name> - GPFS/Spectrumscale yes no /scratch/users/<login> $SCRATCH Lustre no yes /mnt/isilon/projects/<name> - OneFS yes* no","title":"ULHPC File Systems Overview"},{"location":"data/layout/#global-home-directory-home","text":"Home directories provide a convenient means for a user to have access to files such as dotfiles, source files, input files, configuration files regardless of the platform. Refer to your home directory using the environment variable $HOME whenever possible. The absolute path may change, but the value of $HOME will always be correct.","title":"Global Home directory $HOME"},{"location":"data/layout/#global-project-directory-projecthomeworkprojects","text":"Project directories are intended for sharing data within a group of researchers, under /work/projects/<name> Refer to your project base home directory using the environment variable $PROJECTHOME=/work/projects whenever possible.","title":"Global Project directory $PROJECTHOME=/work/projects/"},{"location":"data/layout/#global-scratch-directory-scratch","text":"The scratch area is a Lustre -based file system designed for high performance temporary storage of large files. It is thus intended to support large I/O for jobs that are being actively computed on the ULHPC systems. We recommend that you run your jobs, especially data intensive ones, from the ULHPC scratch file system. Refer to your scratch directory using the environment variable $SCRATCH whenever possible (which expands to /scratch/users/$(whoami) ). The scratch file system is shared via the Infiniband network of the ULHPC facility and is available from all nodes while being tuned for high performance.","title":"Global Scratch directory $SCRATCH"},{"location":"data/layout/#project-cold-data-and-archives","text":"OneFS, A global low -performance Dell/EMC Isilon solution is used to host project data, and serve for backup and archival purposes. You will find them mounted under /mnt/isilon/projects .","title":"Project Cold-Data and Archives"},{"location":"data/project/","text":"Global Project directory $PROJECTHOME=/work/projects/ \u00b6 Project directories are intended for sharing data within a group of researchers, under /work/projects/<name> Refer to your project base home directory using the environment variable $PROJECTHOME=/work/projects whenever possible. Research Project Allocations, Accounting and Reporting The Research Support and Accounting Departments of the University keep track of the list of research projects funded within the University. Starting 2021, a new procedure has been put in place to provide a detailed reporting of the HPC usage for such projects. As part of this process, the following actions are taken by the ULHPC team: a dedicated project account <name> (normally the acronym of the project) is created for accounting purpose at the Slurm level (L3 account - see Account Hierarchy ); a dedicated project directory with the same name ( <name> ) is created, allowing to share data within a group of project researchers, under $PROJECTHOME/<name> , i.e. , /work/projects/<name> You are then entitled to submit jobs associated to the project using -A <name> such that the HPC usage is reported accurately. The ULHPC team will provide to the project PI (Principal Investigator) and the Research Support department a regular report detailing the corresponding HPC usage. In all cases, job billing under the conditions defined in the Job Accounting and Billing section may apply. New project directory \u00b6 You can request a new project directory under ServiceNow (HPC \u2192 Storage & projects \u2192 Request for a new project). Quotas and Backup Policies \u00b6 See quotas for detailed information about inode, space quotas, and file system purge policies. Your projects backup directories are backuped weekly, according to the policy detailed in the ULHPC backup policies . Access rights to project directory: Quota for clusterusers group in project directories is 0 !!! When a project <name> is created, a group of the same name ( <name> ) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory. Be aware that your default group as a user is clusterusers which has ( on purpose ) a quota in project directories set to 0 . You thus need to ensure you always write data in your project directory using the <name> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: chmod g+s [...] When using rsync to transfer file toward the project directory /work/projects/<name> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to: give new files the destination-default permissions with --no-p ( --no-perms ), and use the default group <name> of the destination dir with --no-g ( --no-group ) (eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using --chmod=ug=rwX Your full rsync command becomes (adapt accordingly): rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] <source> /work/projects/<name>/[...] For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory /work/projects/<name> , you want to use the sg as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"<command> [...]\" This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"eb [...] -r --rebuild -D\" # Dry-run - enforce using the '<name>' group sg <name> -c \"eb [...] -r --rebuild\" # Dry-run - enforce using the '<name>' group Project directory modification \u00b6 You can request changes for your project directory (quotas extension, add/remove a group member) under ServiceNow : HPC \u2192 Storage & projects \u2192 Extend quota/Request information HPC \u2192 User access & accounts \u2192 Add/Remove user within project","title":"Project Data Management"},{"location":"data/project/#global-project-directory-projecthomeworkprojects","text":"Project directories are intended for sharing data within a group of researchers, under /work/projects/<name> Refer to your project base home directory using the environment variable $PROJECTHOME=/work/projects whenever possible. Research Project Allocations, Accounting and Reporting The Research Support and Accounting Departments of the University keep track of the list of research projects funded within the University. Starting 2021, a new procedure has been put in place to provide a detailed reporting of the HPC usage for such projects. As part of this process, the following actions are taken by the ULHPC team: a dedicated project account <name> (normally the acronym of the project) is created for accounting purpose at the Slurm level (L3 account - see Account Hierarchy ); a dedicated project directory with the same name ( <name> ) is created, allowing to share data within a group of project researchers, under $PROJECTHOME/<name> , i.e. , /work/projects/<name> You are then entitled to submit jobs associated to the project using -A <name> such that the HPC usage is reported accurately. The ULHPC team will provide to the project PI (Principal Investigator) and the Research Support department a regular report detailing the corresponding HPC usage. In all cases, job billing under the conditions defined in the Job Accounting and Billing section may apply.","title":"Global Project directory $PROJECTHOME=/work/projects/"},{"location":"data/project/#new-project-directory","text":"You can request a new project directory under ServiceNow (HPC \u2192 Storage & projects \u2192 Request for a new project).","title":"New project directory"},{"location":"data/project/#quotas-and-backup-policies","text":"See quotas for detailed information about inode, space quotas, and file system purge policies. Your projects backup directories are backuped weekly, according to the policy detailed in the ULHPC backup policies . Access rights to project directory: Quota for clusterusers group in project directories is 0 !!! When a project <name> is created, a group of the same name ( <name> ) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory. Be aware that your default group as a user is clusterusers which has ( on purpose ) a quota in project directories set to 0 . You thus need to ensure you always write data in your project directory using the <name> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: chmod g+s [...] When using rsync to transfer file toward the project directory /work/projects/<name> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to: give new files the destination-default permissions with --no-p ( --no-perms ), and use the default group <name> of the destination dir with --no-g ( --no-group ) (eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using --chmod=ug=rwX Your full rsync command becomes (adapt accordingly): rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] <source> /work/projects/<name>/[...] For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory /work/projects/<name> , you want to use the sg as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"<command> [...]\" This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"eb [...] -r --rebuild -D\" # Dry-run - enforce using the '<name>' group sg <name> -c \"eb [...] -r --rebuild\" # Dry-run - enforce using the '<name>' group","title":"Quotas and Backup Policies"},{"location":"data/project/#project-directory-modification","text":"You can request changes for your project directory (quotas extension, add/remove a group member) under ServiceNow : HPC \u2192 Storage & projects \u2192 Extend quota/Request information HPC \u2192 User access & accounts \u2192 Add/Remove user within project","title":"Project directory modification"},{"location":"data/project_acl/","text":"Global Project quotas and backup policies See quotas for detailed information about inode, space quotas, and file system purge policies. Your projects backup directories are backuped weekly, according to the policy detailed in the ULHPC backup policies . Access rights to project directory: Quota for clusterusers group in project directories is 0 !!! When a project <name> is created, a group of the same name ( <name> ) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory. Be aware that your default group as a user is clusterusers which has ( on purpose ) a quota in project directories set to 0 . You thus need to ensure you always write data in your project directory using the <name> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: chmod g+s [...] When using rsync to transfer file toward the project directory /work/projects/<name> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to: give new files the destination-default permissions with --no-p ( --no-perms ), and use the default group <name> of the destination dir with --no-g ( --no-group ) (eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using --chmod=ug=rwX Your full rsync command becomes (adapt accordingly): rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] <source> /work/projects/<name>/[...] For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory /work/projects/<name> , you want to use the sg as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"<command> [...]\" This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"eb [...] -r --rebuild -D\" # Dry-run - enforce using the '<name>' group sg <name> -c \"eb [...] -r --rebuild\" # Dry-run - enforce using the '<name>' group","title":"Project acl"},{"location":"data/sharing/","text":"Security and Data Integrity \u00b6 Sharing data with other users must be done carefully. Permissions should be set to the minimum necessary to achieve the desired access. For instance, consider carefully whether it's really necessary before sharing write permssions on data. Be sure to have archived backups of any critical shared data. It is also important to ensure that private login secrets (such as SSH private keys or apache htaccess files) are NOT shared with other users (either intentionally or accidentally). Good practice is to keep things like this in a separare directory that is as locked down as possible. The very first protection is to maintain your Home with access rights 700 chmod 700 $HOME Sharing Data within ULHPC Facility \u00b6 Sharing with Other Members of Your Project \u00b6 We can setup a project directory with specific group read and write permissions, allowing to share data with other members of your project. Sharing with ULHPC Users Outside of Your Project \u00b6 Unix File Permissions \u00b6 You can share files and directories with ULHPC users outside of your project by adjusting the unix file permissions. We have an extensive write up of unix file permissions and how they work here . Sharing Data outside of ULHPC \u00b6 The IT service of the University can be contacted to easily and quickly share data over the web using a dedicated Data Transfer service. Open the appropriate ticket on the Service Now portal.","title":"Data Sharing"},{"location":"data/sharing/#security-and-data-integrity","text":"Sharing data with other users must be done carefully. Permissions should be set to the minimum necessary to achieve the desired access. For instance, consider carefully whether it's really necessary before sharing write permssions on data. Be sure to have archived backups of any critical shared data. It is also important to ensure that private login secrets (such as SSH private keys or apache htaccess files) are NOT shared with other users (either intentionally or accidentally). Good practice is to keep things like this in a separare directory that is as locked down as possible. The very first protection is to maintain your Home with access rights 700 chmod 700 $HOME","title":"Security and Data Integrity"},{"location":"data/sharing/#sharing-data-within-ulhpc-facility","text":"","title":"Sharing Data within ULHPC Facility"},{"location":"data/sharing/#sharing-with-other-members-of-your-project","text":"We can setup a project directory with specific group read and write permissions, allowing to share data with other members of your project.","title":"Sharing with Other Members of Your Project"},{"location":"data/sharing/#sharing-with-ulhpc-users-outside-of-your-project","text":"","title":"Sharing with ULHPC Users Outside of Your Project"},{"location":"data/sharing/#unix-file-permissions","text":"You can share files and directories with ULHPC users outside of your project by adjusting the unix file permissions. We have an extensive write up of unix file permissions and how they work here .","title":"Unix File Permissions"},{"location":"data/sharing/#sharing-data-outside-of-ulhpc","text":"The IT service of the University can be contacted to easily and quickly share data over the web using a dedicated Data Transfer service. Open the appropriate ticket on the Service Now portal.","title":"Sharing Data outside of ULHPC"},{"location":"data/transfer/","text":"Data Transfer to/from/within UL HPC Clusters \u00b6 Introduction \u00b6 Directories such as $HOME , $WORK or $SCRATCH are shared among the nodes of the cluster that you are using (including the login node) via shared filesystems (SpectrumScale, Lustre) meaning that: every file/directory pushed or created on the login node is available on the computing nodes every file/directory pushed or created on the computing nodes is available on the login node The two most common commands you can use for data transfers over SSH: scp : for the full transfer of files and directories (only works fine for single files or directories of small/trivial size) rsync : a software application which synchronizes files and directories from one location to another while minimizing data transfer as only the outdated or inexistent elements are transferred (practically required for lengthy complex transfers, which are more likely to be interrupted in the middle). scp or rsync? While both ensure a secure transfer of the data within an encrypted tunnel, rsync should be preferred : as mentionned in the from openSSH 8.0 release notes : \" The scp protocol is outdated , inflexible and not readily fixed . We recommend the use of more modern protocols like sftp and rsync for file transfer instead \". scp is also relatively slow when compared to rsync as exhibited for instance in the below sample Distem experience: You will find below notes on scp usage, but kindly prefer to use rsync . Consider scp as deprecated! Click nevertheless to get usage details scp (see scp(1) ) or secure copy is probably the easiest of all the methods. The basic syntax is as follows: scp [-P 8022] [-Cr] source_path destination_path the -P option specifies the SSH port to use (in this case 8022) the -C option activates the compression (actually, it passes the -C flag to ssh(1) to enable compression). the -r option states to recursively copy entire directories (in this case, scp follows symbolic links encountered in the tree traversal). Please note that in this case, you must specify the source file as a directory for this to work. The syntax for declaring a remote path is as follows on the cluster: yourlogin@iris-cluster:path/from/homedir Transfer from your local machine to the remote cluster login node For instance, let's assume you have a local directory ~/devel/myproject you want to transfer to the cluster, in your remote homedir. # /!\\ ADAPT yourlogin to... your ULHPC login $> scp -P 8022 -r ~/devel/myproject yourlogin@iris-cluster: This will transfer recursively your local directory ~/devel/myproject on the cluster login node (in your homedir). Note that if you configured (as advised elsewhere) the SSH connection in your ~/.ssh/config file, you can use a much simpler syntax: $> scp -r ~/devel/myproject iris-cluster: Transfer from the remote cluster front-end to your local machine Conversely, let's assume you want to retrieve the files ~/experiments/parallel_run/* $> scp -P 8022 yourlogin@iris-cluster:experiments/parallel_run/* /path/to/local/directory Again, if you configured the SSH connection in your ~/.ssh/config file, you can use a simpler syntax: $> scp iris-cluster:experiments/parallel_run/* /path/to/local/directory See the scp(1) man page or man scp for more details. Danger scp SHOULD NOT be used in the following cases: When you are copying more than a few files, as scp spawns a new process for each file and can be quite slow and resource intensive when copying a large number of files. When using the -r switch, scp does not know about symbolic links and will blindly follow them, even if it has already made a copy of the file. That can lead to scp copying an infinite amount of data and can easily fill up your hard disk (or worse, a system shared disk), so be careful. N.B. There are many alternative ways to transfer files in HPC platforms and you should check your options according to the problem at hand. Windows and OS X users may wish to transfer files from their systems to the clusters' login nodes with easy-to-use GUI applications such as: WinSCP (Windows only) FileZilla Client (Windows, OS X) Cyberduck (Windows, OS X) These applications will need to be configured to connect to the frontends with the same parameters as discussed on the SSH access page . Using rsync \u00b6 The clever alternative to scp is rsync , which has the advantage of transferring only the files which differ between the source and the destination. This feature is often referred to as fast incremental file transfer. Additionally, symbolic links can be preserved. The typical syntax of rsync (see rsync(1) ) for the cluster is similar to the one of scp : # /!\\ ADAPT </path/to/source> and </path/to/destination> # From LOCAL directory (/path/to/local/source) toward REMOTE server <hostname> rsync --rsh = 'ssh -p 8022' -avzu /path/to/local/source [ user@ ] hostname:/path/to/destination # Ex: from REMOTE server <hostname> to LOCAL directory rsync --rsh = 'ssh -p 8022' -avzu [ user@ ] hostname:/path/to/source /path/to/local/destination the --rsh option specifies the connector to use (here SSH on port 8022) the -a option corresponds to the \"Archive\" mode. Most likely you should always keep this on as it preserves file permissions and does not follow symlinks. the -v option enables the verbose mode the -z option enable compression, this will compress each file as it gets sent over the pipe. This can greatly decrease time, depending on what sort of files you are copying. the -u option (or --update ) corresponds to an updating process which skips files that are newer on the receiver. At this level, you may prefer the more dangerous option --delete that deletes extraneous files from dest dirs. Just like scp , the syntax for qualifying a remote path is as follows on the cluster: yourlogin@iris-cluster:path/from/homedir Transfer from your local machine to the remote cluster \u00b6 Coming back to the previous examples, let's assume you have a local directory ~/devel/myproject you want to transfer to the cluster, in your remote homedir. In that case: # /!\\ ADAPT yourlogin to... your ULHPC login $> rsync --rsh = 'ssh -p 8022' -avzu ~/devel/myproject yourlogin@access-iris.uni.lu: This will synchronize your local directory ~/devel/myproject on the cluster front-end (in your homedir). Transfer to Iris, Aion or both? The above example target the access server of Iris. Actually, you could have targetted the access server of Aion: it doesn't matter since the storage is SHARED between both clusters. Note that if you configured (as advised above) your SSH connection in your ~/.ssh/config file with a dedicated SSH entry {iris,aion}-cluster , you can use a simpler syntax: $> rsync -avzu ~/devel/myproject iris-cluster: # OR (it doesn't matter) $> rsync -avzu ~/devel/myproject aion-cluster: Transfer from your local machine to a project directory on the remote cluster \u00b6 When transferring data to a project directory you should keep the group and group permissions imposed by the project directory and quota. Therefore you need to add the options --no-p --no-g to your rsync command: $> rsync -avP --no-p --no-g ~/devel/myproject iris-cluster:/work/projects/myproject/ Transfer from the remote cluster to your local machine \u00b6 Conversely, let's assume you want to synchronize (retrieve) the remote files ~/experiments/parallel_run/* on your local machine: # /!\\ ADAPT yourlogin to... your ULHPC login $> rsync --rsh = 'ssh -p 8022' -avzu yourlogin@access-iris.uni.lu:experiments/parallel_run /path/to/local/directory Again, if you configured the SSH connection in your ~/.ssh/config file, you can use a simpler syntax: $> rsync -avzu iris-cluster:experiments/parallel_run /path/to/local/directory # OR (it doesn't matter) $> rsync -avzu aion-cluster:experiments/parallel_run /path/to/local/directory As always, see the man page or man rsync for more details. Data Transfer within Project directories \u00b6 The ULHPC facility features a Global Project directory $PROJECTHOME hosted within the GPFS/SpecrumScale file-system. You have to pay a particular attention when using rsync to transfer data within your project directory as depicted below. Access rights to project directory: Quota for clusterusers group in project directories is 0 !!! When a project <name> is created, a group of the same name ( <name> ) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory. Be aware that your default group as a user is clusterusers which has ( on purpose ) a quota in project directories set to 0 . You thus need to ensure you always write data in your project directory using the <name> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: chmod g+s [...] When using rsync to transfer file toward the project directory /work/projects/<name> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to: give new files the destination-default permissions with --no-p ( --no-perms ), and use the default group <name> of the destination dir with --no-g ( --no-group ) (eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using --chmod=ug=rwX Your full rsync command becomes (adapt accordingly): rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] <source> /work/projects/<name>/[...] For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory /work/projects/<name> , you want to use the sg as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"<command> [...]\" This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"eb [...] -r --rebuild -D\" # Dry-run - enforce using the '<name>' group sg <name> -c \"eb [...] -r --rebuild\" # Dry-run - enforce using the '<name>' group Using MobaXterm (Windows) \u00b6 If you are under Windows and you have MobaXterm installed and configured , you probably want to use it to transfer your files to the clusters. Here are the steps to use rsync inside MobaXterm in Windows. Warning Be aware that you SHOULD enable MobaXterm SSH Agent -- see SSH Agent instructions for more instructions. Using a local bash, transfer your files \u00b6 Open a local \"bash\" shell. Click on Start local terminal on the welcome page of MobaXterm. Find the location of the files you want to transfer. They should be located under /drives/<name of your disk> . You will have to use the Linux command line to move from one directory to the other. The cd command is used to change the current directory and ls to list files. For example, if your files are under C:\\\\Users\\janedoe\\Downloads\\ you should then go to /drives/c/Users/janedoe/Downloads/ with this command: cd /drives/c/Users/janedoe/Downloads/ Then list the files with ls command. You should see the list of your data files. When you have retrieved the location of your files, we can begin the transfer with rsync . For example /drives/c/Users/janedoe/Downloads/ (watch out, there is no / character at the end of the path, it is important). Launch the command rsync with this parameters to transfer all the content of the Downloads directory to the /isilon/projects/market_data/ directory on the cluster (the syntax is very important, be careful) rsync -avzpP -e \"ssh -p 8022\" /drives/c/Users/janedoe/Downloads/ yourlogin@access-iris.uni.lu:/isilon/projects/market_data/ You should see the output of transfer in progress. Wait for it to finish (it can be very long). Interrupt and resume a transfer in progress \u00b6 If you want to interrupt the transfer to resume it later, press Ctrl-C and exit MobaXterm. To resume a transfer, go in the right location and execute the rsync command again. Only the files that have not been transferred will be transferred again. Alternative approaches \u00b6 You can also consider alternative approaches to synchronize data with the cluster login node: rely on a versioning system such as GIT ; this approach works well for source code trees. mount your remote homedir by SSHFS . On Mac OS X, you should consider installing MacFusion for this purpose - on classical Linux system, just use the command-line sshfs or, mc . see below for details you can also rely on GUI tools like FileZilla , Cyberduck or WindSCP or any paid alternative (like ExpanDrive or ForkLift 3 ) SSHFS \u00b6 Linux # Debian-like sudo apt-get install sshfs # RHEL-like sudo yum install sshfs You may need to add yourself to the fuse group. Mac OS X # Assuming HomeBrew -- see https://brew.sh brew install osxfuse sshfs You can also directly install macFUSE from https://osxfuse.github.io/ You must reboot for the installation of osxfuse to take effect. You can then update to the latest version SSHFS allows any user to remotely mount their ULHPC home directory onto a local workstation through an ssh connection. The CLI format is as follows: sshfs [user@]host:[dir] mountpoint [options] Proceed as follows ( assuming you have a working SSH connection ): # Create a local directory hosting the mountng point mkdir -p ~/ulhpc # /!\\ ADAPT accordingly to match your taste sshfs iris-cluster: ~/ulhpc -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache = no -onolocalcaches # General options: # allow_other: Allow other users than the mounter (i.e. root) to access the share # reconnect: try to reconnnect # Optional options to be more \"Mac-like\": # -ocache=no # -onolocalcaches # -o volname=ulhpc_home Name of the volume in Finder Later on (once you no longer need it), you MUST unmount your remote FS # Linux fusermount -u ~/ulhpc # Mac OS X diskutil umount ~/ulhpc Special transfers \u00b6 Sometimes you may have the case that a lot of files need to go from point A to B over a Wide Area Network (eg. across the Atlantic). Since packet latency and other factors on the network will naturally slow down the transfers, you need to find workarounds, typically with either rsync or tar.","title":"Data Transfer"},{"location":"data/transfer/#data-transfer-tofromwithin-ul-hpc-clusters","text":"","title":"Data Transfer to/from/within UL HPC Clusters"},{"location":"data/transfer/#introduction","text":"Directories such as $HOME , $WORK or $SCRATCH are shared among the nodes of the cluster that you are using (including the login node) via shared filesystems (SpectrumScale, Lustre) meaning that: every file/directory pushed or created on the login node is available on the computing nodes every file/directory pushed or created on the computing nodes is available on the login node The two most common commands you can use for data transfers over SSH: scp : for the full transfer of files and directories (only works fine for single files or directories of small/trivial size) rsync : a software application which synchronizes files and directories from one location to another while minimizing data transfer as only the outdated or inexistent elements are transferred (practically required for lengthy complex transfers, which are more likely to be interrupted in the middle). scp or rsync? While both ensure a secure transfer of the data within an encrypted tunnel, rsync should be preferred : as mentionned in the from openSSH 8.0 release notes : \" The scp protocol is outdated , inflexible and not readily fixed . We recommend the use of more modern protocols like sftp and rsync for file transfer instead \". scp is also relatively slow when compared to rsync as exhibited for instance in the below sample Distem experience: You will find below notes on scp usage, but kindly prefer to use rsync . Consider scp as deprecated! Click nevertheless to get usage details scp (see scp(1) ) or secure copy is probably the easiest of all the methods. The basic syntax is as follows: scp [-P 8022] [-Cr] source_path destination_path the -P option specifies the SSH port to use (in this case 8022) the -C option activates the compression (actually, it passes the -C flag to ssh(1) to enable compression). the -r option states to recursively copy entire directories (in this case, scp follows symbolic links encountered in the tree traversal). Please note that in this case, you must specify the source file as a directory for this to work. The syntax for declaring a remote path is as follows on the cluster: yourlogin@iris-cluster:path/from/homedir Transfer from your local machine to the remote cluster login node For instance, let's assume you have a local directory ~/devel/myproject you want to transfer to the cluster, in your remote homedir. # /!\\ ADAPT yourlogin to... your ULHPC login $> scp -P 8022 -r ~/devel/myproject yourlogin@iris-cluster: This will transfer recursively your local directory ~/devel/myproject on the cluster login node (in your homedir). Note that if you configured (as advised elsewhere) the SSH connection in your ~/.ssh/config file, you can use a much simpler syntax: $> scp -r ~/devel/myproject iris-cluster: Transfer from the remote cluster front-end to your local machine Conversely, let's assume you want to retrieve the files ~/experiments/parallel_run/* $> scp -P 8022 yourlogin@iris-cluster:experiments/parallel_run/* /path/to/local/directory Again, if you configured the SSH connection in your ~/.ssh/config file, you can use a simpler syntax: $> scp iris-cluster:experiments/parallel_run/* /path/to/local/directory See the scp(1) man page or man scp for more details. Danger scp SHOULD NOT be used in the following cases: When you are copying more than a few files, as scp spawns a new process for each file and can be quite slow and resource intensive when copying a large number of files. When using the -r switch, scp does not know about symbolic links and will blindly follow them, even if it has already made a copy of the file. That can lead to scp copying an infinite amount of data and can easily fill up your hard disk (or worse, a system shared disk), so be careful. N.B. There are many alternative ways to transfer files in HPC platforms and you should check your options according to the problem at hand. Windows and OS X users may wish to transfer files from their systems to the clusters' login nodes with easy-to-use GUI applications such as: WinSCP (Windows only) FileZilla Client (Windows, OS X) Cyberduck (Windows, OS X) These applications will need to be configured to connect to the frontends with the same parameters as discussed on the SSH access page .","title":"Introduction"},{"location":"data/transfer/#using-rsync","text":"The clever alternative to scp is rsync , which has the advantage of transferring only the files which differ between the source and the destination. This feature is often referred to as fast incremental file transfer. Additionally, symbolic links can be preserved. The typical syntax of rsync (see rsync(1) ) for the cluster is similar to the one of scp : # /!\\ ADAPT </path/to/source> and </path/to/destination> # From LOCAL directory (/path/to/local/source) toward REMOTE server <hostname> rsync --rsh = 'ssh -p 8022' -avzu /path/to/local/source [ user@ ] hostname:/path/to/destination # Ex: from REMOTE server <hostname> to LOCAL directory rsync --rsh = 'ssh -p 8022' -avzu [ user@ ] hostname:/path/to/source /path/to/local/destination the --rsh option specifies the connector to use (here SSH on port 8022) the -a option corresponds to the \"Archive\" mode. Most likely you should always keep this on as it preserves file permissions and does not follow symlinks. the -v option enables the verbose mode the -z option enable compression, this will compress each file as it gets sent over the pipe. This can greatly decrease time, depending on what sort of files you are copying. the -u option (or --update ) corresponds to an updating process which skips files that are newer on the receiver. At this level, you may prefer the more dangerous option --delete that deletes extraneous files from dest dirs. Just like scp , the syntax for qualifying a remote path is as follows on the cluster: yourlogin@iris-cluster:path/from/homedir","title":"Using rsync"},{"location":"data/transfer/#transfer-from-your-local-machine-to-the-remote-cluster","text":"Coming back to the previous examples, let's assume you have a local directory ~/devel/myproject you want to transfer to the cluster, in your remote homedir. In that case: # /!\\ ADAPT yourlogin to... your ULHPC login $> rsync --rsh = 'ssh -p 8022' -avzu ~/devel/myproject yourlogin@access-iris.uni.lu: This will synchronize your local directory ~/devel/myproject on the cluster front-end (in your homedir). Transfer to Iris, Aion or both? The above example target the access server of Iris. Actually, you could have targetted the access server of Aion: it doesn't matter since the storage is SHARED between both clusters. Note that if you configured (as advised above) your SSH connection in your ~/.ssh/config file with a dedicated SSH entry {iris,aion}-cluster , you can use a simpler syntax: $> rsync -avzu ~/devel/myproject iris-cluster: # OR (it doesn't matter) $> rsync -avzu ~/devel/myproject aion-cluster:","title":"Transfer from your local machine to the remote cluster"},{"location":"data/transfer/#transfer-from-your-local-machine-to-a-project-directory-on-the-remote-cluster","text":"When transferring data to a project directory you should keep the group and group permissions imposed by the project directory and quota. Therefore you need to add the options --no-p --no-g to your rsync command: $> rsync -avP --no-p --no-g ~/devel/myproject iris-cluster:/work/projects/myproject/","title":"Transfer from your local machine to a project directory on the remote cluster"},{"location":"data/transfer/#transfer-from-the-remote-cluster-to-your-local-machine","text":"Conversely, let's assume you want to synchronize (retrieve) the remote files ~/experiments/parallel_run/* on your local machine: # /!\\ ADAPT yourlogin to... your ULHPC login $> rsync --rsh = 'ssh -p 8022' -avzu yourlogin@access-iris.uni.lu:experiments/parallel_run /path/to/local/directory Again, if you configured the SSH connection in your ~/.ssh/config file, you can use a simpler syntax: $> rsync -avzu iris-cluster:experiments/parallel_run /path/to/local/directory # OR (it doesn't matter) $> rsync -avzu aion-cluster:experiments/parallel_run /path/to/local/directory As always, see the man page or man rsync for more details.","title":"Transfer from the remote cluster to your local machine"},{"location":"data/transfer/#data-transfer-within-project-directories","text":"The ULHPC facility features a Global Project directory $PROJECTHOME hosted within the GPFS/SpecrumScale file-system. You have to pay a particular attention when using rsync to transfer data within your project directory as depicted below. Access rights to project directory: Quota for clusterusers group in project directories is 0 !!! When a project <name> is created, a group of the same name ( <name> ) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory. Be aware that your default group as a user is clusterusers which has ( on purpose ) a quota in project directories set to 0 . You thus need to ensure you always write data in your project directory using the <name> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: chmod g+s [...] When using rsync to transfer file toward the project directory /work/projects/<name> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to: give new files the destination-default permissions with --no-p ( --no-perms ), and use the default group <name> of the destination dir with --no-g ( --no-group ) (eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using --chmod=ug=rwX Your full rsync command becomes (adapt accordingly): rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] <source> /work/projects/<name>/[...] For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory /work/projects/<name> , you want to use the sg as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"<command> [...]\" This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"eb [...] -r --rebuild -D\" # Dry-run - enforce using the '<name>' group sg <name> -c \"eb [...] -r --rebuild\" # Dry-run - enforce using the '<name>' group","title":"Data Transfer within Project directories"},{"location":"data/transfer/#using-mobaxterm-windows","text":"If you are under Windows and you have MobaXterm installed and configured , you probably want to use it to transfer your files to the clusters. Here are the steps to use rsync inside MobaXterm in Windows. Warning Be aware that you SHOULD enable MobaXterm SSH Agent -- see SSH Agent instructions for more instructions.","title":"Using MobaXterm (Windows)"},{"location":"data/transfer/#using-a-local-bash-transfer-your-files","text":"Open a local \"bash\" shell. Click on Start local terminal on the welcome page of MobaXterm. Find the location of the files you want to transfer. They should be located under /drives/<name of your disk> . You will have to use the Linux command line to move from one directory to the other. The cd command is used to change the current directory and ls to list files. For example, if your files are under C:\\\\Users\\janedoe\\Downloads\\ you should then go to /drives/c/Users/janedoe/Downloads/ with this command: cd /drives/c/Users/janedoe/Downloads/ Then list the files with ls command. You should see the list of your data files. When you have retrieved the location of your files, we can begin the transfer with rsync . For example /drives/c/Users/janedoe/Downloads/ (watch out, there is no / character at the end of the path, it is important). Launch the command rsync with this parameters to transfer all the content of the Downloads directory to the /isilon/projects/market_data/ directory on the cluster (the syntax is very important, be careful) rsync -avzpP -e \"ssh -p 8022\" /drives/c/Users/janedoe/Downloads/ yourlogin@access-iris.uni.lu:/isilon/projects/market_data/ You should see the output of transfer in progress. Wait for it to finish (it can be very long).","title":"Using a local bash, transfer your files"},{"location":"data/transfer/#interrupt-and-resume-a-transfer-in-progress","text":"If you want to interrupt the transfer to resume it later, press Ctrl-C and exit MobaXterm. To resume a transfer, go in the right location and execute the rsync command again. Only the files that have not been transferred will be transferred again.","title":"Interrupt and resume a transfer in progress"},{"location":"data/transfer/#alternative-approaches","text":"You can also consider alternative approaches to synchronize data with the cluster login node: rely on a versioning system such as GIT ; this approach works well for source code trees. mount your remote homedir by SSHFS . On Mac OS X, you should consider installing MacFusion for this purpose - on classical Linux system, just use the command-line sshfs or, mc . see below for details you can also rely on GUI tools like FileZilla , Cyberduck or WindSCP or any paid alternative (like ExpanDrive or ForkLift 3 )","title":"Alternative approaches"},{"location":"data/transfer/#sshfs","text":"Linux # Debian-like sudo apt-get install sshfs # RHEL-like sudo yum install sshfs You may need to add yourself to the fuse group. Mac OS X # Assuming HomeBrew -- see https://brew.sh brew install osxfuse sshfs You can also directly install macFUSE from https://osxfuse.github.io/ You must reboot for the installation of osxfuse to take effect. You can then update to the latest version SSHFS allows any user to remotely mount their ULHPC home directory onto a local workstation through an ssh connection. The CLI format is as follows: sshfs [user@]host:[dir] mountpoint [options] Proceed as follows ( assuming you have a working SSH connection ): # Create a local directory hosting the mountng point mkdir -p ~/ulhpc # /!\\ ADAPT accordingly to match your taste sshfs iris-cluster: ~/ulhpc -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache = no -onolocalcaches # General options: # allow_other: Allow other users than the mounter (i.e. root) to access the share # reconnect: try to reconnnect # Optional options to be more \"Mac-like\": # -ocache=no # -onolocalcaches # -o volname=ulhpc_home Name of the volume in Finder Later on (once you no longer need it), you MUST unmount your remote FS # Linux fusermount -u ~/ulhpc # Mac OS X diskutil umount ~/ulhpc","title":"SSHFS"},{"location":"data/transfer/#special-transfers","text":"Sometimes you may have the case that a lot of files need to go from point A to B over a Wide Area Network (eg. across the Atlantic). Since packet latency and other factors on the network will naturally slow down the transfers, you need to find workarounds, typically with either rsync or tar.","title":"Special transfers"},{"location":"data-center/","text":"ULHPC Data Center - Centre de Calcul (CDC) \u00b6 The ULHPC facilities are hosted within the University's \" Centre de Calcul \" (CDC) data center located in the Belval Campus. Power and Cooling Capacities \u00b6 Established over two floors underground (CDC-S01 and CDC-S02) of ~1000~100m 2 each, the CDC features five server rooms per level (each of them offering ~100m 2 as IT rooms surface). When the first level CDC-S01 is hosting administrative IT and research equipment, the second floor ( CDC-S02 ) is primarily targeting the hosting of HPC equipment (compute, storage and interconnect) . A power generation station supplies the HPC floor with up to 3 MW of electrical power, and 3 MW of cold water at a 12-18\u00b0C regime used for traditional Airflow with In-Row cooling. A separate hot water circuit (between 30 and 40\u00b0C) allow to implement Direct Liquid Cooling (DLC) solutions as for the Aion supercomputer in two dedicated server rooms. Location Cooling Usage Max Capa. CDC S-02-001 Airflow Future extension 280 kW CDC S-02-002 Airflow Future extension 280 kW CDC S-02-003 DLC Future extension - High Density/Energy efficient HPC 1050 kW CDC S-02-004 DLC High Density/Energy efficient HPC : aion 1050 kW CDC S-02-005 Airflow Storage / Traditional HPC : iris and common equipment 300 kW Data-Center Cooling technologies \u00b6 Airflow with In-Row cooling \u00b6 Most server rooms are designed for traditional airflow-based cooling and implement hot or cold aisle containment , as well as In-row cooling systems work within a row of standard server rack engineered to take up the smallest footprint and offer high-density cooling. Ducting and baffles ensure that the cooling air gets where it needs to go. Iris compute, storage and interconnect equipment are hosted in such a configuration [Direct] Liquid Cooling \u00b6 Traditional solutions implemented in most data centers use air as a medium to remove the heat from the servers and computing equipment and are not well suited to cutting-edge high-density HPC environments due to the limited thermal capacity of air. Liquids\u2019 thermal conductivity is higher than the air, thus concludes the liquid can absorb (through conductivity) more heat than the air. The replacement of air with a liquid cooling medium allows to drastically improve the energy-efficiency as well as the density of the implemented solution, especially with Direct Liquid Cooling (DLC) where the heat from the IT components is directly transferred to a liquid cooling medium through liquid-cooled plates. The Aion supercomputer based on the fan-less Atos BullSequana XH2000 DLC cell design relies on this water-cooled configuration.","title":"Centre de Calcul (CDC)"},{"location":"data-center/#ulhpc-data-center-centre-de-calcul-cdc","text":"The ULHPC facilities are hosted within the University's \" Centre de Calcul \" (CDC) data center located in the Belval Campus.","title":"ULHPC Data Center - Centre de Calcul (CDC)"},{"location":"data-center/#power-and-cooling-capacities","text":"Established over two floors underground (CDC-S01 and CDC-S02) of ~1000~100m 2 each, the CDC features five server rooms per level (each of them offering ~100m 2 as IT rooms surface). When the first level CDC-S01 is hosting administrative IT and research equipment, the second floor ( CDC-S02 ) is primarily targeting the hosting of HPC equipment (compute, storage and interconnect) . A power generation station supplies the HPC floor with up to 3 MW of electrical power, and 3 MW of cold water at a 12-18\u00b0C regime used for traditional Airflow with In-Row cooling. A separate hot water circuit (between 30 and 40\u00b0C) allow to implement Direct Liquid Cooling (DLC) solutions as for the Aion supercomputer in two dedicated server rooms. Location Cooling Usage Max Capa. CDC S-02-001 Airflow Future extension 280 kW CDC S-02-002 Airflow Future extension 280 kW CDC S-02-003 DLC Future extension - High Density/Energy efficient HPC 1050 kW CDC S-02-004 DLC High Density/Energy efficient HPC : aion 1050 kW CDC S-02-005 Airflow Storage / Traditional HPC : iris and common equipment 300 kW","title":"Power and Cooling Capacities"},{"location":"data-center/#data-center-cooling-technologies","text":"","title":"Data-Center Cooling technologies"},{"location":"data-center/#airflow-with-in-row-cooling","text":"Most server rooms are designed for traditional airflow-based cooling and implement hot or cold aisle containment , as well as In-row cooling systems work within a row of standard server rack engineered to take up the smallest footprint and offer high-density cooling. Ducting and baffles ensure that the cooling air gets where it needs to go. Iris compute, storage and interconnect equipment are hosted in such a configuration","title":"Airflow with In-Row cooling"},{"location":"data-center/#direct-liquid-cooling","text":"Traditional solutions implemented in most data centers use air as a medium to remove the heat from the servers and computing equipment and are not well suited to cutting-edge high-density HPC environments due to the limited thermal capacity of air. Liquids\u2019 thermal conductivity is higher than the air, thus concludes the liquid can absorb (through conductivity) more heat than the air. The replacement of air with a liquid cooling medium allows to drastically improve the energy-efficiency as well as the density of the implemented solution, especially with Direct Liquid Cooling (DLC) where the heat from the IT components is directly transferred to a liquid cooling medium through liquid-cooled plates. The Aion supercomputer based on the fan-less Atos BullSequana XH2000 DLC cell design relies on this water-cooled configuration.","title":"[Direct] Liquid Cooling"},{"location":"development/build-tools/easybuild/","text":"Building [custom] software with EasyBuild on the UL HPC platform \u00b6 EasyBuild can be used to ease, automate and script the build of software on the UL HPC platforms. Indeed, as researchers involved in many cutting-edge and hot topics, you probably have access to many theoretical resources to understand the surrounding concepts. Yet it should normally give you a wish to test the corresponding software. Traditionally, this part is rather time-consuming and frustrating, especially when the developers did not rely on a \"regular\" building framework such as CMake or the autotools ( i.e. with build instructions as configure --prefix <path> && make && make install ). And when it comes to have a build adapted to an HPC system, you are somehow forced to make a custom build performed on the target machine to ensure you will get the best possible performances. EasyBuild is one approach to facilitate this step. EasyBuild is a tool that allows to perform automated and reproducible compilation and installation of software. A large number of scientific software are supported ( 1504 supported software packages in the last release 3.6.1) -- see also What is EasyBuild? All builds and installations are performed at user level, so you don't need the admin (i.e. root ) rights. The software are installed in your home directory (by default in $HOME/.local/easybuild/software/ ) and a module file is generated (by default in $HOME/.local/easybuild/modules/ ) to use the software. EasyBuild relies on two main concepts: Toolchains and EasyConfig files . A toolchain corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the foss (\" Free and Open Source Software \") and the intel one. foss is based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.). intel is based on the Intel compiler and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.). An EasyConfig file is a simple text file that describes the build process of a software. For most software that uses standard procedures (like configure , make and make install ), this file is very simple. Many EasyConfig files are already provided with EasyBuild. By default, EasyConfig files and generated modules are named using the following convention: <Software-Name>-<Software-Version>-<Toolchain-Name>-<Toolchain-Version> . However, we use a hierarchical approach where the software are classified under a category (or class) -- see the CategorizedModuleNamingScheme option for the EASYBUILD_MODULE_NAMING_SCHEME environmental variable), meaning that the layout will respect the following hierarchy: <Software-Class>/<Software-Name>/<Software-Version>-<Toolchain-Name>-<Toolchain-Version> Additional details are available on EasyBuild website: EasyBuild homepage EasyBuild tutorial EasyBuild documentation What is EasyBuild? Toolchains EasyConfig files List of supported software packages a. Installation \u00b6 the official instructions . What is important for the installation of Easybuild are the following variables: EASYBUILD_PREFIX : where to install local modules and software, i.e. $HOME/.local/easybuild EASYBUILD_MODULES_TOOL : the type of modules tool you are using, i.e. LMod in this case EASYBUILD_MODULE_NAMING_SCHEME : the way the software and modules should be organized (flat view or hierarchical) -- we're advising on CategorizedModuleNamingScheme Add the following entries to your ~/.bashrc (use your favorite CLI editor like nano or vim ): # Easybuild export EASYBUILD_PREFIX = $HOME /.local/easybuild export EASYBUILD_MODULES_TOOL = Lmod export EASYBUILD_MODULE_NAMING_SCHEME = CategorizedModuleNamingScheme # Use the below variable to run: # module use $LOCAL_MODULES # module load tools/EasyBuild export LOCAL_MODULES = ${ EASYBUILD_PREFIX } /modules/all alias ma = \"module avail\" alias ml = \"module list\" function mu (){ module use $LOCAL_MODULES module load tools/EasyBuild } Then source this file to expose the environment variables: $> source ~/.bashrc $> echo $EASYBUILD_PREFIX /home/users/<login>/.local/easybuild Now let's install Easybuild following the boostrapping procedure : $> cd # download script $> curl -o bootstrap_eb.py https://raw.githubusercontent.com/easybuilders/easybuild-framework/develop/easybuild/scripts/bootstrap_eb.py # install Easybuild $> python bootstrap_eb.py $EASYBUILD_PREFIX Now you can use your freshly built software. The main EasyBuild command is eb : $> eb --version # expected ;) -bash: eb: command not found # Load the newly installed Easybuild $> echo $MODULEPATH /opt/apps/resif/data/stable/default/modules/all/ $> module use $LOCAL_MODULES $> echo $MODULEPATH /home/users/<login>/.local/easybuild/modules/all:/opt/apps/resif/data/stable/default/modules/all $> module spider Easybuild $> module load tools/EasyBuild # TAB is your friend... $> eb --version This is EasyBuild 3 .6.1 ( framework: 3 .6.1, easyblocks: 3 .6.1 ) on host iris-001. Since you are going to use quite often the above command to use locally built modules and load easybuild, an alias mu is provided and can be used from now on. Use it now . $> mu $> module avail # OR 'ma' To get help on the EasyBuild options, use the -h or -H option flags: $> eb -h $> eb -H b. Local vs. global usage \u00b6 As you probably guessed, we are going to use two places for the installed software: local builds ~/.local/easybuild (see $LOCAL_MODULES ) global builds (provided to you by the UL HPC team) in /opt/apps/resif/data/stable/default/modules/all (see default $MODULEPATH ). Default usage (with the eb command) would install your software and modules in ~/.local/easybuild . Before that, let's explore the basic usage of EasyBuild and the eb command. # Search for an Easybuild recipy with 'eb -S <pattern>' $> eb -S Spark CFGS1 = /opt/apps/resif/data/easyconfigs/ulhpc/default/easybuild/easyconfigs/s/Spark CFGS2 = /home/users/<login>/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/s/Spark * $CFGS1 /Spark-2.1.1.eb * $CFGS1 /Spark-2.3.0-intel-2018a-Hadoop-2.7-Java-1.8.0_162-Python-3.6.4.eb * $CFGS2 /Spark-1.3.0.eb * $CFGS2 /Spark-1.4.1.eb * $CFGS2 /Spark-1.5.0.eb * $CFGS2 /Spark-1.6.0.eb * $CFGS2 /Spark-1.6.1.eb * $CFGS2 /Spark-2.0.0.eb * $CFGS2 /Spark-2.0.2.eb * $CFGS2 /Spark-2.2.0-Hadoop-2.6-Java-1.8.0_144.eb * $CFGS2 /Spark-2.2.0-Hadoop-2.6-Java-1.8.0_152.eb * $CFGS2 /Spark-2.2.0-intel-2017b-Hadoop-2.6-Java-1.8.0_152-Python-3.6.3.eb c. Build software using provided EasyConfig file \u00b6 In this part, we propose to build High Performance Linpack (HPL) using EasyBuild. HPL is supported by EasyBuild, this means that an EasyConfig file allowing to build HPL is already provided with EasyBuild. First of all, let's check if that software is not available by default: $> module spider HPL Lmod has detected the following error: Unable to find: \"HPL\" Then, search for available EasyConfig files with HPL in their name. The EasyConfig files are named with the .eb extension. # Search for an Easybuild recipy with 'eb -S <pattern>' $> eb -S HPL-2.2 CFGS1 = /home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/h/HPL * $CFGS1 /HPL-2.2-foss-2016.07.eb * $CFGS1 /HPL-2.2-foss-2016.09.eb * $CFGS1 /HPL-2.2-foss-2017a.eb * $CFGS1 /HPL-2.2-foss-2017b.eb * $CFGS1 /HPL-2.2-foss-2018a.eb * $CFGS1 /HPL-2.2-fosscuda-2018a.eb * $CFGS1 /HPL-2.2-giolf-2017b.eb * $CFGS1 /HPL-2.2-giolf-2018a.eb * $CFGS1 /HPL-2.2-giolfc-2017b.eb * $CFGS1 /HPL-2.2-gmpolf-2017.10.eb * $CFGS1 /HPL-2.2-goolfc-2016.08.eb * $CFGS1 /HPL-2.2-goolfc-2016.10.eb * $CFGS1 /HPL-2.2-intel-2017.00.eb * $CFGS1 /HPL-2.2-intel-2017.01.eb * $CFGS1 /HPL-2.2-intel-2017.02.eb * $CFGS1 /HPL-2.2-intel-2017.09.eb * $CFGS1 /HPL-2.2-intel-2017a.eb * $CFGS1 /HPL-2.2-intel-2017b.eb * $CFGS1 /HPL-2.2-intel-2018.00.eb * $CFGS1 /HPL-2.2-intel-2018.01.eb * $CFGS1 /HPL-2.2-intel-2018.02.eb * $CFGS1 /HPL-2.2-intel-2018a.eb * $CFGS1 /HPL-2.2-intelcuda-2016.10.eb * $CFGS1 /HPL-2.2-iomkl-2016.09-GCC-4.9.3-2.25.eb * $CFGS1 /HPL-2.2-iomkl-2016.09-GCC-5.4.0-2.26.eb * $CFGS1 /HPL-2.2-iomkl-2017.01.eb * $CFGS1 /HPL-2.2-intel-2017.02.eb * $CFGS1 /HPL-2.2-intel-2017.09.eb * $CFGS1 /HPL-2.2-intel-2017a.eb * $CFGS1 /HPL-2.2-intel-2017b.eb * $CFGS1 /HPL-2.2-intel-2018.00.eb * $CFGS1 /HPL-2.2-intel-2018.01.eb * $CFGS1 /HPL-2.2-intel-2018.02.eb * $CFGS1 /HPL-2.2-intel-2018a.eb * $CFGS1 /HPL-2.2-intelcuda-2016.10.eb * $CFGS1 /HPL-2.2-iomkl-2016.09-GCC-4.9.3-2.25.eb * $CFGS1 /HPL-2.2-iomkl-2016.09-GCC-5.4.0-2.26.eb * $CFGS1 /HPL-2.2-iomkl-2017.01.eb * $CFGS1 /HPL-2.2-iomkl-2017a.eb * $CFGS1 /HPL-2.2-iomkl-2017b.eb * $CFGS1 /HPL-2.2-iomkl-2018.02.eb * $CFGS1 /HPL-2.2-iomkl-2018a.eb * $CFGS1 /HPL-2.2-pomkl-2016.09.eb We are going to build HPL 2.2 against the intel toolchain, typically the 2017a version which is available by default on the platform. Pick the corresponding recipy (for instance HPL-2.2-intel-2017a.eb ), install it with eb <name>.eb [-D] -r -D enables the dry-run mode to check what's going to be install -- ALWAYS try it first -r enables the robot mode to automatically install all dependencies while searching for easyconfigs in a set of pre-defined directories -- you can also prepend new directories to search for eb files (like the current directory $PWD ) using the option and syntax --robot-paths=$PWD: (do not forget the ':'). See Controlling the robot search path documentation The $CFGS<n>/ prefix should be dropped unless you know what you're doing (and thus have previously defined the variable -- see the first output of the eb -S [...] command). So let's install HPL version 2.2 and FIRST check which dependencies are satisfied with -Dr : $> eb HPL-2.2-intel-2017a.eb -Dr == temporary log file in case of crash /tmp/eb-CTC2hq/easybuild-gfLf1W.log Dry run: printing build status of easyconfigs and dependencies CFGS = /home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs * [ x ] $CFGS /m/M4/M4-1.4.17.eb ( module: devel/M4/1.4.17 ) * [ x ] $CFGS /b/Bison/Bison-3.0.4.eb ( module: lang/Bison/3.0.4 ) * [ x ] $CFGS /f/flex/flex-2.6.0.eb ( module: lang/flex/2.6.0 ) * [ x ] $CFGS /z/zlib/zlib-1.2.8.eb ( module: lib/zlib/1.2.8 ) * [ x ] $CFGS /b/binutils/binutils-2.27.eb ( module: tools/binutils/2.27 ) * [ x ] $CFGS /g/GCCcore/GCCcore-6.3.0.eb ( module: compiler/GCCcore/6.3.0 ) * [ x ] $CFGS /m/M4/M4-1.4.18-GCCcore-6.3.0.eb ( module: devel/M4/1.4.18-GCCcore-6.3.0 ) * [ x ] $CFGS /z/zlib/zlib-1.2.11-GCCcore-6.3.0.eb ( module: lib/zlib/1.2.11-GCCcore-6.3.0 ) * [ x ] $CFGS /h/help2man/help2man-1.47.4-GCCcore-6.3.0.eb ( module: tools/help2man/1.47.4-GCCcore-6.3.0 ) * [ x ] $CFGS /b/Bison/Bison-3.0.4-GCCcore-6.3.0.eb ( module: lang/Bison/3.0.4-GCCcore-6.3.0 ) * [ x ] $CFGS /f/flex/flex-2.6.3-GCCcore-6.3.0.eb ( module: lang/flex/2.6.3-GCCcore-6.3.0 ) * [ x ] $CFGS /b/binutils/binutils-2.27-GCCcore-6.3.0.eb ( module: tools/binutils/2.27-GCCcore-6.3.0 ) * [ x ] $CFGS /i/icc/icc-2017.1.132-GCC-6.3.0-2.27.eb ( module: compiler/icc/2017.1.132-GCC-6.3.0-2.27 ) * [ x ] $CFGS /i/ifort/ifort-2017.1.132-GCC-6.3.0-2.27.eb ( module: compiler/ifort/2017.1.132-GCC-6.3.0-2.27 ) * [ x ] $CFGS /i/iccifort/iccifort-2017.1.132-GCC-6.3.0-2.27.eb ( module: toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27 ) * [ x ] $CFGS /i/impi/impi-2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27.eb ( module: mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27 ) * [ x ] $CFGS /i/iimpi/iimpi-2017a.eb ( module: toolchain/iimpi/2017a ) * [ x ] $CFGS /i/imkl/imkl-2017.1.132-iimpi-2017a.eb ( module: numlib/imkl/2017.1.132-iimpi-2017a ) * [ x ] $CFGS /i/intel/intel-2017a.eb ( module: toolchain/intel/2017a ) * [ ] $CFGS /h/HPL/HPL-2.2-intel-2017a.eb ( module: tools/HPL/2.2-intel-2017a ) == Temporary log file ( s ) /tmp/eb-CTC2hq/easybuild-gfLf1W.log* have been removed. == Temporary directory /tmp/eb-CTC2hq has been removed. As can be seen, there is a single element to install and this has not been done so far (box not checked). All the dependencies are already present (box checked). Let's really install the selected software -- you may want to prefix the eb command with the time to collect the installation time: $> time eb HPL-2.2-intel-2017a.eb -r # Remove the '-D' (dry-run) flags == temporary log file in case of crash /tmp/eb-nub_oL/easybuild-J8sNzx.log == resolving dependencies ... == processing EasyBuild easyconfig /home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/h/HPL/HPL-2.2-intel-2017a.eb == building and installing tools/HPL/2.2-intel-2017a... == fetching files... == creating build dir, resetting environment... == unpacking... == patching... == preparing... == configuring... == building... == testing... == installing... == taking care of extensions... == postprocessing... == sanity checking... == cleaning up... == creating module... == permissions... == packaging... == COMPLETED: Installation ended successfully == Results of the build can be found in the log file ( s ) /home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a/easybuild/easybuild-HPL-2.2-20180608.094831.log == Build succeeded for 1 out of 1 == Temporary log file ( s ) /tmp/eb-nub_oL/easybuild-J8sNzx.log* have been removed. == Temporary directory /tmp/eb-nub_oL has been removed. real 0m56.472s user 0m15.268s sys 0m19.998s Check the installed software: $> module av HPL ------------------------- /home/users/<login>/.local/easybuild/modules/all ------------------------- tools/HPL/2.2-intel-2017a Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". $> module spider HPL ---------------------------------------------------------------------------------------------------- tools/HPL: tools/HPL/2.2-intel-2017a ---------------------------------------------------------------------------------------------------- Description: HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable as well as freely available implementation of the High Performance Computing Linpack Benchmark. This module can be loaded directly: module load tools/HPL/2.2-intel-2017a Help: Description =========== HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable as well as freely available implementation of the High Performance Computing Linpack Benchmark. More information ================ - Homepage: http://www.netlib.org/benchmark/hpl/ $> module show tools/HPL --------------------------------------------------------------------------------------------------- /home/users/svarrette/.local/easybuild/modules/all/tools/HPL/2.2-intel-2017a.lua: --------------------------------------------------------------------------------------------------- help([[ Description =========== HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable as well as freely available implementation of the High Performance Computing Linpack Benchmark. More information ================ - Homepage: http://www.netlib.org/benchmark/hpl/ ]]) whatis(\"Description: HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable as well as freely available implementation of the High Performance Computing Linpack Benchmark.\") whatis(\"Homepage: http://www.netlib.org/benchmark/hpl/\") conflict(\"tools/HPL\") load(\"toolchain/intel/2017a\") prepend_path(\"PATH\",\"/home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a/bin\") setenv(\"EBROOTHPL\",\"/home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a\") setenv(\"EBVERSIONHPL\",\"2.2\") setenv(\"EBDEVELHPL\",\"/home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a/easybuild/tools-HPL-2.2-intel-2017a-easybuild-devel\") Note : to see the (locally) installed software, the MODULEPATH variable should include the $HOME/.local/easybuild/modules/all/ (of $LOCAL_MODULES ) path (which is what happens when using module use <path> -- see the mu command) You can now load the freshly installed module like any other: $> module load tools/HPL $> module list Currently Loaded Modules: 1 ) tools/EasyBuild/3.6.1 7 ) mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27 2 ) compiler/GCCcore/6.3.0 8 ) toolchain/iimpi/2017a 3 ) tools/binutils/2.27-GCCcore-6.3.0 9 ) numlib/imkl/2017.1.132-iimpi-2017a 4 ) compiler/icc/2017.1.132-GCC-6.3.0-2.27 10 ) toolchain/intel/2017a 5 ) compiler/ifort/2017.1.132-GCC-6.3.0-2.27 11 ) tools/HPL/2.2-intel-2017a 6 ) toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27 Tips : When you load a module <NAME> generated by Easybuild, it is installed within the directory reported by the $EBROOT<NAME> variable. In the above case, you will find the generated binary for HPL in ${EBROOTHPL}/bin/xhpl . You may want to test the newly built HPL benchmark (you need to reserve at least 4 cores for that to succeed): # In another terminal, connect to the cluster frontend # Have an interactive job ############### iris cluster (slurm) ############### ( access-iris ) $> si -n 4 # this time reserve for 4 (mpi) tasks $> mu $> module load tools/HPL $> cd $EBROOTHPL $> ls $> cd bin $> ls $> srun -n $SLURM_NTASKS ./xhpl Running HPL benchmarks requires more attention -- a full tutorial is dedicated to it. Yet you can see that we obtained HPL 2.2 without writing any EasyConfig file. d. Build software using a customized EasyConfig file \u00b6 There are multiple ways to amend an EasyConfig file. Check the --try-* option flags for all the possibilities. Generally you want to do that when the up-to-date version of the software you want is not available as a recipy within Easybuild. For instance, a very popular building environment CMake has recently released a new version (3.11.3), which you want to give a try. It is not available as module, so let's build it. First let's check for available easyconfigs recipy if one exist for the expected version: $> eb -S Cmake-3 [...] * $CFGS2/CMake-3.9.1.eb * $CFGS2/CMake-3.9.4-GCCcore-6.4.0.eb * $CFGS2/CMake-3.9.5-GCCcore-6.4.0.eb We are going to reuse one of the latest EasyConfig available, for instance lets copy $CFGS2/CMake-3.9.1.eb # Work in a dedicated directory $> mkdir -p ~/software/CMake $> cd ~/software/CMake $> eb -S Cmake-3 | less # collect the definition of the CFGS2 variable $> CFGS2 = /home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/c/CMake $> cp $CFGS2 /CMake-3.9.1.eb . $> mv CMake-3.9.1.eb CMake-3.11.3.eb # Adapt version suffix to the lastest realse You need to perform the following changes (here: version upgrade, and adapted checksum) --- CMake-3.9.1.eb 2018-06-08 10:56:24.447699000 +0200 +++ CMake-3.11.3.eb 2018-06-08 11:07:39.716672000 +0200 @@ -1,7 +1,7 @@ easyblock = 'ConfigureMake' name = 'CMake' -version = '3.9.1' +version = '3.11.3' homepage = 'http://www.cmake.org' description = \"\"\"CMake, the cross-platform, open-source build system. @@ -11,7 +11,7 @@ source_urls = ['http://www.cmake.org/files/v%(version_major_minor)s'] sources = [SOURCELOWER_TAR_GZ] -checksums = ['d768ee83d217f91bb597b3ca2ac663da7a8603c97e1f1a5184bc01e0ad2b12bb'] +checksums = ['287135b6beb7ffc1ccd02707271080bbf14c21d80c067ae2c0040e5f3508c39a'] configopts = '-- -DCMAKE_USE_OPENSSL=1' If the checksum is not provided on the official software page , you will need to compute it yourself by downloading the sources and collect the checksum: $> gsha256sum ~/Download/cmake-3.11.3.tar.gz 287135b6beb7ffc1ccd02707271080bbf14c21d80c067ae2c0040e5f3508c39a cmake-3.11.3.tar.gz Let's build it: $> eb ./CMake-3.11.3.eb -Dr == temporary log file in case of crash /tmp/eb-UX7APP/easybuild-gxnyIv.log Dry run: printing build status of easyconfigs and dependencies CFGS = /mnt/irisgpfs/users/<login>/software/CMake * [ ] $CFGS /CMake-3.11.3.eb ( module: devel/CMake/3.11.3 ) == Temporary log file ( s ) /tmp/eb-UX7APP/easybuild-gxnyIv.log* have been removed. == Temporary directory /tmp/eb-UX7APP has been removed. Dependencies are fine, so let's build it: $> time eb ./CMake-3.11.3.eb -r == temporary log file in case of crash /tmp/eb-JjF92B/easybuild-RjzRjb.log == resolving dependencies ... == processing EasyBuild easyconfig /mnt/irisgpfs/users/<login>/software/CMake/CMake-3.11.3.eb == building and installing devel/CMake/3.11.3... == fetching files... == creating build dir, resetting environment... == unpacking... == patching... == preparing... == configuring... == building... == testing... == installing... == taking care of extensions... == postprocessing... == sanity checking... == cleaning up... == creating module... == permissions... == packaging... == COMPLETED: Installation ended successfully == Results of the build can be found in the log file ( s ) /home/users/<login>/.local/easybuild/software/devel/CMake/3.11.3/easybuild/easybuild-CMake-3.11.3-20180608.111611.log == Build succeeded for 1 out of 1 == Temporary log file ( s ) /tmp/eb-JjF92B/easybuild-RjzRjb.log* have been removed. == Temporary directory /tmp/eb-JjF92B has been removed. real 7m40.358s user 5m56.442s sys 1m15.185s Note you can follow the progress of the installation in a separate shell on the node: Check the result: $> module av CMake That's all ;-) Final remaks This workflow (copying an existing recipy, adapting the filename, the version and the source checksum) covers most of the test cases. Yet sometimes you need to work on a more complex dependency check, in which case you'll need to adapt many eb files. In this case, for each build, you need to instruct Easybuild to search for easyconfigs also in the current directory, in which case you will use: $> eb <filename>.eb --robot = $PWD : $EASYBUILD_ROBOT -D $> eb <filename>.eb --robot = $PWD : $EASYBUILD_ROBOT (OLD) Build software using your own EasyConfig file \u00b6 Below are obsolete instructions to write a full Easyconfig file, left for archiving and informal purposes. For this example, we create an EasyConfig file to build GZip 1.4 with the GOOLF toolchain. Open your favorite editor and create a file named gzip-1.4-goolf-1.4.10.eb with the following content: easyblock = 'ConfigureMake' name = 'gzip' version = '1.4' homepage = 'http://www.gnu.org/software/gzip/' description = \"gzip (GNU zip) is a popular data compression program as a replacement for compress\" # use the GOOLF toolchain toolchain = {'name': 'goolf', 'version': '1.4.10'} # specify that GCC compiler should be used to build gzip preconfigopts = \"CC='gcc'\" # source tarball filename sources = ['%s-%s.tar.gz'%(name,version)] # download location for source files source_urls = ['http://ftpmirror.gnu.org/gzip'] # make sure the gzip and gunzip binaries are available after installation sanity_check_paths = { 'files': [\"bin/gunzip\", \"bin/gzip\"], 'dirs': [] } # run 'gzip -h' and 'gzip --version' after installation sanity_check_commands = [True, ('gzip', '--version')] This is a simple EasyConfig. Most of the fields are self-descriptive. No build method is explicitely defined, so it uses by default the standard configure/make/make install approach. Let's build GZip with this EasyConfig file: $> time eb gzip-1.4-goolf-1.4.10.eb == temporary log file in case of crash /tmp/eb-hiyyN1/easybuild-ynLsHC.log == processing EasyBuild easyconfig /mnt/nfs/users/homedirs/mschmitt/gzip-1.4-goolf-1.4.10.eb == building and installing base/gzip/1.4-goolf-1.4.10... == fetching files... == creating build dir, resetting environment... == unpacking... == patching... == preparing... == configuring... == building... == testing... == installing... == taking care of extensions... == packaging... == postprocessing... == sanity checking... == cleaning up... == creating module... == COMPLETED: Installation ended successfully == Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/base/gzip/1.4-goolf-1.4.10/easybuild/easybuild-gzip-1.4-20150624.114745.log == Build succeeded for 1 out of 1 == temporary log file(s) /tmp/eb-hiyyN1/easybuild-ynLsHC.log* have been removed. == temporary directory /tmp/eb-hiyyN1 has been removed. real 1m39.982s user 0m52.743s sys 0m11.297s We can now check that our version of GZip is available via the modules: $> module avail gzip --------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all --------- base/gzip/1.4-goolf-1.4.10 To go further into details \u00b6 Please refer to the following pointers to get additionnal features: EasyBuild homepage EasyBuild tutorial EasyBuild documentation Getting started Using EasyBuild Step-by-step guide","title":"Building [custom] software with EasyBuild on the UL HPC platform"},{"location":"development/build-tools/easybuild/#building-custom-software-with-easybuild-on-the-ul-hpc-platform","text":"EasyBuild can be used to ease, automate and script the build of software on the UL HPC platforms. Indeed, as researchers involved in many cutting-edge and hot topics, you probably have access to many theoretical resources to understand the surrounding concepts. Yet it should normally give you a wish to test the corresponding software. Traditionally, this part is rather time-consuming and frustrating, especially when the developers did not rely on a \"regular\" building framework such as CMake or the autotools ( i.e. with build instructions as configure --prefix <path> && make && make install ). And when it comes to have a build adapted to an HPC system, you are somehow forced to make a custom build performed on the target machine to ensure you will get the best possible performances. EasyBuild is one approach to facilitate this step. EasyBuild is a tool that allows to perform automated and reproducible compilation and installation of software. A large number of scientific software are supported ( 1504 supported software packages in the last release 3.6.1) -- see also What is EasyBuild? All builds and installations are performed at user level, so you don't need the admin (i.e. root ) rights. The software are installed in your home directory (by default in $HOME/.local/easybuild/software/ ) and a module file is generated (by default in $HOME/.local/easybuild/modules/ ) to use the software. EasyBuild relies on two main concepts: Toolchains and EasyConfig files . A toolchain corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the foss (\" Free and Open Source Software \") and the intel one. foss is based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.). intel is based on the Intel compiler and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.). An EasyConfig file is a simple text file that describes the build process of a software. For most software that uses standard procedures (like configure , make and make install ), this file is very simple. Many EasyConfig files are already provided with EasyBuild. By default, EasyConfig files and generated modules are named using the following convention: <Software-Name>-<Software-Version>-<Toolchain-Name>-<Toolchain-Version> . However, we use a hierarchical approach where the software are classified under a category (or class) -- see the CategorizedModuleNamingScheme option for the EASYBUILD_MODULE_NAMING_SCHEME environmental variable), meaning that the layout will respect the following hierarchy: <Software-Class>/<Software-Name>/<Software-Version>-<Toolchain-Name>-<Toolchain-Version> Additional details are available on EasyBuild website: EasyBuild homepage EasyBuild tutorial EasyBuild documentation What is EasyBuild? Toolchains EasyConfig files List of supported software packages","title":"Building [custom] software with EasyBuild on the UL HPC platform"},{"location":"development/build-tools/easybuild/#a-installation","text":"the official instructions . What is important for the installation of Easybuild are the following variables: EASYBUILD_PREFIX : where to install local modules and software, i.e. $HOME/.local/easybuild EASYBUILD_MODULES_TOOL : the type of modules tool you are using, i.e. LMod in this case EASYBUILD_MODULE_NAMING_SCHEME : the way the software and modules should be organized (flat view or hierarchical) -- we're advising on CategorizedModuleNamingScheme Add the following entries to your ~/.bashrc (use your favorite CLI editor like nano or vim ): # Easybuild export EASYBUILD_PREFIX = $HOME /.local/easybuild export EASYBUILD_MODULES_TOOL = Lmod export EASYBUILD_MODULE_NAMING_SCHEME = CategorizedModuleNamingScheme # Use the below variable to run: # module use $LOCAL_MODULES # module load tools/EasyBuild export LOCAL_MODULES = ${ EASYBUILD_PREFIX } /modules/all alias ma = \"module avail\" alias ml = \"module list\" function mu (){ module use $LOCAL_MODULES module load tools/EasyBuild } Then source this file to expose the environment variables: $> source ~/.bashrc $> echo $EASYBUILD_PREFIX /home/users/<login>/.local/easybuild Now let's install Easybuild following the boostrapping procedure : $> cd # download script $> curl -o bootstrap_eb.py https://raw.githubusercontent.com/easybuilders/easybuild-framework/develop/easybuild/scripts/bootstrap_eb.py # install Easybuild $> python bootstrap_eb.py $EASYBUILD_PREFIX Now you can use your freshly built software. The main EasyBuild command is eb : $> eb --version # expected ;) -bash: eb: command not found # Load the newly installed Easybuild $> echo $MODULEPATH /opt/apps/resif/data/stable/default/modules/all/ $> module use $LOCAL_MODULES $> echo $MODULEPATH /home/users/<login>/.local/easybuild/modules/all:/opt/apps/resif/data/stable/default/modules/all $> module spider Easybuild $> module load tools/EasyBuild # TAB is your friend... $> eb --version This is EasyBuild 3 .6.1 ( framework: 3 .6.1, easyblocks: 3 .6.1 ) on host iris-001. Since you are going to use quite often the above command to use locally built modules and load easybuild, an alias mu is provided and can be used from now on. Use it now . $> mu $> module avail # OR 'ma' To get help on the EasyBuild options, use the -h or -H option flags: $> eb -h $> eb -H","title":"a. Installation"},{"location":"development/build-tools/easybuild/#b-local-vs-global-usage","text":"As you probably guessed, we are going to use two places for the installed software: local builds ~/.local/easybuild (see $LOCAL_MODULES ) global builds (provided to you by the UL HPC team) in /opt/apps/resif/data/stable/default/modules/all (see default $MODULEPATH ). Default usage (with the eb command) would install your software and modules in ~/.local/easybuild . Before that, let's explore the basic usage of EasyBuild and the eb command. # Search for an Easybuild recipy with 'eb -S <pattern>' $> eb -S Spark CFGS1 = /opt/apps/resif/data/easyconfigs/ulhpc/default/easybuild/easyconfigs/s/Spark CFGS2 = /home/users/<login>/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/s/Spark * $CFGS1 /Spark-2.1.1.eb * $CFGS1 /Spark-2.3.0-intel-2018a-Hadoop-2.7-Java-1.8.0_162-Python-3.6.4.eb * $CFGS2 /Spark-1.3.0.eb * $CFGS2 /Spark-1.4.1.eb * $CFGS2 /Spark-1.5.0.eb * $CFGS2 /Spark-1.6.0.eb * $CFGS2 /Spark-1.6.1.eb * $CFGS2 /Spark-2.0.0.eb * $CFGS2 /Spark-2.0.2.eb * $CFGS2 /Spark-2.2.0-Hadoop-2.6-Java-1.8.0_144.eb * $CFGS2 /Spark-2.2.0-Hadoop-2.6-Java-1.8.0_152.eb * $CFGS2 /Spark-2.2.0-intel-2017b-Hadoop-2.6-Java-1.8.0_152-Python-3.6.3.eb","title":"b. Local vs. global usage"},{"location":"development/build-tools/easybuild/#c-build-software-using-provided-easyconfig-file","text":"In this part, we propose to build High Performance Linpack (HPL) using EasyBuild. HPL is supported by EasyBuild, this means that an EasyConfig file allowing to build HPL is already provided with EasyBuild. First of all, let's check if that software is not available by default: $> module spider HPL Lmod has detected the following error: Unable to find: \"HPL\" Then, search for available EasyConfig files with HPL in their name. The EasyConfig files are named with the .eb extension. # Search for an Easybuild recipy with 'eb -S <pattern>' $> eb -S HPL-2.2 CFGS1 = /home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/h/HPL * $CFGS1 /HPL-2.2-foss-2016.07.eb * $CFGS1 /HPL-2.2-foss-2016.09.eb * $CFGS1 /HPL-2.2-foss-2017a.eb * $CFGS1 /HPL-2.2-foss-2017b.eb * $CFGS1 /HPL-2.2-foss-2018a.eb * $CFGS1 /HPL-2.2-fosscuda-2018a.eb * $CFGS1 /HPL-2.2-giolf-2017b.eb * $CFGS1 /HPL-2.2-giolf-2018a.eb * $CFGS1 /HPL-2.2-giolfc-2017b.eb * $CFGS1 /HPL-2.2-gmpolf-2017.10.eb * $CFGS1 /HPL-2.2-goolfc-2016.08.eb * $CFGS1 /HPL-2.2-goolfc-2016.10.eb * $CFGS1 /HPL-2.2-intel-2017.00.eb * $CFGS1 /HPL-2.2-intel-2017.01.eb * $CFGS1 /HPL-2.2-intel-2017.02.eb * $CFGS1 /HPL-2.2-intel-2017.09.eb * $CFGS1 /HPL-2.2-intel-2017a.eb * $CFGS1 /HPL-2.2-intel-2017b.eb * $CFGS1 /HPL-2.2-intel-2018.00.eb * $CFGS1 /HPL-2.2-intel-2018.01.eb * $CFGS1 /HPL-2.2-intel-2018.02.eb * $CFGS1 /HPL-2.2-intel-2018a.eb * $CFGS1 /HPL-2.2-intelcuda-2016.10.eb * $CFGS1 /HPL-2.2-iomkl-2016.09-GCC-4.9.3-2.25.eb * $CFGS1 /HPL-2.2-iomkl-2016.09-GCC-5.4.0-2.26.eb * $CFGS1 /HPL-2.2-iomkl-2017.01.eb * $CFGS1 /HPL-2.2-intel-2017.02.eb * $CFGS1 /HPL-2.2-intel-2017.09.eb * $CFGS1 /HPL-2.2-intel-2017a.eb * $CFGS1 /HPL-2.2-intel-2017b.eb * $CFGS1 /HPL-2.2-intel-2018.00.eb * $CFGS1 /HPL-2.2-intel-2018.01.eb * $CFGS1 /HPL-2.2-intel-2018.02.eb * $CFGS1 /HPL-2.2-intel-2018a.eb * $CFGS1 /HPL-2.2-intelcuda-2016.10.eb * $CFGS1 /HPL-2.2-iomkl-2016.09-GCC-4.9.3-2.25.eb * $CFGS1 /HPL-2.2-iomkl-2016.09-GCC-5.4.0-2.26.eb * $CFGS1 /HPL-2.2-iomkl-2017.01.eb * $CFGS1 /HPL-2.2-iomkl-2017a.eb * $CFGS1 /HPL-2.2-iomkl-2017b.eb * $CFGS1 /HPL-2.2-iomkl-2018.02.eb * $CFGS1 /HPL-2.2-iomkl-2018a.eb * $CFGS1 /HPL-2.2-pomkl-2016.09.eb We are going to build HPL 2.2 against the intel toolchain, typically the 2017a version which is available by default on the platform. Pick the corresponding recipy (for instance HPL-2.2-intel-2017a.eb ), install it with eb <name>.eb [-D] -r -D enables the dry-run mode to check what's going to be install -- ALWAYS try it first -r enables the robot mode to automatically install all dependencies while searching for easyconfigs in a set of pre-defined directories -- you can also prepend new directories to search for eb files (like the current directory $PWD ) using the option and syntax --robot-paths=$PWD: (do not forget the ':'). See Controlling the robot search path documentation The $CFGS<n>/ prefix should be dropped unless you know what you're doing (and thus have previously defined the variable -- see the first output of the eb -S [...] command). So let's install HPL version 2.2 and FIRST check which dependencies are satisfied with -Dr : $> eb HPL-2.2-intel-2017a.eb -Dr == temporary log file in case of crash /tmp/eb-CTC2hq/easybuild-gfLf1W.log Dry run: printing build status of easyconfigs and dependencies CFGS = /home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs * [ x ] $CFGS /m/M4/M4-1.4.17.eb ( module: devel/M4/1.4.17 ) * [ x ] $CFGS /b/Bison/Bison-3.0.4.eb ( module: lang/Bison/3.0.4 ) * [ x ] $CFGS /f/flex/flex-2.6.0.eb ( module: lang/flex/2.6.0 ) * [ x ] $CFGS /z/zlib/zlib-1.2.8.eb ( module: lib/zlib/1.2.8 ) * [ x ] $CFGS /b/binutils/binutils-2.27.eb ( module: tools/binutils/2.27 ) * [ x ] $CFGS /g/GCCcore/GCCcore-6.3.0.eb ( module: compiler/GCCcore/6.3.0 ) * [ x ] $CFGS /m/M4/M4-1.4.18-GCCcore-6.3.0.eb ( module: devel/M4/1.4.18-GCCcore-6.3.0 ) * [ x ] $CFGS /z/zlib/zlib-1.2.11-GCCcore-6.3.0.eb ( module: lib/zlib/1.2.11-GCCcore-6.3.0 ) * [ x ] $CFGS /h/help2man/help2man-1.47.4-GCCcore-6.3.0.eb ( module: tools/help2man/1.47.4-GCCcore-6.3.0 ) * [ x ] $CFGS /b/Bison/Bison-3.0.4-GCCcore-6.3.0.eb ( module: lang/Bison/3.0.4-GCCcore-6.3.0 ) * [ x ] $CFGS /f/flex/flex-2.6.3-GCCcore-6.3.0.eb ( module: lang/flex/2.6.3-GCCcore-6.3.0 ) * [ x ] $CFGS /b/binutils/binutils-2.27-GCCcore-6.3.0.eb ( module: tools/binutils/2.27-GCCcore-6.3.0 ) * [ x ] $CFGS /i/icc/icc-2017.1.132-GCC-6.3.0-2.27.eb ( module: compiler/icc/2017.1.132-GCC-6.3.0-2.27 ) * [ x ] $CFGS /i/ifort/ifort-2017.1.132-GCC-6.3.0-2.27.eb ( module: compiler/ifort/2017.1.132-GCC-6.3.0-2.27 ) * [ x ] $CFGS /i/iccifort/iccifort-2017.1.132-GCC-6.3.0-2.27.eb ( module: toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27 ) * [ x ] $CFGS /i/impi/impi-2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27.eb ( module: mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27 ) * [ x ] $CFGS /i/iimpi/iimpi-2017a.eb ( module: toolchain/iimpi/2017a ) * [ x ] $CFGS /i/imkl/imkl-2017.1.132-iimpi-2017a.eb ( module: numlib/imkl/2017.1.132-iimpi-2017a ) * [ x ] $CFGS /i/intel/intel-2017a.eb ( module: toolchain/intel/2017a ) * [ ] $CFGS /h/HPL/HPL-2.2-intel-2017a.eb ( module: tools/HPL/2.2-intel-2017a ) == Temporary log file ( s ) /tmp/eb-CTC2hq/easybuild-gfLf1W.log* have been removed. == Temporary directory /tmp/eb-CTC2hq has been removed. As can be seen, there is a single element to install and this has not been done so far (box not checked). All the dependencies are already present (box checked). Let's really install the selected software -- you may want to prefix the eb command with the time to collect the installation time: $> time eb HPL-2.2-intel-2017a.eb -r # Remove the '-D' (dry-run) flags == temporary log file in case of crash /tmp/eb-nub_oL/easybuild-J8sNzx.log == resolving dependencies ... == processing EasyBuild easyconfig /home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/h/HPL/HPL-2.2-intel-2017a.eb == building and installing tools/HPL/2.2-intel-2017a... == fetching files... == creating build dir, resetting environment... == unpacking... == patching... == preparing... == configuring... == building... == testing... == installing... == taking care of extensions... == postprocessing... == sanity checking... == cleaning up... == creating module... == permissions... == packaging... == COMPLETED: Installation ended successfully == Results of the build can be found in the log file ( s ) /home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a/easybuild/easybuild-HPL-2.2-20180608.094831.log == Build succeeded for 1 out of 1 == Temporary log file ( s ) /tmp/eb-nub_oL/easybuild-J8sNzx.log* have been removed. == Temporary directory /tmp/eb-nub_oL has been removed. real 0m56.472s user 0m15.268s sys 0m19.998s Check the installed software: $> module av HPL ------------------------- /home/users/<login>/.local/easybuild/modules/all ------------------------- tools/HPL/2.2-intel-2017a Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". $> module spider HPL ---------------------------------------------------------------------------------------------------- tools/HPL: tools/HPL/2.2-intel-2017a ---------------------------------------------------------------------------------------------------- Description: HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable as well as freely available implementation of the High Performance Computing Linpack Benchmark. This module can be loaded directly: module load tools/HPL/2.2-intel-2017a Help: Description =========== HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable as well as freely available implementation of the High Performance Computing Linpack Benchmark. More information ================ - Homepage: http://www.netlib.org/benchmark/hpl/ $> module show tools/HPL --------------------------------------------------------------------------------------------------- /home/users/svarrette/.local/easybuild/modules/all/tools/HPL/2.2-intel-2017a.lua: --------------------------------------------------------------------------------------------------- help([[ Description =========== HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable as well as freely available implementation of the High Performance Computing Linpack Benchmark. More information ================ - Homepage: http://www.netlib.org/benchmark/hpl/ ]]) whatis(\"Description: HPL is a software package that solves a (random) dense linear system in double precision (64 bits) arithmetic on distributed-memory computers. It can thus be regarded as a portable as well as freely available implementation of the High Performance Computing Linpack Benchmark.\") whatis(\"Homepage: http://www.netlib.org/benchmark/hpl/\") conflict(\"tools/HPL\") load(\"toolchain/intel/2017a\") prepend_path(\"PATH\",\"/home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a/bin\") setenv(\"EBROOTHPL\",\"/home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a\") setenv(\"EBVERSIONHPL\",\"2.2\") setenv(\"EBDEVELHPL\",\"/home/users/svarrette/.local/easybuild/software/tools/HPL/2.2-intel-2017a/easybuild/tools-HPL-2.2-intel-2017a-easybuild-devel\") Note : to see the (locally) installed software, the MODULEPATH variable should include the $HOME/.local/easybuild/modules/all/ (of $LOCAL_MODULES ) path (which is what happens when using module use <path> -- see the mu command) You can now load the freshly installed module like any other: $> module load tools/HPL $> module list Currently Loaded Modules: 1 ) tools/EasyBuild/3.6.1 7 ) mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27 2 ) compiler/GCCcore/6.3.0 8 ) toolchain/iimpi/2017a 3 ) tools/binutils/2.27-GCCcore-6.3.0 9 ) numlib/imkl/2017.1.132-iimpi-2017a 4 ) compiler/icc/2017.1.132-GCC-6.3.0-2.27 10 ) toolchain/intel/2017a 5 ) compiler/ifort/2017.1.132-GCC-6.3.0-2.27 11 ) tools/HPL/2.2-intel-2017a 6 ) toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27 Tips : When you load a module <NAME> generated by Easybuild, it is installed within the directory reported by the $EBROOT<NAME> variable. In the above case, you will find the generated binary for HPL in ${EBROOTHPL}/bin/xhpl . You may want to test the newly built HPL benchmark (you need to reserve at least 4 cores for that to succeed): # In another terminal, connect to the cluster frontend # Have an interactive job ############### iris cluster (slurm) ############### ( access-iris ) $> si -n 4 # this time reserve for 4 (mpi) tasks $> mu $> module load tools/HPL $> cd $EBROOTHPL $> ls $> cd bin $> ls $> srun -n $SLURM_NTASKS ./xhpl Running HPL benchmarks requires more attention -- a full tutorial is dedicated to it. Yet you can see that we obtained HPL 2.2 without writing any EasyConfig file.","title":"c. Build software using provided EasyConfig file"},{"location":"development/build-tools/easybuild/#d-build-software-using-a-customized-easyconfig-file","text":"There are multiple ways to amend an EasyConfig file. Check the --try-* option flags for all the possibilities. Generally you want to do that when the up-to-date version of the software you want is not available as a recipy within Easybuild. For instance, a very popular building environment CMake has recently released a new version (3.11.3), which you want to give a try. It is not available as module, so let's build it. First let's check for available easyconfigs recipy if one exist for the expected version: $> eb -S Cmake-3 [...] * $CFGS2/CMake-3.9.1.eb * $CFGS2/CMake-3.9.4-GCCcore-6.4.0.eb * $CFGS2/CMake-3.9.5-GCCcore-6.4.0.eb We are going to reuse one of the latest EasyConfig available, for instance lets copy $CFGS2/CMake-3.9.1.eb # Work in a dedicated directory $> mkdir -p ~/software/CMake $> cd ~/software/CMake $> eb -S Cmake-3 | less # collect the definition of the CFGS2 variable $> CFGS2 = /home/users/svarrette/.local/easybuild/software/tools/EasyBuild/3.6.1/lib/python2.7/site-packages/easybuild_easyconfigs-3.6.1-py2.7.egg/easybuild/easyconfigs/c/CMake $> cp $CFGS2 /CMake-3.9.1.eb . $> mv CMake-3.9.1.eb CMake-3.11.3.eb # Adapt version suffix to the lastest realse You need to perform the following changes (here: version upgrade, and adapted checksum) --- CMake-3.9.1.eb 2018-06-08 10:56:24.447699000 +0200 +++ CMake-3.11.3.eb 2018-06-08 11:07:39.716672000 +0200 @@ -1,7 +1,7 @@ easyblock = 'ConfigureMake' name = 'CMake' -version = '3.9.1' +version = '3.11.3' homepage = 'http://www.cmake.org' description = \"\"\"CMake, the cross-platform, open-source build system. @@ -11,7 +11,7 @@ source_urls = ['http://www.cmake.org/files/v%(version_major_minor)s'] sources = [SOURCELOWER_TAR_GZ] -checksums = ['d768ee83d217f91bb597b3ca2ac663da7a8603c97e1f1a5184bc01e0ad2b12bb'] +checksums = ['287135b6beb7ffc1ccd02707271080bbf14c21d80c067ae2c0040e5f3508c39a'] configopts = '-- -DCMAKE_USE_OPENSSL=1' If the checksum is not provided on the official software page , you will need to compute it yourself by downloading the sources and collect the checksum: $> gsha256sum ~/Download/cmake-3.11.3.tar.gz 287135b6beb7ffc1ccd02707271080bbf14c21d80c067ae2c0040e5f3508c39a cmake-3.11.3.tar.gz Let's build it: $> eb ./CMake-3.11.3.eb -Dr == temporary log file in case of crash /tmp/eb-UX7APP/easybuild-gxnyIv.log Dry run: printing build status of easyconfigs and dependencies CFGS = /mnt/irisgpfs/users/<login>/software/CMake * [ ] $CFGS /CMake-3.11.3.eb ( module: devel/CMake/3.11.3 ) == Temporary log file ( s ) /tmp/eb-UX7APP/easybuild-gxnyIv.log* have been removed. == Temporary directory /tmp/eb-UX7APP has been removed. Dependencies are fine, so let's build it: $> time eb ./CMake-3.11.3.eb -r == temporary log file in case of crash /tmp/eb-JjF92B/easybuild-RjzRjb.log == resolving dependencies ... == processing EasyBuild easyconfig /mnt/irisgpfs/users/<login>/software/CMake/CMake-3.11.3.eb == building and installing devel/CMake/3.11.3... == fetching files... == creating build dir, resetting environment... == unpacking... == patching... == preparing... == configuring... == building... == testing... == installing... == taking care of extensions... == postprocessing... == sanity checking... == cleaning up... == creating module... == permissions... == packaging... == COMPLETED: Installation ended successfully == Results of the build can be found in the log file ( s ) /home/users/<login>/.local/easybuild/software/devel/CMake/3.11.3/easybuild/easybuild-CMake-3.11.3-20180608.111611.log == Build succeeded for 1 out of 1 == Temporary log file ( s ) /tmp/eb-JjF92B/easybuild-RjzRjb.log* have been removed. == Temporary directory /tmp/eb-JjF92B has been removed. real 7m40.358s user 5m56.442s sys 1m15.185s Note you can follow the progress of the installation in a separate shell on the node: Check the result: $> module av CMake That's all ;-) Final remaks This workflow (copying an existing recipy, adapting the filename, the version and the source checksum) covers most of the test cases. Yet sometimes you need to work on a more complex dependency check, in which case you'll need to adapt many eb files. In this case, for each build, you need to instruct Easybuild to search for easyconfigs also in the current directory, in which case you will use: $> eb <filename>.eb --robot = $PWD : $EASYBUILD_ROBOT -D $> eb <filename>.eb --robot = $PWD : $EASYBUILD_ROBOT","title":"d. Build software using a customized EasyConfig file"},{"location":"development/build-tools/easybuild/#old-build-software-using-your-own-easyconfig-file","text":"Below are obsolete instructions to write a full Easyconfig file, left for archiving and informal purposes. For this example, we create an EasyConfig file to build GZip 1.4 with the GOOLF toolchain. Open your favorite editor and create a file named gzip-1.4-goolf-1.4.10.eb with the following content: easyblock = 'ConfigureMake' name = 'gzip' version = '1.4' homepage = 'http://www.gnu.org/software/gzip/' description = \"gzip (GNU zip) is a popular data compression program as a replacement for compress\" # use the GOOLF toolchain toolchain = {'name': 'goolf', 'version': '1.4.10'} # specify that GCC compiler should be used to build gzip preconfigopts = \"CC='gcc'\" # source tarball filename sources = ['%s-%s.tar.gz'%(name,version)] # download location for source files source_urls = ['http://ftpmirror.gnu.org/gzip'] # make sure the gzip and gunzip binaries are available after installation sanity_check_paths = { 'files': [\"bin/gunzip\", \"bin/gzip\"], 'dirs': [] } # run 'gzip -h' and 'gzip --version' after installation sanity_check_commands = [True, ('gzip', '--version')] This is a simple EasyConfig. Most of the fields are self-descriptive. No build method is explicitely defined, so it uses by default the standard configure/make/make install approach. Let's build GZip with this EasyConfig file: $> time eb gzip-1.4-goolf-1.4.10.eb == temporary log file in case of crash /tmp/eb-hiyyN1/easybuild-ynLsHC.log == processing EasyBuild easyconfig /mnt/nfs/users/homedirs/mschmitt/gzip-1.4-goolf-1.4.10.eb == building and installing base/gzip/1.4-goolf-1.4.10... == fetching files... == creating build dir, resetting environment... == unpacking... == patching... == preparing... == configuring... == building... == testing... == installing... == taking care of extensions... == packaging... == postprocessing... == sanity checking... == cleaning up... == creating module... == COMPLETED: Installation ended successfully == Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/base/gzip/1.4-goolf-1.4.10/easybuild/easybuild-gzip-1.4-20150624.114745.log == Build succeeded for 1 out of 1 == temporary log file(s) /tmp/eb-hiyyN1/easybuild-ynLsHC.log* have been removed. == temporary directory /tmp/eb-hiyyN1 has been removed. real 1m39.982s user 0m52.743s sys 0m11.297s We can now check that our version of GZip is available via the modules: $> module avail gzip --------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all --------- base/gzip/1.4-goolf-1.4.10","title":"(OLD) Build software using your own EasyConfig file"},{"location":"development/build-tools/easybuild/#to-go-further-into-details","text":"Please refer to the following pointers to get additionnal features: EasyBuild homepage EasyBuild tutorial EasyBuild documentation Getting started Using EasyBuild Step-by-step guide","title":"To go further into details"},{"location":"development/build-tools/spack/","text":"","title":"Spack"},{"location":"development/performance-debugging-tools/advisor/","text":"Intel Advisor \u00b6 Intel Advisor provides two workflows to help ensure that Fortran, C, and C++ applications can make the most of modern Intel processors. Advisor contains three key capabilities: Vectorization Advisor identifies loops that will benefit most from vectorization, specifies what is blocking effective vectorization, finds the benefit of alternative data reorganizations, and increases the confidence that vectorization is safe. Threading Advisor is used for threading design and prototyping and to analyze, design, tune, and check threading design options without disrupting normal code development. Advisor Roofline enables visualization of actual performance against hardware-imposed performance ceilings (rooflines) such as memory bandwidth and compute capacity - which provide an ideal roadmap of potential optimization steps. The links to each capability above provide detailed information regarding how to use each feature in Advisor. For more information on Intel Advisor, visit this page . Environmental models for Advisor on UL-HPC\u00b6 \u00b6 module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load perf/Advisor/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 Interactive mode \u00b6 # Compilation $ icc -qopenmp example.c # Code execution $ export OMP_NUM_THREADS = 16 $ advixe-cl -collect survey -project-dir my_result -- ./a.out # Report collection $ advixe-cl -report survey -project-dir my_result # To see the result in GUI $ advixe-gui my_result $ advixe-cl will list out the analysis types and $ advixe-cl -hlep report will list out available reports in Advisor. Batch mode \u00b6 Shared memory programming model (OpenMP) \u00b6 Example for the batch script: #!/bin/bash -l #SBATCH -J Advisor #SBATCH -N 1 ###SBATCH -A <project_name> #SBATCH -c 28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load perf/Advisor/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 export OMP_NUM_THREADS = 16 advixe-cl -collect survey -project-dir my_result -- ./a.out Distributed memory programming model (MPI) \u00b6 To compile just MPI application run $ mpiicc example.c and for MPI+OpenMP run $ mpiicc -qopenmp example.c Example for the batch script: #!/bin/bash -l #SBATCH -J Advisor #SBATCH -N 2 ###SBATCH -A <project_name> #SBATCH --ntasks-per-node=28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load perf/Advisor/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 srun -n ${ SLURM_NTASKS } advixe-cl --collect survey --project-dir result -- ./a.out To collect the result and see the result in GUI use the below commands # Report collection $ advixe-cl --report survey --project-dir result # Result visualization $ advixe-gui result The below figure shows the hybrid(MPI+OpenMP) programming analysis results: Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Intel Advisor"},{"location":"development/performance-debugging-tools/advisor/#intel-advisor","text":"Intel Advisor provides two workflows to help ensure that Fortran, C, and C++ applications can make the most of modern Intel processors. Advisor contains three key capabilities: Vectorization Advisor identifies loops that will benefit most from vectorization, specifies what is blocking effective vectorization, finds the benefit of alternative data reorganizations, and increases the confidence that vectorization is safe. Threading Advisor is used for threading design and prototyping and to analyze, design, tune, and check threading design options without disrupting normal code development. Advisor Roofline enables visualization of actual performance against hardware-imposed performance ceilings (rooflines) such as memory bandwidth and compute capacity - which provide an ideal roadmap of potential optimization steps. The links to each capability above provide detailed information regarding how to use each feature in Advisor. For more information on Intel Advisor, visit this page .","title":"Intel Advisor"},{"location":"development/performance-debugging-tools/advisor/#environmental-models-for-advisor-on-ul-hpc","text":"module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load perf/Advisor/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0","title":"Environmental models for Advisor on UL-HPC\u00b6"},{"location":"development/performance-debugging-tools/advisor/#interactive-mode","text":"# Compilation $ icc -qopenmp example.c # Code execution $ export OMP_NUM_THREADS = 16 $ advixe-cl -collect survey -project-dir my_result -- ./a.out # Report collection $ advixe-cl -report survey -project-dir my_result # To see the result in GUI $ advixe-gui my_result $ advixe-cl will list out the analysis types and $ advixe-cl -hlep report will list out available reports in Advisor.","title":"Interactive mode"},{"location":"development/performance-debugging-tools/advisor/#batch-mode","text":"","title":"Batch mode"},{"location":"development/performance-debugging-tools/advisor/#shared-memory-programming-model-openmp","text":"Example for the batch script: #!/bin/bash -l #SBATCH -J Advisor #SBATCH -N 1 ###SBATCH -A <project_name> #SBATCH -c 28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load perf/Advisor/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 export OMP_NUM_THREADS = 16 advixe-cl -collect survey -project-dir my_result -- ./a.out","title":"Shared memory programming model (OpenMP)"},{"location":"development/performance-debugging-tools/advisor/#distributed-memory-programming-model-mpi","text":"To compile just MPI application run $ mpiicc example.c and for MPI+OpenMP run $ mpiicc -qopenmp example.c Example for the batch script: #!/bin/bash -l #SBATCH -J Advisor #SBATCH -N 2 ###SBATCH -A <project_name> #SBATCH --ntasks-per-node=28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load perf/Advisor/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 srun -n ${ SLURM_NTASKS } advixe-cl --collect survey --project-dir result -- ./a.out To collect the result and see the result in GUI use the below commands # Report collection $ advixe-cl --report survey --project-dir result # Result visualization $ advixe-gui result The below figure shows the hybrid(MPI+OpenMP) programming analysis results: Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Distributed memory programming model (MPI)"},{"location":"development/performance-debugging-tools/aps/","text":"Application Performance Snapshot (APS) \u00b6 Application Performance Snapshot (APS) is a lightweight open source profiling tool developed by the Intel VTune developers. Use Application Performance Snapshot for a quick view into a shared memory or MPI application's use of available hardware (CPU, FPU, and memory). Application Performance Snapshot analyzes your application's time spent in MPI, MPI and OpenMP imbalance, memory access efficiency, FPU usage, and I/O and memory footprint. After analysis, it displays basic performance enhancement opportunities for systems using Intel platforms. Use this tool as a first step in application performance analysis to get a simple snapshot of key optimization areas and learn about profiling tools that specialize in particular aspects of application performance. Prerequisites \u00b6 Optional Configuration Optional: Use the following software to get an advanced metric set when running Application Performance Snapshot: Recommended compilers: Intel C/C++ or Fortran Compiler (other compilers can be used, but information about OpenMP imbalance is only available from the Intel OpenMP library) Use Intel MPI library version 2017 or later. Other MPICH-based MPI implementations can be used, but information about MPI imbalance is only available from the Intel MPI library. There is no support for OpenMPI. Optional: Enable system-wide monitoring to reduce collection overhead and collect memory bandwidth measurements. Use one of these options to enable system-wide monitoring: Set the /proc/sys/kernel/perf_event_paranoid value to 0 (or less), or Install the Intel VTune Amplifier drivers. Driver sources are available in <APS_install_dir>/internal/sepdk/src . Installation instructions are available online at https://software.intel.com/en-us/vtune-amplifier-help-building-and-installing-the-sampling-drivers-for-linux-targets . Before running the tool, set up your environment appropriately: module purge module load swenv/default-env/v1.2-20191021-production module load tools/VTune/2019_update4 module load toolchain/intel/2019a Analyzing Shared Memory Applications \u00b6 Run the following commands (interactive mode): # Compilation $ icc -qopenmp example.c # Code execution aps --collection-mode = all -r report_output ./a.out aps -help will list out --collection-mode=<mode> available in APS. # To create a .html file aps-report -g report_output # To open an APS results in the browser firefox report_output_<postfix>.html The below figure shows the example of result can be seen in the browser: # To see the command line output $ aps-report <result_dir> Example for the batch script: #!/bin/bash -l #SBATCH -J APS #SBATCH -N 1 ###SBATCH -A <project_name> #SBATCH -c 28 #SBATCH --time=00:10:00 #SBATCH -p batch #SBATCH --nodelist=node0xx module purge module load swenv/default-env/v1.2-20191021-production module load tools/VTune/2019_update4 module load toolchain/intel/2019a export OMP_NUM_THREADS = 16 aps --collection-mode = all -r report_output ./a.out Analyzing MPI Applications \u00b6 To compile just MPI application run $ mpiicc example.c and for MPI+OpenMP run $ mpiicc -qopenmp example.c Example for the batch script: #!/bin/bash -l #SBATCH -J APS #SBATCH -N 2 ###SBATCH -A <project_name> #SBATCH --ntasks-per-node=14 #SBATCH -c 2 #SBATCH --time=00:10:00 #SBATCH -p batch #SBATCH --reservation=<name> module purge module load swenv/default-env/v1.2-20191021-production module load tools/VTune/2019_update4 module load toolchain/intel/2019a # To collect all the results export MPS_STAT_LEVEL = ${ SLURM_CPUS_PER_TASK :- 1 } # An option for the OpenMP+MPI application export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } srun -n ${ SLURM_NTASKS } aps --collection-mode = mpi -r result_output ./a.out The below figure shows the hybrid(MPI+OpenMP) programming analysis results: Next Steps \u00b6 Intel Trace Analyzer and Collector is a graphical tool for understanding MPI application behavior, quickly identifying bottlenecks, improving correctness, and achieving high performance for parallel cluster applications running on Intel architecture. Improve weak and strong scaling for applications. Get started . Intel VTune Amplifier provides a deep insight into node-level performance including algorithmic hotspot analysis, OpenMP threading, general exploration microarchitecture analysis, memory access efficiency, and more. It supports C/C++, Fortran, Java, Python, and profiling in containers. Get started . Intel Advisor provides two tools to help ensure your Fortran, C, and C++ applications realize full performance potential on modern processors. Get started . Vectorization Advisor is an optimization tool to identify loops that will benefit most from vectorization, analyze what is blocking effective vectorization, and forecast the benefit of alternative data reorganizations Threading Advisor is a threading design and prototyping tool to analyze, design, tune, and check threading design options without disrupting a regular environment Quick Metrics Reference The following metrics are collected with Application Performance Snapshot. Additional detail about each of these metrics is available in the Intel VTune Amplifier online help . Elapsed Time : Execution time of specified application in seconds. SP GFLOPS : Number of single precision giga-floating point operations calculated per second. All double operations are converted to two single operations. SP GFLOPS metrics are only available for 3 rd Generation Intel Core processors, 5 th Generation Intel processors, and 6 th Generation Intel processors. Cycles per Instruction Retired (CPI) : The amount of time each executed instruction took measured by cycles. A CPI of 1 is considered acceptable for high performance computing (HPC) applications, but different application domains will have varied expected values. The CPI value tends to be greater when there is long-latency memory, floating-point, or SIMD operations, non-retired instructions due to branch mispredictions, or instruction starvation at the front end. MPI Time : Average time per process spent in MPI calls. This metric does not include the time spent in MPI_Finalize . High values could be caused by high wait times inside the library, active communications, or sub-optimal settings of the MPI library. The metric is available for MPICH-based MPIs. MPI Imbalance : CPU time spent by ranks spinning in waits on communication operations. A high value can be caused by application workload imbalance between ranks, or non-optimal communication schema or MPI library settings. This metric is available only for Intel MPI Library version 2017 and later. OpenMP Imbalance : Percentage of elapsed time that your application wastes at OpenMP synchronization barriers because of load imbalance. This metric is only available for the Intel OpenMP Runtime Library. CPU Utilization : Estimate of the utilization of all logical CPU cores on the system by your application. Use this metric to help evaluate the parallel efficiency of your application. A utilization of 100% means that your application keeps all of the logical CPU cores busy for the entire time that it runs. Note that the metric does not distinguish between useful application work and the time that is spent in parallel runtimes. Memory Stalls : Indicates how memory subsystem issues affect application performance. This metric measures a fraction of slots where pipeline could be stalled due to demand load or store instructions. If the metric value is high, review the Cache and DRAM Stalls and the percent of remote accesses metrics to understand the nature of memory-related performance bottlenecks. If the average memory bandwidth numbers are close to the system bandwidth limit, optimization techniques for memory bound applications may be required to avoid memory stalls. FPU Utilization : The effective FPU usage while the application was running. Use the FPU Utilization value to evaluate the vector efficiency of your application. The value is calculated by estimating the percentage of operations that are performed by the FPU. A value of 100% means that the FPU is fully loaded. Any value over 50% requires additional analysis. FPU metrics are only available for 3 rd Generation Intel Core processors, 5 th Generation Intel processors, and 6 th Generation Intel processors. I/O Operations : The time spent by the application while reading data from the disk or writing data to the disk. Read and Write values denote mean and maximum amounts of data read and written during the elapsed time. This metric is only available for MPI applications. Memory Footprint : Average per-rank and per-node consumption of both virtual and resident memory. Documentation and Resources \u00b6 Intel Performance Snapshot User Forum : User forum dedicated to all Intel Performance Snapshot tools, including Application Performance Snapshot Application Performance Snapshot : Application Performance Snapshot product page, see this page for support and online documentation Application Performance Snapshot User's Guide : Learn more about Application Performance Snapshot, including details on specific metrics and best practices for application optimization Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"APS"},{"location":"development/performance-debugging-tools/aps/#application-performance-snapshot-aps","text":"Application Performance Snapshot (APS) is a lightweight open source profiling tool developed by the Intel VTune developers. Use Application Performance Snapshot for a quick view into a shared memory or MPI application's use of available hardware (CPU, FPU, and memory). Application Performance Snapshot analyzes your application's time spent in MPI, MPI and OpenMP imbalance, memory access efficiency, FPU usage, and I/O and memory footprint. After analysis, it displays basic performance enhancement opportunities for systems using Intel platforms. Use this tool as a first step in application performance analysis to get a simple snapshot of key optimization areas and learn about profiling tools that specialize in particular aspects of application performance.","title":"Application Performance Snapshot (APS)"},{"location":"development/performance-debugging-tools/aps/#prerequisites","text":"Optional Configuration Optional: Use the following software to get an advanced metric set when running Application Performance Snapshot: Recommended compilers: Intel C/C++ or Fortran Compiler (other compilers can be used, but information about OpenMP imbalance is only available from the Intel OpenMP library) Use Intel MPI library version 2017 or later. Other MPICH-based MPI implementations can be used, but information about MPI imbalance is only available from the Intel MPI library. There is no support for OpenMPI. Optional: Enable system-wide monitoring to reduce collection overhead and collect memory bandwidth measurements. Use one of these options to enable system-wide monitoring: Set the /proc/sys/kernel/perf_event_paranoid value to 0 (or less), or Install the Intel VTune Amplifier drivers. Driver sources are available in <APS_install_dir>/internal/sepdk/src . Installation instructions are available online at https://software.intel.com/en-us/vtune-amplifier-help-building-and-installing-the-sampling-drivers-for-linux-targets . Before running the tool, set up your environment appropriately: module purge module load swenv/default-env/v1.2-20191021-production module load tools/VTune/2019_update4 module load toolchain/intel/2019a","title":"Prerequisites"},{"location":"development/performance-debugging-tools/aps/#analyzing-shared-memory-applications","text":"Run the following commands (interactive mode): # Compilation $ icc -qopenmp example.c # Code execution aps --collection-mode = all -r report_output ./a.out aps -help will list out --collection-mode=<mode> available in APS. # To create a .html file aps-report -g report_output # To open an APS results in the browser firefox report_output_<postfix>.html The below figure shows the example of result can be seen in the browser: # To see the command line output $ aps-report <result_dir> Example for the batch script: #!/bin/bash -l #SBATCH -J APS #SBATCH -N 1 ###SBATCH -A <project_name> #SBATCH -c 28 #SBATCH --time=00:10:00 #SBATCH -p batch #SBATCH --nodelist=node0xx module purge module load swenv/default-env/v1.2-20191021-production module load tools/VTune/2019_update4 module load toolchain/intel/2019a export OMP_NUM_THREADS = 16 aps --collection-mode = all -r report_output ./a.out","title":"Analyzing Shared Memory Applications"},{"location":"development/performance-debugging-tools/aps/#analyzing-mpi-applications","text":"To compile just MPI application run $ mpiicc example.c and for MPI+OpenMP run $ mpiicc -qopenmp example.c Example for the batch script: #!/bin/bash -l #SBATCH -J APS #SBATCH -N 2 ###SBATCH -A <project_name> #SBATCH --ntasks-per-node=14 #SBATCH -c 2 #SBATCH --time=00:10:00 #SBATCH -p batch #SBATCH --reservation=<name> module purge module load swenv/default-env/v1.2-20191021-production module load tools/VTune/2019_update4 module load toolchain/intel/2019a # To collect all the results export MPS_STAT_LEVEL = ${ SLURM_CPUS_PER_TASK :- 1 } # An option for the OpenMP+MPI application export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } srun -n ${ SLURM_NTASKS } aps --collection-mode = mpi -r result_output ./a.out The below figure shows the hybrid(MPI+OpenMP) programming analysis results:","title":"Analyzing MPI Applications"},{"location":"development/performance-debugging-tools/aps/#next-steps","text":"Intel Trace Analyzer and Collector is a graphical tool for understanding MPI application behavior, quickly identifying bottlenecks, improving correctness, and achieving high performance for parallel cluster applications running on Intel architecture. Improve weak and strong scaling for applications. Get started . Intel VTune Amplifier provides a deep insight into node-level performance including algorithmic hotspot analysis, OpenMP threading, general exploration microarchitecture analysis, memory access efficiency, and more. It supports C/C++, Fortran, Java, Python, and profiling in containers. Get started . Intel Advisor provides two tools to help ensure your Fortran, C, and C++ applications realize full performance potential on modern processors. Get started . Vectorization Advisor is an optimization tool to identify loops that will benefit most from vectorization, analyze what is blocking effective vectorization, and forecast the benefit of alternative data reorganizations Threading Advisor is a threading design and prototyping tool to analyze, design, tune, and check threading design options without disrupting a regular environment Quick Metrics Reference The following metrics are collected with Application Performance Snapshot. Additional detail about each of these metrics is available in the Intel VTune Amplifier online help . Elapsed Time : Execution time of specified application in seconds. SP GFLOPS : Number of single precision giga-floating point operations calculated per second. All double operations are converted to two single operations. SP GFLOPS metrics are only available for 3 rd Generation Intel Core processors, 5 th Generation Intel processors, and 6 th Generation Intel processors. Cycles per Instruction Retired (CPI) : The amount of time each executed instruction took measured by cycles. A CPI of 1 is considered acceptable for high performance computing (HPC) applications, but different application domains will have varied expected values. The CPI value tends to be greater when there is long-latency memory, floating-point, or SIMD operations, non-retired instructions due to branch mispredictions, or instruction starvation at the front end. MPI Time : Average time per process spent in MPI calls. This metric does not include the time spent in MPI_Finalize . High values could be caused by high wait times inside the library, active communications, or sub-optimal settings of the MPI library. The metric is available for MPICH-based MPIs. MPI Imbalance : CPU time spent by ranks spinning in waits on communication operations. A high value can be caused by application workload imbalance between ranks, or non-optimal communication schema or MPI library settings. This metric is available only for Intel MPI Library version 2017 and later. OpenMP Imbalance : Percentage of elapsed time that your application wastes at OpenMP synchronization barriers because of load imbalance. This metric is only available for the Intel OpenMP Runtime Library. CPU Utilization : Estimate of the utilization of all logical CPU cores on the system by your application. Use this metric to help evaluate the parallel efficiency of your application. A utilization of 100% means that your application keeps all of the logical CPU cores busy for the entire time that it runs. Note that the metric does not distinguish between useful application work and the time that is spent in parallel runtimes. Memory Stalls : Indicates how memory subsystem issues affect application performance. This metric measures a fraction of slots where pipeline could be stalled due to demand load or store instructions. If the metric value is high, review the Cache and DRAM Stalls and the percent of remote accesses metrics to understand the nature of memory-related performance bottlenecks. If the average memory bandwidth numbers are close to the system bandwidth limit, optimization techniques for memory bound applications may be required to avoid memory stalls. FPU Utilization : The effective FPU usage while the application was running. Use the FPU Utilization value to evaluate the vector efficiency of your application. The value is calculated by estimating the percentage of operations that are performed by the FPU. A value of 100% means that the FPU is fully loaded. Any value over 50% requires additional analysis. FPU metrics are only available for 3 rd Generation Intel Core processors, 5 th Generation Intel processors, and 6 th Generation Intel processors. I/O Operations : The time spent by the application while reading data from the disk or writing data to the disk. Read and Write values denote mean and maximum amounts of data read and written during the elapsed time. This metric is only available for MPI applications. Memory Footprint : Average per-rank and per-node consumption of both virtual and resident memory.","title":"Next Steps"},{"location":"development/performance-debugging-tools/aps/#documentation-and-resources","text":"Intel Performance Snapshot User Forum : User forum dedicated to all Intel Performance Snapshot tools, including Application Performance Snapshot Application Performance Snapshot : Application Performance Snapshot product page, see this page for support and online documentation Application Performance Snapshot User's Guide : Learn more about Application Performance Snapshot, including details on specific metrics and best practices for application optimization Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Documentation and Resources"},{"location":"development/performance-debugging-tools/arm-forge/","text":"Arm Forge is the leading server and HPC development tool suite in research, industry, and academia for C, C++, Fortran, and Python high performance code on Linux. Arm Forge includes Arm DDT, the best debugger for time-saving high performance application debugging, Arm MAP, the trusted performance profiler for invaluable optimization advice, and Arm Performance Reports to help you analyze your HPC application runs. Environmental models for Arm Forge in ULHPC \u00b6 module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/ArmForge/19.1 module load tools/ArmReports/19.1 Interactive Mode \u00b6 To compile $ icc -qopenmp example.c For debugging, profiling and analysing # for debugging $ ddt ./a .out # for profiling $ map ./a .out # for analysis $ perf-report ./a .out Batch Mode \u00b6 Shared memory programming model (OpenMP) \u00b6 Example for the batch script: #!/bin/bash -l #SBATCH -J ArmForge #SBATCH -N 1 ###SBATCH -A <project_name> #SBATCH -c 16 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/ArmForge/19.1 module load tools/ArmReports/19.1 export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } # for debugging $ ddt ./a .out # for profiling $ map ./a .out # for analysis $ perf-report ./a .out Distributed memory programming model (MPI) \u00b6 Example for the batch script: #!/bin/bash -l #SBATCH -J ArmForge ###SBATCH -A <project_name> #SBATCH -N 2 #SBATCH --ntasks-per-node 28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/ArmForge/19.1 module load tools/ArmReports/19.1 # for debugging $ ddt srun -n ${ SLURM_NTASKS } ./a .out # for profiling $ map srun -n ${ SLURM_NTASKS } ./a .out # for analysis $ perf-report srun -n ${ SLURM_NTASKS } ./a .out To see the result Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Arm Forge"},{"location":"development/performance-debugging-tools/arm-forge/#environmental-models-for-arm-forge-in-ulhpc","text":"module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/ArmForge/19.1 module load tools/ArmReports/19.1","title":"Environmental models for Arm Forge in ULHPC"},{"location":"development/performance-debugging-tools/arm-forge/#interactive-mode","text":"To compile $ icc -qopenmp example.c For debugging, profiling and analysing # for debugging $ ddt ./a .out # for profiling $ map ./a .out # for analysis $ perf-report ./a .out","title":"Interactive Mode"},{"location":"development/performance-debugging-tools/arm-forge/#batch-mode","text":"","title":"Batch Mode"},{"location":"development/performance-debugging-tools/arm-forge/#shared-memory-programming-model-openmp","text":"Example for the batch script: #!/bin/bash -l #SBATCH -J ArmForge #SBATCH -N 1 ###SBATCH -A <project_name> #SBATCH -c 16 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/ArmForge/19.1 module load tools/ArmReports/19.1 export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } # for debugging $ ddt ./a .out # for profiling $ map ./a .out # for analysis $ perf-report ./a .out","title":"Shared memory programming model (OpenMP)"},{"location":"development/performance-debugging-tools/arm-forge/#distributed-memory-programming-model-mpi","text":"Example for the batch script: #!/bin/bash -l #SBATCH -J ArmForge ###SBATCH -A <project_name> #SBATCH -N 2 #SBATCH --ntasks-per-node 28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/ArmForge/19.1 module load tools/ArmReports/19.1 # for debugging $ ddt srun -n ${ SLURM_NTASKS } ./a .out # for profiling $ map srun -n ${ SLURM_NTASKS } ./a .out # for analysis $ perf-report srun -n ${ SLURM_NTASKS } ./a .out To see the result Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Distributed memory programming model (MPI)"},{"location":"development/performance-debugging-tools/inspector/","text":"Intel Inspector \u00b6 Intel Inspector is a memory and threading error checking tool for users developing serial and multithreaded applications on Windows and Linux operating systems. The essential features of Intel Inspector for Linux are: Standalone GUI and command-line environments Preset analysis configurations (with some configurable settings) and the ability to create custom analysis configurations to help the user control analysis scope and cost Interactive debugging capability so one can investigate problems more deeply during the analysis A large number of reported memory errors, including on-demand memory leak detection Memory growth measurement to help ensure that the application uses no more memory than expected Data race, deadlock, lock hierarchy violation, and cross-thread stack access error detection Options for the Collect Action \u00b6 Option Description mi1 Detect memory leaks mi2 Detect memory problems mi3 Locate memory problems ti1 Detect deadlocks ti2 Detect deadlocks and data races ti3 Locate deadlocks and data races Options for the Report Action \u00b6 Option Description summary A brief statement of the total number of new problems found grouped by problem type problems A detailed report of detected problem sets in the result, along with their location in the source code observations A detailed report of all code locations used to form new problem sets status A brief statement of the total number of detected problems and the number that are not investigated , grouped by category For more information on Intel Inspector, please visit https://software.intel.com/en-us/intel-inspector-xe . Environmental models for Inspector on UL-HPC \u00b6 module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/Inspector/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 Interactive Mode \u00b6 To launch Inspector on Iris, we recommend that you use the command line tool inspxe-cl to collect data via batch jobs and then display results using the GUI, inspxe-gui , on a login node. # Compilation $ icc -qopenmp example.cc # Result collection $ inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out # Result view $ cat inspxe-cl.txt === Start: [ 2020 /04/08 02 :11:50 ] === 2 new problem ( s ) found 1 Memory leak problem ( s ) detected 1 Memory not deallocated problem ( s ) detected === End: [ 2020 /04/08 02 :11:55 ] === Batch Mode \u00b6 Shared memory programming model (OpenMP) \u00b6 Example for the batch script: #!/bin/bash -l #SBATCH -J Inspector #SBATCH -N 1 ###SBATCH -A <project_name> #SBATCH -c 28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/Inspector/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out ` To see the result: # Result view $ cat inspxe-cl.txt === Start: [ 2020 /04/08 02 :11:50 ] === 2 new problem ( s ) found 1 Memory leak problem ( s ) detected 1 Memory not deallocated problem ( s ) detected === End: [ 2020 /04/08 02 :11:55 ] === Distributed memory programming model (MPI) \u00b6 To compile: # Compilation $ mpiicc -qopenmp example.cc Example for batch script: #!/bin/bash -l #SBATCH -J Inspector #SBATCH -N 2 ###SBATCH -A <project_name> #SBATCH --ntasks-per-node 28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/Inspector/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 srun -n { SLURM_NTASKS } inspxe-cl -collect = ti2 -r result ./a.out To see result output: $ cat inspxe-cl.txt 0 new problem ( s ) found === End: [ 2020 /04/08 16 :41:56 ] === === End: [ 2020 /04/08 16 :41:56 ] === 0 new problem ( s ) found === End: [ 2020 /04/08 16 :41:56 ] === Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Intel Inspector"},{"location":"development/performance-debugging-tools/inspector/#intel-inspector","text":"Intel Inspector is a memory and threading error checking tool for users developing serial and multithreaded applications on Windows and Linux operating systems. The essential features of Intel Inspector for Linux are: Standalone GUI and command-line environments Preset analysis configurations (with some configurable settings) and the ability to create custom analysis configurations to help the user control analysis scope and cost Interactive debugging capability so one can investigate problems more deeply during the analysis A large number of reported memory errors, including on-demand memory leak detection Memory growth measurement to help ensure that the application uses no more memory than expected Data race, deadlock, lock hierarchy violation, and cross-thread stack access error detection","title":"Intel Inspector"},{"location":"development/performance-debugging-tools/inspector/#options-for-the-collect-action","text":"Option Description mi1 Detect memory leaks mi2 Detect memory problems mi3 Locate memory problems ti1 Detect deadlocks ti2 Detect deadlocks and data races ti3 Locate deadlocks and data races","title":"Options for the Collect Action"},{"location":"development/performance-debugging-tools/inspector/#options-for-the-report-action","text":"Option Description summary A brief statement of the total number of new problems found grouped by problem type problems A detailed report of detected problem sets in the result, along with their location in the source code observations A detailed report of all code locations used to form new problem sets status A brief statement of the total number of detected problems and the number that are not investigated , grouped by category For more information on Intel Inspector, please visit https://software.intel.com/en-us/intel-inspector-xe .","title":"Options for the Report Action"},{"location":"development/performance-debugging-tools/inspector/#environmental-models-for-inspector-on-ul-hpc","text":"module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/Inspector/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0","title":"Environmental models for Inspector on UL-HPC"},{"location":"development/performance-debugging-tools/inspector/#interactive-mode","text":"To launch Inspector on Iris, we recommend that you use the command line tool inspxe-cl to collect data via batch jobs and then display results using the GUI, inspxe-gui , on a login node. # Compilation $ icc -qopenmp example.cc # Result collection $ inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out # Result view $ cat inspxe-cl.txt === Start: [ 2020 /04/08 02 :11:50 ] === 2 new problem ( s ) found 1 Memory leak problem ( s ) detected 1 Memory not deallocated problem ( s ) detected === End: [ 2020 /04/08 02 :11:55 ] ===","title":"Interactive Mode"},{"location":"development/performance-debugging-tools/inspector/#batch-mode","text":"","title":"Batch Mode"},{"location":"development/performance-debugging-tools/inspector/#shared-memory-programming-model-openmp","text":"Example for the batch script: #!/bin/bash -l #SBATCH -J Inspector #SBATCH -N 1 ###SBATCH -A <project_name> #SBATCH -c 28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/Inspector/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out ` To see the result: # Result view $ cat inspxe-cl.txt === Start: [ 2020 /04/08 02 :11:50 ] === 2 new problem ( s ) found 1 Memory leak problem ( s ) detected 1 Memory not deallocated problem ( s ) detected === End: [ 2020 /04/08 02 :11:55 ] ===","title":"Shared memory programming model (OpenMP)"},{"location":"development/performance-debugging-tools/inspector/#distributed-memory-programming-model-mpi","text":"To compile: # Compilation $ mpiicc -qopenmp example.cc Example for batch script: #!/bin/bash -l #SBATCH -J Inspector #SBATCH -N 2 ###SBATCH -A <project_name> #SBATCH --ntasks-per-node 28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/Inspector/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 srun -n { SLURM_NTASKS } inspxe-cl -collect = ti2 -r result ./a.out To see result output: $ cat inspxe-cl.txt 0 new problem ( s ) found === End: [ 2020 /04/08 16 :41:56 ] === === End: [ 2020 /04/08 16 :41:56 ] === 0 new problem ( s ) found === End: [ 2020 /04/08 16 :41:56 ] === Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Distributed memory programming model (MPI)"},{"location":"development/performance-debugging-tools/itac/","text":"Intel Trace Analyzer and Collector (ITAC) are two tools used for analyzing MPI behavior in parallel applications. ITAC identifies MPI load imbalance and communication hotspots in order to help developers optimize MPI parallelization and minimize communication and synchronization in their applications. Using Trace Collector on Cori must be done with a command line interface, while Trace Analyzer supports both a command line and graphical user interface which analyzes the data from Trace Collector. Environmental models for ITAC in ULHPC \u00b6 module load purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/itac/2019.4.036 module load vis/GTK+/3.24.8-GCCcore-8.2.0 Interactive mode \u00b6 # Compilation $ icc -qopenmp -trance example.c # Code execution $ export OMP_NUM_THREADS = 16 $ -trace-collective ./a.out # Report collection $ export VT_STATISTICS = ON $ stftool tracefile.stf --print-statistics Batch mode \u00b6 Shared memory programming model (OpenMP) \u00b6 Example for the batch script: #!/bin/bash -l #SBATCH -J ITAC ###SBATCH -A <project_name> #SBATCH -N 1 #SBATCH -c 16 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/itac/2019.4.036 module load vis/GTK+/3.24.8-GCCcore-8.2.0 $ export OMP_NUM_THREADS = 16 $ -trace-collective ./a.out To see the result $ export VT_STATISTICS = ON $ stftool tracefile.stf --print-statistics Distributed memory programming model (MPI) \u00b6 To compile $ mpiicc -trace example.c Example for the batch script: #!/bin/bash -l #SBATCH -J ITAC ###SBATCH -A <project_name> #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/itac/2019.4.036 module load vis/GTK+/3.24.8-GCCcore-8.2.0 srun -n ${ SLURM_NTASKS } -trace-collective ./a.out To collect the result and see the result in GUI use the below commands $ export VT_STATISTICS = ON $ stftool tracefile.stf --print-statistics Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Intel Trace Analyzer and Collector"},{"location":"development/performance-debugging-tools/itac/#environmental-models-for-itac-in-ulhpc","text":"module load purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/itac/2019.4.036 module load vis/GTK+/3.24.8-GCCcore-8.2.0","title":"Environmental models for ITAC in ULHPC"},{"location":"development/performance-debugging-tools/itac/#interactive-mode","text":"# Compilation $ icc -qopenmp -trance example.c # Code execution $ export OMP_NUM_THREADS = 16 $ -trace-collective ./a.out # Report collection $ export VT_STATISTICS = ON $ stftool tracefile.stf --print-statistics","title":"Interactive mode"},{"location":"development/performance-debugging-tools/itac/#batch-mode","text":"","title":"Batch mode"},{"location":"development/performance-debugging-tools/itac/#shared-memory-programming-model-openmp","text":"Example for the batch script: #!/bin/bash -l #SBATCH -J ITAC ###SBATCH -A <project_name> #SBATCH -N 1 #SBATCH -c 16 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/itac/2019.4.036 module load vis/GTK+/3.24.8-GCCcore-8.2.0 $ export OMP_NUM_THREADS = 16 $ -trace-collective ./a.out To see the result $ export VT_STATISTICS = ON $ stftool tracefile.stf --print-statistics","title":"Shared memory programming model (OpenMP)"},{"location":"development/performance-debugging-tools/itac/#distributed-memory-programming-model-mpi","text":"To compile $ mpiicc -trace example.c Example for the batch script: #!/bin/bash -l #SBATCH -J ITAC ###SBATCH -A <project_name> #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/itac/2019.4.036 module load vis/GTK+/3.24.8-GCCcore-8.2.0 srun -n ${ SLURM_NTASKS } -trace-collective ./a.out To collect the result and see the result in GUI use the below commands $ export VT_STATISTICS = ON $ stftool tracefile.stf --print-statistics Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Distributed memory programming model (MPI)"},{"location":"development/performance-debugging-tools/scalasca/","text":"Scalasca is a performance analysis tool that supports large-scale systems, including IBM Blue Gene and CrayXT and small systems. The Scalasca provides information about the communication and synchronization among the processors. This information will help to do the performance analysis, optimization, and tunning of scientificcodes. Scalasca supports OpenMP, MPI, and hybrid programming model, and a analysis can be done by using the GUI which can be seen in below figure. Environmental models for Scalasca on ULHPC \u00b6 module load purge module load swenv/default-env/v1.1-20180716-production module load toolchain/foss/2018a module load perf/Scalasca/2.3.1-foss-2018a module load perf/Score-P/3.1-foss-2018a Interactive Mode \u00b6 Work flow: # instrument $ scorep mpicxx example.cc # analyze scalasca -analyze mpirun -n 28 ./a.out # examine $ scalasca -examine -s scorep_a_28_sum INFO: Post-processing runtime summarization report... INFO: Score report written to ./scorep_a_28_sum/scorep.score # graphical visualization $ scalasca -examine result_folder Batch mode \u00b6 Shared memory programming (OpenMP) \u00b6 #!/bin/bash -l #SBATCH -J Scalasca ###SBATCH -A <project_name> #SBATCH -N 1 #SBATCH -c 16 #SBATCH --time=00:10:00 #SBATCH -p batch module load purge module load swenv/default-env/v1.1-20180716-production module load toolchain/foss/2018a module load perf/Scalasca/2.3.1-foss-2018a module load perf/Score-P/3.1-foss-2018a export OMP_NUM_THREADS = 16 # analyze scalasca -analyze ./a.out Report collection and visualization # examine $ scalasca -examine -s scorep_a_28_sum INFO: Post-processing runtime summarization report... INFO: Score report written to ./scorep_a_28_sum/scorep.score # graphical visualization $ scalasca -examine result_folder Distributed memory programming (MPI) \u00b6 #!/bin/bash -l #SBATCH -J Scalasca ###SBATCH -A <project_name> #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --time=00:10:00 #SBATCH -p batch module load purge module load swenv/default-env/v1.1-20180716-production module load toolchain/foss/2018a module load perf/Scalasca/2.3.1-foss-2018a module load perf/Score-P/3.1-foss-2018a scalasca -analyze srun -n ${ SLURM_NTASKS } ./a.out Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Scalasca"},{"location":"development/performance-debugging-tools/scalasca/#environmental-models-for-scalasca-on-ulhpc","text":"module load purge module load swenv/default-env/v1.1-20180716-production module load toolchain/foss/2018a module load perf/Scalasca/2.3.1-foss-2018a module load perf/Score-P/3.1-foss-2018a","title":"Environmental models for Scalasca on ULHPC"},{"location":"development/performance-debugging-tools/scalasca/#interactive-mode","text":"Work flow: # instrument $ scorep mpicxx example.cc # analyze scalasca -analyze mpirun -n 28 ./a.out # examine $ scalasca -examine -s scorep_a_28_sum INFO: Post-processing runtime summarization report... INFO: Score report written to ./scorep_a_28_sum/scorep.score # graphical visualization $ scalasca -examine result_folder","title":"Interactive Mode"},{"location":"development/performance-debugging-tools/scalasca/#batch-mode","text":"","title":"Batch mode"},{"location":"development/performance-debugging-tools/scalasca/#shared-memory-programming-openmp","text":"#!/bin/bash -l #SBATCH -J Scalasca ###SBATCH -A <project_name> #SBATCH -N 1 #SBATCH -c 16 #SBATCH --time=00:10:00 #SBATCH -p batch module load purge module load swenv/default-env/v1.1-20180716-production module load toolchain/foss/2018a module load perf/Scalasca/2.3.1-foss-2018a module load perf/Score-P/3.1-foss-2018a export OMP_NUM_THREADS = 16 # analyze scalasca -analyze ./a.out Report collection and visualization # examine $ scalasca -examine -s scorep_a_28_sum INFO: Post-processing runtime summarization report... INFO: Score report written to ./scorep_a_28_sum/scorep.score # graphical visualization $ scalasca -examine result_folder","title":"Shared memory programming (OpenMP)"},{"location":"development/performance-debugging-tools/scalasca/#distributed-memory-programming-mpi","text":"#!/bin/bash -l #SBATCH -J Scalasca ###SBATCH -A <project_name> #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --time=00:10:00 #SBATCH -p batch module load purge module load swenv/default-env/v1.1-20180716-production module load toolchain/foss/2018a module load perf/Scalasca/2.3.1-foss-2018a module load perf/Score-P/3.1-foss-2018a scalasca -analyze srun -n ${ SLURM_NTASKS } ./a.out Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Distributed memory programming (MPI)"},{"location":"development/performance-debugging-tools/valgrind/","text":"The Valgrind tool suite provides a number of debugging and profiling tools that help you make your programs faster and more correct. The most popular of these tools is called Memcheck which can detect many memory-related errors and memory leaks. Prepare Your Program \u00b6 Compile your program with -g to include debugging information so that Memcheck's error messages include exact line numbers. Using -O0 is also a good idea, if you can tolerate the slowdown. With -O1 line numbers in error messages can be inaccurate, although generally speaking running Memcheck on code compiled at -O1 works fairly well, and the speed improvement compared to running -O0 is quite significant. Use of -O2 and above is not recommended as Memcheck occasionally reports uninitialised-value errors which don't really exist. Environmental models for Valgrind in ULHPC \u00b6 $ module purge $ module load debugger/Valgrind/3.15.0-intel-2019a Interactive mode \u00b6 Example code: #include <iostream> using namespace std ; int main () { const int SIZE = 1000 ; int * array = new int ( SIZE ); for ( int i = 0 ; i < SIZE ; i ++ ) array [ i ] = i + 1 ; // delete[] array return 0 ; } # Compilation $ icc -g example.cc # Code execution $ valgrind --leak-check = full --show-leak-kinds = all ./a.out Result output (with leak) If we do not delete delete[] array the memory, then there will be a memory leak. == 26756 == Memcheck, a memory error detector == 26756 == Copyright ( C ) 2002 -2017, and GNU GPL 'd, by Julian Seward et al. ==26756== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info ==26756== Command: ./a.out ==26756== ==26756== Invalid write of size 4 ==26756== at 0x401275: main (mem-leak.cc:10) ==26756== Address 0x5309c84 is 0 bytes after a block of size 4 alloc' d == 26756 == at 0x402DBE9: operator new ( unsigned long ) ( vg_replace_malloc.c:344 ) == 26756 == by 0x401265: main ( mem-leak.cc:8 ) == 26756 == == 26756 == == 26756 == HEAP SUMMARY: == 26756 == in use at exit: 4 bytes in 1 blocks == 26756 == total heap usage: 2 allocs, 1 frees, 72 ,708 bytes allocated == 26756 == == 26756 == 4 bytes in 1 blocks are definitely lost in loss record 1 of 1 == 26756 == at 0x402DBE9: operator new ( unsigned long ) ( vg_replace_malloc.c:344 ) == 26756 == by 0x401265: main ( mem-leak.cc:8 ) == 26756 == == 26756 == LEAK SUMMARY: == 26756 == definitely lost: 4 bytes in 1 blocks == 26756 == indirectly lost: 0 bytes in 0 blocks == 26756 == possibly lost: 0 bytes in 0 blocks == 26756 == still reachable: 0 bytes in 0 blocks == 26756 == suppressed: 0 bytes in 0 blocks == 26756 == == 26756 == For lists of detected and suppressed errors, rerun with: -s == 26756 == ERROR SUMMARY: 1000 errors from 2 contexts ( suppressed: 0 from 0 ) Result output (without leak) When we delete delete[] array the allocated memory, there will not be leaked memory. == 26172 == Memcheck, a memory error detector == 26172 == Copyright ( C ) 2002 -2017, and GNU GPL ' d, by Julian Seward et al. == 26172 == Using Valgrind-3.15.0 and LibVEX ; rerun with -h for copyright info == 26172 == Command: ./a.out == 26172 == == 26172 == == 26172 == HEAP SUMMARY: == 26172 == in use at exit: 4 bytes in 1 blocks == 26172 == total heap usage: 2 allocs, 1 frees, 72 ,708 bytes allocated == 26172 == == 26172 == 4 bytes in 1 blocks are definitely lost in loss record 1 of 1 == 26172 == at 0x402DBE9: operator new ( unsigned long ) ( vg_replace_malloc.c:344 ) == 26172 == by 0x401283: main ( in /mnt/irisgpfs/users/ekrishnasamy/BPG/Valgrind/a.out ) == 26172 == == 26172 == LEAK SUMMARY: == 26172 == definitely lost: 4 bytes in 1 blocks == 26172 == indirectly lost: 0 bytes in 0 blocks == 26172 == possibly lost: 0 bytes in 0 blocks == 26172 == still reachable: 0 bytes in 0 blocks == 26172 == suppressed: 0 bytes in 0 blocks == 26172 == == 26172 == For lists of detected and suppressed errors, rerun with: -s == 26172 == ERROR SUMMARY: 1 errors from 1 contexts ( suppressed: 0 from 0 ) Additional information \u00b6 This page is based on the \"Valgrind Quick Start Page\". For more information about valgrind, please refer to http://valgrind.org/ . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Valgrind"},{"location":"development/performance-debugging-tools/valgrind/#prepare-your-program","text":"Compile your program with -g to include debugging information so that Memcheck's error messages include exact line numbers. Using -O0 is also a good idea, if you can tolerate the slowdown. With -O1 line numbers in error messages can be inaccurate, although generally speaking running Memcheck on code compiled at -O1 works fairly well, and the speed improvement compared to running -O0 is quite significant. Use of -O2 and above is not recommended as Memcheck occasionally reports uninitialised-value errors which don't really exist.","title":"Prepare Your Program"},{"location":"development/performance-debugging-tools/valgrind/#environmental-models-for-valgrind-in-ulhpc","text":"$ module purge $ module load debugger/Valgrind/3.15.0-intel-2019a","title":"Environmental models for Valgrind in ULHPC"},{"location":"development/performance-debugging-tools/valgrind/#interactive-mode","text":"Example code: #include <iostream> using namespace std ; int main () { const int SIZE = 1000 ; int * array = new int ( SIZE ); for ( int i = 0 ; i < SIZE ; i ++ ) array [ i ] = i + 1 ; // delete[] array return 0 ; } # Compilation $ icc -g example.cc # Code execution $ valgrind --leak-check = full --show-leak-kinds = all ./a.out Result output (with leak) If we do not delete delete[] array the memory, then there will be a memory leak. == 26756 == Memcheck, a memory error detector == 26756 == Copyright ( C ) 2002 -2017, and GNU GPL 'd, by Julian Seward et al. ==26756== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info ==26756== Command: ./a.out ==26756== ==26756== Invalid write of size 4 ==26756== at 0x401275: main (mem-leak.cc:10) ==26756== Address 0x5309c84 is 0 bytes after a block of size 4 alloc' d == 26756 == at 0x402DBE9: operator new ( unsigned long ) ( vg_replace_malloc.c:344 ) == 26756 == by 0x401265: main ( mem-leak.cc:8 ) == 26756 == == 26756 == == 26756 == HEAP SUMMARY: == 26756 == in use at exit: 4 bytes in 1 blocks == 26756 == total heap usage: 2 allocs, 1 frees, 72 ,708 bytes allocated == 26756 == == 26756 == 4 bytes in 1 blocks are definitely lost in loss record 1 of 1 == 26756 == at 0x402DBE9: operator new ( unsigned long ) ( vg_replace_malloc.c:344 ) == 26756 == by 0x401265: main ( mem-leak.cc:8 ) == 26756 == == 26756 == LEAK SUMMARY: == 26756 == definitely lost: 4 bytes in 1 blocks == 26756 == indirectly lost: 0 bytes in 0 blocks == 26756 == possibly lost: 0 bytes in 0 blocks == 26756 == still reachable: 0 bytes in 0 blocks == 26756 == suppressed: 0 bytes in 0 blocks == 26756 == == 26756 == For lists of detected and suppressed errors, rerun with: -s == 26756 == ERROR SUMMARY: 1000 errors from 2 contexts ( suppressed: 0 from 0 ) Result output (without leak) When we delete delete[] array the allocated memory, there will not be leaked memory. == 26172 == Memcheck, a memory error detector == 26172 == Copyright ( C ) 2002 -2017, and GNU GPL ' d, by Julian Seward et al. == 26172 == Using Valgrind-3.15.0 and LibVEX ; rerun with -h for copyright info == 26172 == Command: ./a.out == 26172 == == 26172 == == 26172 == HEAP SUMMARY: == 26172 == in use at exit: 4 bytes in 1 blocks == 26172 == total heap usage: 2 allocs, 1 frees, 72 ,708 bytes allocated == 26172 == == 26172 == 4 bytes in 1 blocks are definitely lost in loss record 1 of 1 == 26172 == at 0x402DBE9: operator new ( unsigned long ) ( vg_replace_malloc.c:344 ) == 26172 == by 0x401283: main ( in /mnt/irisgpfs/users/ekrishnasamy/BPG/Valgrind/a.out ) == 26172 == == 26172 == LEAK SUMMARY: == 26172 == definitely lost: 4 bytes in 1 blocks == 26172 == indirectly lost: 0 bytes in 0 blocks == 26172 == possibly lost: 0 bytes in 0 blocks == 26172 == still reachable: 0 bytes in 0 blocks == 26172 == suppressed: 0 bytes in 0 blocks == 26172 == == 26172 == For lists of detected and suppressed errors, rerun with: -s == 26172 == ERROR SUMMARY: 1 errors from 1 contexts ( suppressed: 0 from 0 )","title":"Interactive mode"},{"location":"development/performance-debugging-tools/valgrind/#additional-information","text":"This page is based on the \"Valgrind Quick Start Page\". For more information about valgrind, please refer to http://valgrind.org/ . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"development/performance-debugging-tools/vtune/","text":"VTune \u00b6 Use Intel VTune Profiler to profile serial and multithreaded applications that are executed on a variety of hardware platforms (CPU, GPU, FPGA). The tool is delivered as a Performance Profiler with Intel Performance Snapshots and supports local and remote target analysis on the Windows , Linux , and Android* platforms. Without the right data, you\u2019re guessing about how to improve software performance and are unlikely to make the most effective improvements. Intel\u00ae VTune\u2122 Profiler collects key profiling data and presents it with a powerful interface that simplifies its analysis and interpretation. Environmental models for VTune on ULHPC: \u00b6 module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/VTune/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 Interactive Mode \u00b6 # Compilation $ icc -qopenmp example.c # Code execution $ export OMP_NUM_THREADS = 16 $ amplxe-cl -collect hotspots -r my_result ./a.out To see the result in GUI $ amplxe-gui my_result $ amplxe-cl will list out the analysis types and $ amplxe-cl -hlep report will list out available reports in VTune. Batch Mode \u00b6 Shared Memory Programming Model (OpenMP) \u00b6 #!/bin/bash -l #SBATCH -J VTune ###SBATCH -A <project_name> #SBATCH -N 1 #SBATCH -c 28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/VTune/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 export OMP_NUM_THREADS = 16 amplxe-cl -collect hotspots-r my_result ./a.out Distributed Memory Programming Model \u00b6 To compile just MPI application run $ mpiicc example.c and for MPI+OpenMP run $ mpiicc -qopenmp example.c #!/bin/bash -l #SBATCH -J VTune ###SBATCH -A <project_name> #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/VTune/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 srun -n ${ SLURM_NTASKS } amplxe-cl -collect uarch-exploration -r vtune_mpi -- ./a.out # Report collection $ amplxe-cl -report uarch-exploration -report-output output -r vtune_mpi # Result visualization $ amplxe-gui vtune_mpi The below figure shows the hybrid(MPI+OpenMP) programming analysis results: Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Intel VTune"},{"location":"development/performance-debugging-tools/vtune/#vtune","text":"Use Intel VTune Profiler to profile serial and multithreaded applications that are executed on a variety of hardware platforms (CPU, GPU, FPGA). The tool is delivered as a Performance Profiler with Intel Performance Snapshots and supports local and remote target analysis on the Windows , Linux , and Android* platforms. Without the right data, you\u2019re guessing about how to improve software performance and are unlikely to make the most effective improvements. Intel\u00ae VTune\u2122 Profiler collects key profiling data and presents it with a powerful interface that simplifies its analysis and interpretation.","title":"VTune"},{"location":"development/performance-debugging-tools/vtune/#environmental-models-for-vtune-on-ulhpc","text":"module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/VTune/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0","title":"Environmental models for VTune on ULHPC:"},{"location":"development/performance-debugging-tools/vtune/#interactive-mode","text":"# Compilation $ icc -qopenmp example.c # Code execution $ export OMP_NUM_THREADS = 16 $ amplxe-cl -collect hotspots -r my_result ./a.out To see the result in GUI $ amplxe-gui my_result $ amplxe-cl will list out the analysis types and $ amplxe-cl -hlep report will list out available reports in VTune.","title":"Interactive Mode"},{"location":"development/performance-debugging-tools/vtune/#batch-mode","text":"","title":"Batch Mode"},{"location":"development/performance-debugging-tools/vtune/#shared-memory-programming-model-openmp","text":"#!/bin/bash -l #SBATCH -J VTune ###SBATCH -A <project_name> #SBATCH -N 1 #SBATCH -c 28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/VTune/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 export OMP_NUM_THREADS = 16 amplxe-cl -collect hotspots-r my_result ./a.out","title":"Shared Memory Programming Model (OpenMP)"},{"location":"development/performance-debugging-tools/vtune/#distributed-memory-programming-model","text":"To compile just MPI application run $ mpiicc example.c and for MPI+OpenMP run $ mpiicc -qopenmp example.c #!/bin/bash -l #SBATCH -J VTune ###SBATCH -A <project_name> #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --time=00:10:00 #SBATCH -p batch module purge module load swenv/default-env/v1.2-20191021-production module load toolchain/intel/2019a module load tools/VTune/2019_update4 module load vis/GTK+/3.24.8-GCCcore-8.2.0 srun -n ${ SLURM_NTASKS } amplxe-cl -collect uarch-exploration -r vtune_mpi -- ./a.out # Report collection $ amplxe-cl -report uarch-exploration -report-output output -r vtune_mpi # Result visualization $ amplxe-gui vtune_mpi The below figure shows the hybrid(MPI+OpenMP) programming analysis results: Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Distributed Memory Programming Model"},{"location":"environment/","text":"ULHPC User Environment \u00b6 Your typical journey on the ULHPC facility is illustrated in the below figure. Typical workflow on UL HPC resources You daily interaction with the ULHPC facility includes the following actions: Preliminary setup Connect to the access/login servers This can be done either by ssh ( recommended ) or via the ULHPC OOD portal ( advanced users ) at this point, you probably want to create (or reattach) to a screen or tmux session Synchronize you code and/or transfer your input data using rsync/svn/git typically recall that the different storage filesystems are shared (via a high-speed interconnect network ) among the computational resources of the ULHPC facilities. In particular, it is sufficient to exchange data with the access servers to make them available on the clusters Reserve a few interactive resources with salloc -p interactive [...] recall that the module command (used to load the ULHPC User software ) is only available on the compute nodes ( eventually ) build your program, typically using gcc/icc/mpicc/nvcc.. Test your workflow / HPC analysis on a small size problem ( srun/python/sh... ) Prepare a launcher script <launcher>.{sh|py} Then you can proceed with your Real Experiments : Reserve passive resources : sbatch [...] <launcher> Grab the results and (eventually) transfer back your output results using rsync/svn/git For more information: Getting Started Connecting to ULHPC supercomputers ULHPC Storage Systems Overview '-bash: module : command not found' on access/login servers Recall that by default, the module command is ( on purpose ) NOT available on the access/login servers . You HAVE to be on a computing node (within a slurm job ) Home and Directories Layout \u00b6 All ULHPC systems use global home directories . You also have access to several other pre-defined directories setup over several different File Systems which co-exist on the ULHPC facility and are configured for different purposes. They are listed below: Directory Env. file system backup purging /home/users/<login> $HOME GPFS/Spectrumscale yes no /work/projects/ <name> - GPFS/Spectrumscale yes no /scratch/users/<login> $SCRATCH Lustre no yes /mnt/isilon/projects/<name> - OneFS yes* no Shell and Dotfiles \u00b6 The default login shell is bash -- see /etc/shells for supported shells. ULHPC dotfiles vs. default dotfiles The ULHPC team DOES NOT populate shell initialization files (also known as dotfiles) on users' home directories - the default system ones are used in your home -- you can check them in /etc/skel/.* on the access/login servers . However, you may want to install the ULHPC/dotfiles available as a Github repository . See installation notes . A working copy of that repository exists in /etc/dotfiles.d on the access/login servers . You can thus use it: $ /etc/dotfiles.d/install.sh -h # Example to install ULHPC GNU screen configuration file $ /etc/dotfiles.d/install.sh -d /etc/dotfiles.d/ --screen -n # Dry-run $ /etc/dotfiles.d/install.sh -d /etc/dotfiles.d/ --screen # real install Changing Default Login Shell (or NOT) If you want to change your your default login shell, you should set that up using the ULHPC IPA portal (change the Login Shell attribute). Note however that we STRONGLY discourage you to do so. You may hit unexpected issues with system profile scripts expecting bash as running shell. System Profile \u00b6 /etc/profile contains Linux system wide environment and startup programs. Specific scripts are set to improve your ULHPC experience, in particular those set in the ULHPC/tools repository, for instance: /etc/profile.d/slurm-prompt.sh : provide info of your running Slurm job on your prompt /etc/profile.d/slurm.sh : several helper function to Customizing Shell Environment \u00b6 You can create dotfiles (e.g., .bashrc , .bash_profile , or .profile , etc) in your $HOME directory to put your personal shell modifications. Custom Bash Initialisation Files On ULHPC system ~/.bash_profile and ~/.profile are sourced by login shells, while ~/.bashrc is sourced by most of the shell invocations including the login shells. In general you can put the environment variables, such as PATH , which are inheritable to subshells in ~/.bash_profile or ~/.profile and functions and aliases in the ~/.bashrc file in order to make them available in subshells. ULHPC/dotfiles bash configuration even source the following files for that specific purpose: ~/.bash_private : custom private functions ~/.bash_aliases : custom private aliases. Understanding Bash Startup Files order See reference documentation . That's somehow hard to understand. Some tried to explicit it under the form of a \"simple\" graph -- credits for the one below to Ian Miell ( another one ) This explains why normally all ULHPC launcher scripts start with the following sha-bang ( #! ) header #!/bin/bash -l # #SBATCH [...] [ ... ] That's indeed the only way ( i.e. using /bin/bash -l instead of the classical /bin/bash ) to ensure that /etc/profile is sourced natively, and thus that all ULHPC environments variables and modules are loaded. If you don't proceed that way (i.e. following the classical approach), you MUST then use the following template you may see from other HPC centers: #!/bin/bash # #SBATCH [...] [ ... ] # Load ULHPC Profile if [ -f /etc/profile ] ; then . /etc/profile fi Since all ULHPC systems share the Global HOME filesystem, the same $HOME is available regardless of the platform. To make system specific customizations use the pre-defined environment ULHPC_CLUSTER variable: Example of cluster specific settings case $ULHPC_CLUSTER in \"iris\" ) : # Settings for iris export MYVARIABLE = \"value-for-iris\" ;; \"aion\" ) : # settings for aion export MYVARIABLE = \"value-for-aion\" ;; * ) : # default value for export MYVARIABLE = \"default-value\" ;; esac Operating Systems \u00b6 The ULHPC facility runs RedHat-based Linux Distributions , in particular: the Iris cluster run CentOS and RedHat (RHEL) Linux operating system, version 7 the Aion cluster run RedHat (RHEL) Linux operating system, version 8 Experimental Grid5000 cluster run Debian Linux, version 10 Thus, you are more than encouraged to become familiar - if not yet - with Linux commands . We can recommend the following sites and resources: Software Carpentry: The Unix Shell Unix/Linux Command Reference Impact of CentOS project shifting focus starting 2021 from CentOS Linux to CentOS Stream You may have followed the official announcement on Dec 8, 2020 where Red Hat announced that it will discontinue CentOS 8 by the end of 2021 and instead will focus on CentOS Stream going forward. Fortunately CentOS 7 will continue to be updated until 2024 and is therefore not affected by this change. While CentOS traditionally has been a rebuild of RHEL, CentOS Stream will be more or less a testing ground for changes that will eventually go into RHEL. Unfortunately this means that CentOS Stream will likely become incompatible with RHEL (e.g. binaries compiled on CentOS Stream will not necessarily run on RHEL and vice versa). It is also questionable whether CentOS Stream is a suitable environment for running production systems. For all these reasons, the migration to CentOS 8 for Iris (initially planned for Q1 2021) has been cancelled . Alternative approaches are under investigation , including an homogeneous setup between Iris and Aion over Redhat 8. Discovering, visualizing and reserving UL HPC resources \u00b6 See ULHPC Tutorial / Getting Started ULHPC User Software Environment \u00b6 The UL HPC facility provides a large variety of scientific applications to its user community, either domain-specific codes and general purpose development tools which enable research and innovation excellence across a wide set of computational fields. -- see software list . We use the Environment Modules / LMod framework which provided the module utility on Compute nodes to manage nearly all software. There are two main advantages of the module approach: ULHPC can provide many different versions and/or installations of a single software package on a given machine, including a default version as well as several older and newer version. Users can easily switch to different versions or installations without having to explicitly specify different paths. With modules, the MANPATH and related environment variables are automatically managed. ULHPC modules are in practice automatically generated by Easybuild . EasyBuild (EB for short) is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. A large number of scientific software are supported ( at least 2175 supported software packages since the 4.3.2 release) - see also What is EasyBuild? . For several years now, Easybuild is used to manage the ULHPC User Software Set and generate automatically the module files available to you on our computational resources in either prod (default) or devel (early development/testing) environment -- see ULHPC Toolchains and Software Set Versioning . This enables users to easily extend the global Software Set with their own local software builds, either performed within their global home directory or ( better ) in a shared project directory though Easybuild , which generate automatically module files compliant with the ULHPC module setup . ULHPC Environment modules Using Easybuild on ULHPC Clusters","title":"Overview"},{"location":"environment/#ulhpc-user-environment","text":"Your typical journey on the ULHPC facility is illustrated in the below figure. Typical workflow on UL HPC resources You daily interaction with the ULHPC facility includes the following actions: Preliminary setup Connect to the access/login servers This can be done either by ssh ( recommended ) or via the ULHPC OOD portal ( advanced users ) at this point, you probably want to create (or reattach) to a screen or tmux session Synchronize you code and/or transfer your input data using rsync/svn/git typically recall that the different storage filesystems are shared (via a high-speed interconnect network ) among the computational resources of the ULHPC facilities. In particular, it is sufficient to exchange data with the access servers to make them available on the clusters Reserve a few interactive resources with salloc -p interactive [...] recall that the module command (used to load the ULHPC User software ) is only available on the compute nodes ( eventually ) build your program, typically using gcc/icc/mpicc/nvcc.. Test your workflow / HPC analysis on a small size problem ( srun/python/sh... ) Prepare a launcher script <launcher>.{sh|py} Then you can proceed with your Real Experiments : Reserve passive resources : sbatch [...] <launcher> Grab the results and (eventually) transfer back your output results using rsync/svn/git For more information: Getting Started Connecting to ULHPC supercomputers ULHPC Storage Systems Overview '-bash: module : command not found' on access/login servers Recall that by default, the module command is ( on purpose ) NOT available on the access/login servers . You HAVE to be on a computing node (within a slurm job )","title":"ULHPC User Environment"},{"location":"environment/#home-and-directories-layout","text":"All ULHPC systems use global home directories . You also have access to several other pre-defined directories setup over several different File Systems which co-exist on the ULHPC facility and are configured for different purposes. They are listed below: Directory Env. file system backup purging /home/users/<login> $HOME GPFS/Spectrumscale yes no /work/projects/ <name> - GPFS/Spectrumscale yes no /scratch/users/<login> $SCRATCH Lustre no yes /mnt/isilon/projects/<name> - OneFS yes* no","title":"Home and Directories Layout"},{"location":"environment/#shell-and-dotfiles","text":"The default login shell is bash -- see /etc/shells for supported shells. ULHPC dotfiles vs. default dotfiles The ULHPC team DOES NOT populate shell initialization files (also known as dotfiles) on users' home directories - the default system ones are used in your home -- you can check them in /etc/skel/.* on the access/login servers . However, you may want to install the ULHPC/dotfiles available as a Github repository . See installation notes . A working copy of that repository exists in /etc/dotfiles.d on the access/login servers . You can thus use it: $ /etc/dotfiles.d/install.sh -h # Example to install ULHPC GNU screen configuration file $ /etc/dotfiles.d/install.sh -d /etc/dotfiles.d/ --screen -n # Dry-run $ /etc/dotfiles.d/install.sh -d /etc/dotfiles.d/ --screen # real install Changing Default Login Shell (or NOT) If you want to change your your default login shell, you should set that up using the ULHPC IPA portal (change the Login Shell attribute). Note however that we STRONGLY discourage you to do so. You may hit unexpected issues with system profile scripts expecting bash as running shell.","title":"Shell and Dotfiles"},{"location":"environment/#system-profile","text":"/etc/profile contains Linux system wide environment and startup programs. Specific scripts are set to improve your ULHPC experience, in particular those set in the ULHPC/tools repository, for instance: /etc/profile.d/slurm-prompt.sh : provide info of your running Slurm job on your prompt /etc/profile.d/slurm.sh : several helper function to","title":"System Profile"},{"location":"environment/#customizing-shell-environment","text":"You can create dotfiles (e.g., .bashrc , .bash_profile , or .profile , etc) in your $HOME directory to put your personal shell modifications. Custom Bash Initialisation Files On ULHPC system ~/.bash_profile and ~/.profile are sourced by login shells, while ~/.bashrc is sourced by most of the shell invocations including the login shells. In general you can put the environment variables, such as PATH , which are inheritable to subshells in ~/.bash_profile or ~/.profile and functions and aliases in the ~/.bashrc file in order to make them available in subshells. ULHPC/dotfiles bash configuration even source the following files for that specific purpose: ~/.bash_private : custom private functions ~/.bash_aliases : custom private aliases. Understanding Bash Startup Files order See reference documentation . That's somehow hard to understand. Some tried to explicit it under the form of a \"simple\" graph -- credits for the one below to Ian Miell ( another one ) This explains why normally all ULHPC launcher scripts start with the following sha-bang ( #! ) header #!/bin/bash -l # #SBATCH [...] [ ... ] That's indeed the only way ( i.e. using /bin/bash -l instead of the classical /bin/bash ) to ensure that /etc/profile is sourced natively, and thus that all ULHPC environments variables and modules are loaded. If you don't proceed that way (i.e. following the classical approach), you MUST then use the following template you may see from other HPC centers: #!/bin/bash # #SBATCH [...] [ ... ] # Load ULHPC Profile if [ -f /etc/profile ] ; then . /etc/profile fi Since all ULHPC systems share the Global HOME filesystem, the same $HOME is available regardless of the platform. To make system specific customizations use the pre-defined environment ULHPC_CLUSTER variable: Example of cluster specific settings case $ULHPC_CLUSTER in \"iris\" ) : # Settings for iris export MYVARIABLE = \"value-for-iris\" ;; \"aion\" ) : # settings for aion export MYVARIABLE = \"value-for-aion\" ;; * ) : # default value for export MYVARIABLE = \"default-value\" ;; esac","title":"Customizing Shell Environment"},{"location":"environment/#operating-systems","text":"The ULHPC facility runs RedHat-based Linux Distributions , in particular: the Iris cluster run CentOS and RedHat (RHEL) Linux operating system, version 7 the Aion cluster run RedHat (RHEL) Linux operating system, version 8 Experimental Grid5000 cluster run Debian Linux, version 10 Thus, you are more than encouraged to become familiar - if not yet - with Linux commands . We can recommend the following sites and resources: Software Carpentry: The Unix Shell Unix/Linux Command Reference Impact of CentOS project shifting focus starting 2021 from CentOS Linux to CentOS Stream You may have followed the official announcement on Dec 8, 2020 where Red Hat announced that it will discontinue CentOS 8 by the end of 2021 and instead will focus on CentOS Stream going forward. Fortunately CentOS 7 will continue to be updated until 2024 and is therefore not affected by this change. While CentOS traditionally has been a rebuild of RHEL, CentOS Stream will be more or less a testing ground for changes that will eventually go into RHEL. Unfortunately this means that CentOS Stream will likely become incompatible with RHEL (e.g. binaries compiled on CentOS Stream will not necessarily run on RHEL and vice versa). It is also questionable whether CentOS Stream is a suitable environment for running production systems. For all these reasons, the migration to CentOS 8 for Iris (initially planned for Q1 2021) has been cancelled . Alternative approaches are under investigation , including an homogeneous setup between Iris and Aion over Redhat 8.","title":"Operating Systems "},{"location":"environment/#discovering-visualizing-and-reserving-ul-hpc-resources","text":"See ULHPC Tutorial / Getting Started","title":"Discovering, visualizing and reserving UL HPC resources"},{"location":"environment/#ulhpc-user-software-environment","text":"The UL HPC facility provides a large variety of scientific applications to its user community, either domain-specific codes and general purpose development tools which enable research and innovation excellence across a wide set of computational fields. -- see software list . We use the Environment Modules / LMod framework which provided the module utility on Compute nodes to manage nearly all software. There are two main advantages of the module approach: ULHPC can provide many different versions and/or installations of a single software package on a given machine, including a default version as well as several older and newer version. Users can easily switch to different versions or installations without having to explicitly specify different paths. With modules, the MANPATH and related environment variables are automatically managed. ULHPC modules are in practice automatically generated by Easybuild . EasyBuild (EB for short) is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. A large number of scientific software are supported ( at least 2175 supported software packages since the 4.3.2 release) - see also What is EasyBuild? . For several years now, Easybuild is used to manage the ULHPC User Software Set and generate automatically the module files available to you on our computational resources in either prod (default) or devel (early development/testing) environment -- see ULHPC Toolchains and Software Set Versioning . This enables users to easily extend the global Software Set with their own local software builds, either performed within their global home directory or ( better ) in a shared project directory though Easybuild , which generate automatically module files compliant with the ULHPC module setup . ULHPC Environment modules Using Easybuild on ULHPC Clusters","title":"ULHPC User Software Environment"},{"location":"environment/easybuild/","text":"Easybuild \u00b6 EasyBuild (EB for short) is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. A large number of scientific software are supported ( at least 2175 supported software packages since the 4.3.2 release) - see also What is EasyBuild? . For several years now, Easybuild is used to manage the ULHPC User Software Set and generate automatically the module files available to you on our computational resources in either prod (default) or devel (early development/testing) environment -- see ULHPC Toolchains and Software Set Versioning . This enables users to easily extend the global Software Set with their own local software builds, either performed within their global home directory or ( better ) in a shared project directory though Easybuild , which generate automatically module files compliant with the ULHPC module setup . Why using an automatic building tool on HPC environment like Easybuild or Spack ? Well that may seem obvious to some of you, but scientific software is often difficult to build. Not all rely on standard building tools like Autotools/Automake (and the famous configure; make; make install ) or CMake. And even in that case, parsing the available option to ensure matching the hardware configuration of the computing resources used for the execution is time consuming and error-prone. Most of the time unfortunately, scientific software embed hardcoded parameters and/or poor/outdated documentation with incomplete build procedures. In this context, software build and installation frameworks like Easybuild or Spack helps to facilitate the building task in a consistent and automatic way, while generating also the LMod modulefiles. We select [Easybuild](( https://docs.easybuild.io ) as primary building tool to ensure the best optimized builds. Some HPC sites use both -- see this talk from William Lucas at EPCC for instance. It does not prevent from maintaining your own build instructions notes . Easybuild Concepts and terminology \u00b6 Official Easybuild Tutorial EasyBuild relies on two main concepts: Toolchains and EasyConfig files . A toolchain corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the foss (\" Free and Open Source Software \") and the intel one. foss , based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.). intel , based on the Intel compiler suit ([])and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.). An EasyConfig file is a simple text file that describes the build process of a software. For most software that uses standard procedures (like configure , make and make install ), this file is very simple. Many EasyConfig files are already provided with EasyBuild. ULHPC Easybuild Configuration \u00b6 To build software with Easybuild compliant with the configuration in place on the ULHPC facility, you need to be aware of the following setup: Modules tool ( $EASYBUILD_MODULES_TOOL ): Lmod (see docs ) Module Naming Scheme ( EASYBUILD_MODULE_NAMING_SCHEME ): we use a special hierarchical organization where the software are classified/ categorized under a pre-defined class. These variables are defined at the global profile level, under /etc/profile.d/ulhpc_resif.sh on the compute nodes as follows: export EASYBUILD_MODULES_TOOL = Lmod export EASYBUILD_MODULE_NAMING_SCHEME = CategorizedModuleNamingScheme All builds and installations are performed at user level, so you don't need the admin (i.e. root ) rights. Another very important configuration variable is the Overall Easybuild prefix path $EASYBUILD_PREFIX which affects the default value of several configuration options: built software are placed under ${EASYBUILD_PREFIX}/software/ modules install path: ${EASYBUILD_PREFIX}/modules/all (determined via Overall prefix path (--prefix), --subdir-modules and --suffix-modules-path) You can thus extend the ULHPC Software set with your own local builds by setting appropriately the variable $EASYBUILD_PREFIX : For installation in your home directory: export EASYBUILD_PREFIX=$HOME/.local/easybuild For installation in a shared project directory <name> : export EASYBUILD_PREFIX=$PROJECTHOME/<name>/easybuild Adapting you custom build to cluster, the toolchain version and the architecture Just like the ULHPC software set ( installed in EASYBUILD_PREFIX=/opt/apps/resif/<cluster>/<version>/<arch> ), you may want to isolate your local builds to take into account the cluster $ULHPC_CLUSTER (\"iris\" or \"aion\"), the toolchain version <version> (Ex: 2019b, 2020b etc.) you build upon and eventually the architecture <arch> . In that case, you can use the following helper scripts: resif-load-home-swset-prod which is roughly equivalent to the following code: # EASYBUILD_PREFIX: [basedir]/<cluster>/<environment>/<arch> # Ex: Default EASYBUILD_PREFIX in your home - Adapt to project directory if needed _EB_PREFIX = $HOME /.local/easybuild # ... eventually complemented with cluster [ -n \" ${ ULHPC_CLUSTER } \" ] && _EB_PREFIX = \" ${ _EB_PREFIX } / ${ ULHPC_CLUSTER } \" # ... eventually complemented with software set version _EB_PREFIX = \" ${ _EB_PREFIX } / ${ RESIF_VERSION_PROD } \" # ... eventually complemented with arch [ -n \" ${ RESIF_ARCH } \" ] && _EB_PREFIX = \" ${ _EB_PREFIX } / ${ RESIF_ARCH } \" export EASYBUILD_PREFIX = \" ${ _EB_PREFIX } \" export LOCAL_MODULES = ${ EASYBUILD_PREFIX } /modules/all For a shared project directory <name> located under $PROJECTHOME/<name> , you can use the following following helper scripts: resif-load-project-swset-prod $PROJECTHOME /<name> ACM PEARC'21: RESIF 3.0 For more details on the way we setup and deploy the User Software Environment on ULHPC systems through the RESIF 3 framework, see the ACM PEARC'21 conference paper presented on July 22, 2021. ACM Reference Format | ORBilu entry | OpenAccess | ULHPC blog post | slides | Github : Sebastien Varrette, Emmanuel Kieffer, Frederic Pinel, Ezhilmathi Krishnasamy, Sarah Peter, Hyacinthe Cartiaux, and Xavier Besseron. 2021. RESIF 3.0: Toward a Flexible & Automated Management of User Software Environment on HPC facility. In Practice and Experience in Advanced Research Computing (PEARC '21) . Association for Computing Machinery (ACM), New York, NY, USA, Article 33, 1\u20134. https://doi.org/10.1145/3437359.3465600 Installation / Update local Easybuild \u00b6 You can of course use the default Easubuild that comes with the ULHPC software setwith module load tools/EasyBuild . But as soon as you want to install your local builds, you have interest to install the up-to-date release of EasyBuild in your local $EASYBUILD_PREFIX . You can later update any time EasyBuild in $EASYBUILD_PREFIX via the same bootstrapping procedure : ### /!\\ IMPORTANT: You need to be on a computing node to access the module ### command and permit the installation # download script curl -o /tmp/bootstrap_eb.py https://raw.githubusercontent.com/easybuilders/easybuild-framework/develop/easybuild/scripts/bootstrap_eb.py # double check the installation prefix echo $EASYBUILD_PREFIX # install Easybuild python /tmp/bootstrap_eb.py $EASYBUILD_PREFIX Repeat when you need to update your local installation.","title":"Easybuild"},{"location":"environment/easybuild/#easybuild","text":"EasyBuild (EB for short) is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. A large number of scientific software are supported ( at least 2175 supported software packages since the 4.3.2 release) - see also What is EasyBuild? . For several years now, Easybuild is used to manage the ULHPC User Software Set and generate automatically the module files available to you on our computational resources in either prod (default) or devel (early development/testing) environment -- see ULHPC Toolchains and Software Set Versioning . This enables users to easily extend the global Software Set with their own local software builds, either performed within their global home directory or ( better ) in a shared project directory though Easybuild , which generate automatically module files compliant with the ULHPC module setup . Why using an automatic building tool on HPC environment like Easybuild or Spack ? Well that may seem obvious to some of you, but scientific software is often difficult to build. Not all rely on standard building tools like Autotools/Automake (and the famous configure; make; make install ) or CMake. And even in that case, parsing the available option to ensure matching the hardware configuration of the computing resources used for the execution is time consuming and error-prone. Most of the time unfortunately, scientific software embed hardcoded parameters and/or poor/outdated documentation with incomplete build procedures. In this context, software build and installation frameworks like Easybuild or Spack helps to facilitate the building task in a consistent and automatic way, while generating also the LMod modulefiles. We select [Easybuild](( https://docs.easybuild.io ) as primary building tool to ensure the best optimized builds. Some HPC sites use both -- see this talk from William Lucas at EPCC for instance. It does not prevent from maintaining your own build instructions notes .","title":"Easybuild"},{"location":"environment/easybuild/#easybuild-concepts-and-terminology","text":"Official Easybuild Tutorial EasyBuild relies on two main concepts: Toolchains and EasyConfig files . A toolchain corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the foss (\" Free and Open Source Software \") and the intel one. foss , based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.). intel , based on the Intel compiler suit ([])and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.). An EasyConfig file is a simple text file that describes the build process of a software. For most software that uses standard procedures (like configure , make and make install ), this file is very simple. Many EasyConfig files are already provided with EasyBuild.","title":"Easybuild Concepts and terminology"},{"location":"environment/easybuild/#ulhpc-easybuild-configuration","text":"To build software with Easybuild compliant with the configuration in place on the ULHPC facility, you need to be aware of the following setup: Modules tool ( $EASYBUILD_MODULES_TOOL ): Lmod (see docs ) Module Naming Scheme ( EASYBUILD_MODULE_NAMING_SCHEME ): we use a special hierarchical organization where the software are classified/ categorized under a pre-defined class. These variables are defined at the global profile level, under /etc/profile.d/ulhpc_resif.sh on the compute nodes as follows: export EASYBUILD_MODULES_TOOL = Lmod export EASYBUILD_MODULE_NAMING_SCHEME = CategorizedModuleNamingScheme All builds and installations are performed at user level, so you don't need the admin (i.e. root ) rights. Another very important configuration variable is the Overall Easybuild prefix path $EASYBUILD_PREFIX which affects the default value of several configuration options: built software are placed under ${EASYBUILD_PREFIX}/software/ modules install path: ${EASYBUILD_PREFIX}/modules/all (determined via Overall prefix path (--prefix), --subdir-modules and --suffix-modules-path) You can thus extend the ULHPC Software set with your own local builds by setting appropriately the variable $EASYBUILD_PREFIX : For installation in your home directory: export EASYBUILD_PREFIX=$HOME/.local/easybuild For installation in a shared project directory <name> : export EASYBUILD_PREFIX=$PROJECTHOME/<name>/easybuild Adapting you custom build to cluster, the toolchain version and the architecture Just like the ULHPC software set ( installed in EASYBUILD_PREFIX=/opt/apps/resif/<cluster>/<version>/<arch> ), you may want to isolate your local builds to take into account the cluster $ULHPC_CLUSTER (\"iris\" or \"aion\"), the toolchain version <version> (Ex: 2019b, 2020b etc.) you build upon and eventually the architecture <arch> . In that case, you can use the following helper scripts: resif-load-home-swset-prod which is roughly equivalent to the following code: # EASYBUILD_PREFIX: [basedir]/<cluster>/<environment>/<arch> # Ex: Default EASYBUILD_PREFIX in your home - Adapt to project directory if needed _EB_PREFIX = $HOME /.local/easybuild # ... eventually complemented with cluster [ -n \" ${ ULHPC_CLUSTER } \" ] && _EB_PREFIX = \" ${ _EB_PREFIX } / ${ ULHPC_CLUSTER } \" # ... eventually complemented with software set version _EB_PREFIX = \" ${ _EB_PREFIX } / ${ RESIF_VERSION_PROD } \" # ... eventually complemented with arch [ -n \" ${ RESIF_ARCH } \" ] && _EB_PREFIX = \" ${ _EB_PREFIX } / ${ RESIF_ARCH } \" export EASYBUILD_PREFIX = \" ${ _EB_PREFIX } \" export LOCAL_MODULES = ${ EASYBUILD_PREFIX } /modules/all For a shared project directory <name> located under $PROJECTHOME/<name> , you can use the following following helper scripts: resif-load-project-swset-prod $PROJECTHOME /<name> ACM PEARC'21: RESIF 3.0 For more details on the way we setup and deploy the User Software Environment on ULHPC systems through the RESIF 3 framework, see the ACM PEARC'21 conference paper presented on July 22, 2021. ACM Reference Format | ORBilu entry | OpenAccess | ULHPC blog post | slides | Github : Sebastien Varrette, Emmanuel Kieffer, Frederic Pinel, Ezhilmathi Krishnasamy, Sarah Peter, Hyacinthe Cartiaux, and Xavier Besseron. 2021. RESIF 3.0: Toward a Flexible & Automated Management of User Software Environment on HPC facility. In Practice and Experience in Advanced Research Computing (PEARC '21) . Association for Computing Machinery (ACM), New York, NY, USA, Article 33, 1\u20134. https://doi.org/10.1145/3437359.3465600","title":"ULHPC Easybuild Configuration"},{"location":"environment/easybuild/#installation-update-local-easybuild","text":"You can of course use the default Easubuild that comes with the ULHPC software setwith module load tools/EasyBuild . But as soon as you want to install your local builds, you have interest to install the up-to-date release of EasyBuild in your local $EASYBUILD_PREFIX . You can later update any time EasyBuild in $EASYBUILD_PREFIX via the same bootstrapping procedure : ### /!\\ IMPORTANT: You need to be on a computing node to access the module ### command and permit the installation # download script curl -o /tmp/bootstrap_eb.py https://raw.githubusercontent.com/easybuilders/easybuild-framework/develop/easybuild/scripts/bootstrap_eb.py # double check the installation prefix echo $EASYBUILD_PREFIX # install Easybuild python /tmp/bootstrap_eb.py $EASYBUILD_PREFIX Repeat when you need to update your local installation.","title":"Installation / Update local Easybuild"},{"location":"environment/modules/","text":"ULHPC Software/Modules Environment \u00b6 The UL HPC facility provides a large variety of scientific applications to its user community, either domain-specific codes and general purpose development tools which enable research and innovation excellence across a wide set of computational fields. -- see software list . We use the Environment Modules / LMod framework which provided the module utility on Compute nodes to manage nearly all software. There are two main advantages of the module approach: ULHPC can provide many different versions and/or installations of a single software package on a given machine, including a default version as well as several older and newer version. Users can easily switch to different versions or installations without having to explicitly specify different paths. With modules, the MANPATH and related environment variables are automatically managed. ULHPC modules are in practice automatically generated by Easybuild . EasyBuild (EB for short) is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. A large number of scientific software are supported ( at least 2175 supported software packages since the 4.3.2 release) - see also What is EasyBuild? . For several years now, Easybuild is used to manage the ULHPC User Software Set and generate automatically the module files available to you on our computational resources in either prod (default) or devel (early development/testing) environment -- see ULHPC Toolchains and Software Set Versioning . This enables users to easily extend the global Software Set with their own local software builds, either performed within their global home directory or ( better ) in a shared project directory though Easybuild , which generate automatically module files compliant with the ULHPC module setup . Environment modules and LMod \u00b6 Environment Modules are a standard and well-established technology across HPC sites, to permit developing and using complex software and libraries build with dependencies, allowing multiple versions of software stacks and combinations thereof to co-exist. It brings the module command which is used to manage environment variables such as PATH , LD_LIBRARY_PATH and MANPATH , enabling the easy loading and unloading of application/library profiles and their dependencies. Why do you need [Environment] Modules? When users login to a Linux system, they get a login shell and the shell uses Environment variables to run commands and applications. Most common are: PATH : colon-separated list of directories in which your system looks for executable files; MANPATH : colon-separated list of directories in which man searches for the man pages; LD_LIBRARY_PATH : colon-separated list of directories in which your system looks for for ELF / *.so libraries at execution time needed by applications. There are also application specific environment variables such as CPATH , LIBRARY_PATH , JAVA_HOME , LM_LICENSE_FILE , MKLROOT etc. A traditional way to setup these Environment variables is by customizing the shell initialization files : i.e. /etc/profile , .bash_profile , and .bashrc This proves to be very impractical on multi-user systems with various applications and multiple application versions installed as on an HPC facility. To overcome the difficulty of setting and changing the Environment variables, the TCL/C Environment Modules were introduced over 2 decades ago. The Environment Modules package is a tool that simplify shell initialization and lets users easily modify their environment during the session with modulefiles . Each modulefile contains the information needed to configure the shell for an application. Once the Modules package is initialized, the environment can be modified on a per-module basis using the module command which interprets modulefiles. Typically modulefiles instruct the module command to alter or set shell environment variables such as PATH , MANPATH , etc. Modulefiles may be shared by many users on a system (as done on the ULHPC clusters) and users may have their own collection to supplement or replace the shared modulefiles. Modules can be loaded and unloaded dynamically and atomically, in an clean fashion. All popular shells are supported, including bash , ksh , zsh , sh , csh , tcsh , fish , as well as some scripting languages such as perl , ruby , tcl , python , cmake and R . Modules are useful in managing different versions of applications. Modules can also be bundled into metamodules that will load an entire suite of different applications -- this is precisely the way we manage the ULHPC Software Set Tcl/C Environment Modules (Tmod) vs. Tcl Environment Modules vs. Lmod There exists several implementation of the module tool: Tcl/C Environment Modules (3.2.10 \\leq \\leq version < 4), also called Tmod : the seminal ( old ) implementation Tcl-only variant of Environment modules (version \\geq \\geq 4), previously called Modules-Tcl ( recommended ) Lmod , a Lua based Environment Module System Lmod (\"L\" stands for Lua ) provides all of the functionality of TCL/C Environment Modules plus more features: support for hierarchical module file structure MODULEPATH is dynamically updated when modules are loaded. makes loaded modules inactive and active to provide sane environment. supports for hidden modules support for optional usage tracking (implemented on ULHPC facilities) In particular, Lmod enforces the following safety features that are not always guaranted with the other tools: The One Name Rule : Users can only have one version active Users can only load one compiler or MPI stack at a time (through the family(...) directive) The ULHPC Facility relies on Lmod -- the associated Modulefiles being automatically generated by Easybuild . The ULHPC Facility relies on Lmod , a Lua-based Environment module system that easily handles the MODULEPATH Hierarchical problem. In this context, the module command supports the following subcommands: Command Description module avail Lists all the modules which are available to be loaded module spider <pattern> Search for among available modules (Lmod only) module load <mod1> [mod2...] Load a module module unload <module> Unload a module module list List loaded modules module purge Unload all modules (purge) module display <module> Display what a module does module use <path> Prepend the directory to the MODULEPATH environment variable module unuse <path> Remove the directory from the MODULEPATH environment variable What is module ? module is a shell function that modifies user shell upon load of a modulefile. It is defined as follows $ type module module is a function module () { eval $($LMOD_CMD bash \"$@\") && eval $(${LMOD_SETTARG_CMD:-:} -s sh) } In particular, module is NOT a program At the heart of environment modules interaction resides the following components: the MODULEPATH environment variable, which defines a colon-separated list of directories to search for modulefiles modulefile (see an example ) associated to each available software. Example of ULHPC toolchain/foss (auto-generated) Modulefile $ module show toolchain/foss ------------------------------------------------------------------------------- /opt/apps/resif/iris/2019b/broadwell/modules/all/toolchain/foss/2019b.lua: ------------------------------------------------------------------------------- help([[ Description =========== GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. More information ================ - Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain ]]) whatis(\"Description: GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.\") whatis(\"Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain\") whatis(\"URL: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain\") conflict(\"toolchain/foss\") load(\"compiler/GCC/8.3.0\") load(\"mpi/OpenMPI/3.1.4-GCC-8.3.0\") load(\"numlib/OpenBLAS/0.3.7-GCC-8.3.0\") load(\"numlib/FFTW/3.3.8-gompi-2019b\") load(\"numlib/ScaLAPACK/2.0.2-gompi-2019b\") setenv(\"EBROOTFOSS\",\"/opt/apps/resif/iris/2019b/broadwell/software/foss/2019b\") setenv(\"EBVERSIONFOSS\",\"2019b\") setenv(\"EBDEVELFOSS\",\"/opt/apps/resif/iris/2019b/broadwell/software/foss/2019b/easybuild/toolchain-foss-2019b-easybuild-devel\") ( reminder ): the module command is ONLY available on the compute nodes, NOT on the access front-ends. In particular, you need to be within a job to load ULHPC or private modules. ULHPC $MODULEPATH \u00b6 By default, the MODULEPATH environment variable holds a single searched directory holding the optimized builds prepared for you by the ULHPC Team. The general format of this directory is as follows: /opt/apps/resif/<cluster>/<version>/<arch>/modules/all where: <cluster> depicts the name of the cluster ( iris or aion ). Stored as $ULHPC_CLUSTER . <version> corresponds to the ULHPC Software set release (aligned with Easybuid toolchains release ), i.e. 2019b , 2020a etc. Stored as $RESIF_VERSION_{PROD,DEVEL,LEGACY} depending on the Production / development / legacy ULHPC software set version <arch> is a lower-case strings that categorize the CPU architecture of the build host, and permits to easyli identify optimized target architecture. It is stored as $RESIF_ARCH . On Intel nodes: broadwell ( default ), skylake On AMD nodes: epyc On GPU nodes: gpu Cluster Arch. $RESIF_ARCH $MODULEPATH Environment variable Iris broadwell (default) /opt/apps/resif/iris/<version>/broadwell/modules/all Iris skylake /opt/apps/resif/iris/<version>/skylake/modules/all Iris gpu /opt/apps/resif/iris/<version>/gpu/modules/all Aion epyc (default) /opt/apps/resif/aion/<version>/{epyc}/modules/all On skylake nodes, you may want to use the optimized modules for skylake On GPU nodes, you may want to use the CPU-optimized builds for skylake (in addition to the gpu -enabled softwares) ACM PEARC'21: RESIF 3.0 If you are interested to know more on the wey we setup and deploy the User Software Environment on ULHPC systems through the RESIF 3 framework, you can refer to the below article presented during the ACM PEARC'21 conference, on July 22, 2021. ACM Reference Format | ORBilu entry | OpenAccess | ULHPC blog post | slides | Github : Sebastien Varrette, Emmanuel Kieffer, Frederic Pinel, Ezhilmathi Krishnasamy, Sarah Peter, Hyacinthe Cartiaux, and Xavier Besseron. 2021. RESIF 3.0: Toward a Flexible & Automated Management of User Software Environment on HPC facility. In Practice and Experience in Advanced Research Computing (PEARC '21) . Association for Computing Machinery (ACM), New York, NY, USA, Article 33, 1\u20134. https://doi.org/10.1145/3437359.3465600 Module Naming Schemes \u00b6 What is a Module Naming Scheme? The full software and module install paths for a particular software package are determined by the active module naming scheme along with the general software and modules install paths specified by the EasyBuild configuration. You can list the supported module naming schemes of Easybuild using: $ eb --avail-module-naming-schemes List of supported module naming schemes: EasyBuildMNS CategorizedHMNS MigrateFromEBToHMNS HierarchicalMNS CategorizedModuleNamingScheme See Flat vs. Hierarchical module naming scheme for an illustrated explaination of the difference between two extreme cases: flat or 3-level hierarchical. On ULHPC systems, we selected an intermediate scheme called CategorizedModuleNamingScheme . Module Naming Schemes on ULHPC system ULHPC modules are organised through the Categorized Naming Scheme Format: <category>/<name>/<version>-<toolchain><versionsuffix> This means that the typical module hierarchy has as prefix a category level, taken out from one of the supported software category or module class : $ eb --show-default-moduleclasses Default available module classes: base: Default module class astro: Astronomy, Astrophysics and Cosmology bio: Bioinformatics, biology and biomedical cae: Computer Aided Engineering (incl. CFD) chem: Chemistry, Computational Chemistry and Quantum Chemistry compiler: Compilers data: Data management & processing tools debugger: Debuggers devel: Development tools geo: Earth Sciences ide: Integrated Development Environments (e.g. editors) lang: Languages and programming aids lib: General purpose libraries math: High-level mathematical software mpi: MPI stacks numlib: Numerical Libraries perf: Performance tools quantum: Quantum Computing phys: Physics and physical systems simulations system: System utilities (e.g. highly depending on system OS and hardware) toolchain: EasyBuild toolchains tools: General purpose tools vis: Visualization, plotting, documentation and typesetting It follows that the ULHPC software modules are structured according to the organization depicted below ( click to enlarge ). ULHPC Toolchains and Software Set Versioning \u00b6 We offer a YEARLY release of the ULHPC Software Set based on Easybuid release of toolchains -- see Component versions ( fixed per release ) in the foss and intel toolchains. However , count at least 6 months of validation/import after EB release before ULHPC release An overview of the currently available component versions is depicted below: Name Type 2019b ( legacy ) 2020a 2020b ( prod ) 2021a 2021b ( devel ) GCCCore compiler 8.3.0 9.3.0 10.2.0 10.3.0 11.2.0 foss toolchain 2019b 2020a 2020b 2021a 2021b intel toolchain 2019b 2020a 2020b 2021a 2021b binutils 2.32 2.34 2.35 2.36 2.37 Python 3.7.4 (and 2.7.16) 3.8.2 (and 2.7.18) 3.8.6 3.9.2 3.9.6 LLVM compiler 9.0.1 10.0.1 11.0.0 11.1.0 12.0.1 OpenMPI MPI 3.1.4 4.0.3 4.0.5 4.1.1 4.1.2 Once on a node, the current version of the ULHPC Software Set in production is stored in $RESIF_VERSION_PROD . You can use the variables $MODULEPATH_{LEGACY,PROD,DEVEL} to access or set the MODULEPATH command with the appropriate value. Yet we have define utility scripts to facilitate your quick reset of the module environment, i.e., resif-load-swset-{legacy,prod,devel} and resif-reset-swset For instance, if you want to use the legacy software set, proceed as follows in your launcher scripts: resif-load-swset-legacy # Eq. of export MODULEPATH=$MODULEPATH_LEGACY # [...] # Restore production settings resif-load-swset-prod # Eq. of export MODULEPATH=$MODULEPATH_PROD If on the contrary you want to test the (new) development software set, i.e., the devel version, stored in $RESIF_VERSION_DEVEL : resif-load-swset-devel # Eq. of export MODULEPATH=$MODULEPATH_DEVEL # [...] # Restore production settings resif-reset-swset # As resif-load-swset-prod (iris only) Skylake Optimized builds Skylake optimized build can be loaded on regular nodes using resif-load-swset-skylake # Eq. of export MODULEPATH=$MODULEPATH_PROD_SKYLAKE You MUST obviously be on a Skylake node ( sbatch -C skylake [...] ) to take benefit from it. Note that this action is not required on GPU nodes. GPU Optimized builds vs. CPU software set on GPU nodes On GPU nodes, be aware that the default MODULEPATH holds two directories: GPU Optimized builds ( i.e. typically against the {foss,intel}cuda toolchains) stored under /opt/apps/resif/<cluster>/<version>/gpu/modules/all CPU Optimized builds (ex: skylake on Iris )) stored under /opt/apps/resif/<cluster>/<version>/skylake/modules/all You may want to exclude CPU builds to ensure you take the most out of the GPU accelerators. In that case, you may want to run: # /!\\ ADAPT <version> accordingly module unuse /opt/apps/resif/ ${ ULHPC_CLUSTER } / ${ RESIF_VERSION_PROD } /skylake/modules/all Using Easybuild to Create Custom Modules \u00b6 Just like we do, you probably want to use Easybuild to complete the existing software set with your own modules and software builds. See Building Custom (or missing) software documentation for more details. Creating a Custom Module Environment \u00b6 You can modify your environment so that certain modules are loaded whenever you log in. Use module save [<name>] and module restore [<name>] for that purpose -- see Lmod documentation on User collections You can also create and install your own modules for your convenience or for sharing software among collaborators. See the modulefile documentation for details of the required format and available commands. These custom modulefiles can be made visible to the module command by module use /path/to/the/custom/modulefiles Warning Make sure the UNIX file permissions grant access to all users who want to use the software. Do not give write permissions to your home directory to anyone else. Note The module use command adds new directories before other module search paths (defined as $MODULEPATH ), so modules defined in a custom directory will have precedence if there are other modules with the same name in the module search paths. If you prefer to have the new directory added at the end of $MODULEPATH , use module use -a instead of module use . Module FAQ \u00b6 Is there an environment variable that captures loaded modules? Yes, active modules can be retrieved via $LOADEDMODULES , this environment variable is automatically changed to reflect active loaded modules that is reflected via module list . If you want to access modulefile path for loaded modules you can retrieve via $_LM_FILES","title":"Modules"},{"location":"environment/modules/#ulhpc-softwaremodules-environment","text":"The UL HPC facility provides a large variety of scientific applications to its user community, either domain-specific codes and general purpose development tools which enable research and innovation excellence across a wide set of computational fields. -- see software list . We use the Environment Modules / LMod framework which provided the module utility on Compute nodes to manage nearly all software. There are two main advantages of the module approach: ULHPC can provide many different versions and/or installations of a single software package on a given machine, including a default version as well as several older and newer version. Users can easily switch to different versions or installations without having to explicitly specify different paths. With modules, the MANPATH and related environment variables are automatically managed. ULHPC modules are in practice automatically generated by Easybuild . EasyBuild (EB for short) is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. A large number of scientific software are supported ( at least 2175 supported software packages since the 4.3.2 release) - see also What is EasyBuild? . For several years now, Easybuild is used to manage the ULHPC User Software Set and generate automatically the module files available to you on our computational resources in either prod (default) or devel (early development/testing) environment -- see ULHPC Toolchains and Software Set Versioning . This enables users to easily extend the global Software Set with their own local software builds, either performed within their global home directory or ( better ) in a shared project directory though Easybuild , which generate automatically module files compliant with the ULHPC module setup .","title":"ULHPC Software/Modules Environment"},{"location":"environment/modules/#environment-modules-and-lmod","text":"Environment Modules are a standard and well-established technology across HPC sites, to permit developing and using complex software and libraries build with dependencies, allowing multiple versions of software stacks and combinations thereof to co-exist. It brings the module command which is used to manage environment variables such as PATH , LD_LIBRARY_PATH and MANPATH , enabling the easy loading and unloading of application/library profiles and their dependencies. Why do you need [Environment] Modules? When users login to a Linux system, they get a login shell and the shell uses Environment variables to run commands and applications. Most common are: PATH : colon-separated list of directories in which your system looks for executable files; MANPATH : colon-separated list of directories in which man searches for the man pages; LD_LIBRARY_PATH : colon-separated list of directories in which your system looks for for ELF / *.so libraries at execution time needed by applications. There are also application specific environment variables such as CPATH , LIBRARY_PATH , JAVA_HOME , LM_LICENSE_FILE , MKLROOT etc. A traditional way to setup these Environment variables is by customizing the shell initialization files : i.e. /etc/profile , .bash_profile , and .bashrc This proves to be very impractical on multi-user systems with various applications and multiple application versions installed as on an HPC facility. To overcome the difficulty of setting and changing the Environment variables, the TCL/C Environment Modules were introduced over 2 decades ago. The Environment Modules package is a tool that simplify shell initialization and lets users easily modify their environment during the session with modulefiles . Each modulefile contains the information needed to configure the shell for an application. Once the Modules package is initialized, the environment can be modified on a per-module basis using the module command which interprets modulefiles. Typically modulefiles instruct the module command to alter or set shell environment variables such as PATH , MANPATH , etc. Modulefiles may be shared by many users on a system (as done on the ULHPC clusters) and users may have their own collection to supplement or replace the shared modulefiles. Modules can be loaded and unloaded dynamically and atomically, in an clean fashion. All popular shells are supported, including bash , ksh , zsh , sh , csh , tcsh , fish , as well as some scripting languages such as perl , ruby , tcl , python , cmake and R . Modules are useful in managing different versions of applications. Modules can also be bundled into metamodules that will load an entire suite of different applications -- this is precisely the way we manage the ULHPC Software Set Tcl/C Environment Modules (Tmod) vs. Tcl Environment Modules vs. Lmod There exists several implementation of the module tool: Tcl/C Environment Modules (3.2.10 \\leq \\leq version < 4), also called Tmod : the seminal ( old ) implementation Tcl-only variant of Environment modules (version \\geq \\geq 4), previously called Modules-Tcl ( recommended ) Lmod , a Lua based Environment Module System Lmod (\"L\" stands for Lua ) provides all of the functionality of TCL/C Environment Modules plus more features: support for hierarchical module file structure MODULEPATH is dynamically updated when modules are loaded. makes loaded modules inactive and active to provide sane environment. supports for hidden modules support for optional usage tracking (implemented on ULHPC facilities) In particular, Lmod enforces the following safety features that are not always guaranted with the other tools: The One Name Rule : Users can only have one version active Users can only load one compiler or MPI stack at a time (through the family(...) directive) The ULHPC Facility relies on Lmod -- the associated Modulefiles being automatically generated by Easybuild . The ULHPC Facility relies on Lmod , a Lua-based Environment module system that easily handles the MODULEPATH Hierarchical problem. In this context, the module command supports the following subcommands: Command Description module avail Lists all the modules which are available to be loaded module spider <pattern> Search for among available modules (Lmod only) module load <mod1> [mod2...] Load a module module unload <module> Unload a module module list List loaded modules module purge Unload all modules (purge) module display <module> Display what a module does module use <path> Prepend the directory to the MODULEPATH environment variable module unuse <path> Remove the directory from the MODULEPATH environment variable What is module ? module is a shell function that modifies user shell upon load of a modulefile. It is defined as follows $ type module module is a function module () { eval $($LMOD_CMD bash \"$@\") && eval $(${LMOD_SETTARG_CMD:-:} -s sh) } In particular, module is NOT a program At the heart of environment modules interaction resides the following components: the MODULEPATH environment variable, which defines a colon-separated list of directories to search for modulefiles modulefile (see an example ) associated to each available software. Example of ULHPC toolchain/foss (auto-generated) Modulefile $ module show toolchain/foss ------------------------------------------------------------------------------- /opt/apps/resif/iris/2019b/broadwell/modules/all/toolchain/foss/2019b.lua: ------------------------------------------------------------------------------- help([[ Description =========== GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. More information ================ - Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain ]]) whatis(\"Description: GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.\") whatis(\"Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain\") whatis(\"URL: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain\") conflict(\"toolchain/foss\") load(\"compiler/GCC/8.3.0\") load(\"mpi/OpenMPI/3.1.4-GCC-8.3.0\") load(\"numlib/OpenBLAS/0.3.7-GCC-8.3.0\") load(\"numlib/FFTW/3.3.8-gompi-2019b\") load(\"numlib/ScaLAPACK/2.0.2-gompi-2019b\") setenv(\"EBROOTFOSS\",\"/opt/apps/resif/iris/2019b/broadwell/software/foss/2019b\") setenv(\"EBVERSIONFOSS\",\"2019b\") setenv(\"EBDEVELFOSS\",\"/opt/apps/resif/iris/2019b/broadwell/software/foss/2019b/easybuild/toolchain-foss-2019b-easybuild-devel\") ( reminder ): the module command is ONLY available on the compute nodes, NOT on the access front-ends. In particular, you need to be within a job to load ULHPC or private modules.","title":"Environment modules and LMod"},{"location":"environment/modules/#ulhpc-modulepath","text":"By default, the MODULEPATH environment variable holds a single searched directory holding the optimized builds prepared for you by the ULHPC Team. The general format of this directory is as follows: /opt/apps/resif/<cluster>/<version>/<arch>/modules/all where: <cluster> depicts the name of the cluster ( iris or aion ). Stored as $ULHPC_CLUSTER . <version> corresponds to the ULHPC Software set release (aligned with Easybuid toolchains release ), i.e. 2019b , 2020a etc. Stored as $RESIF_VERSION_{PROD,DEVEL,LEGACY} depending on the Production / development / legacy ULHPC software set version <arch> is a lower-case strings that categorize the CPU architecture of the build host, and permits to easyli identify optimized target architecture. It is stored as $RESIF_ARCH . On Intel nodes: broadwell ( default ), skylake On AMD nodes: epyc On GPU nodes: gpu Cluster Arch. $RESIF_ARCH $MODULEPATH Environment variable Iris broadwell (default) /opt/apps/resif/iris/<version>/broadwell/modules/all Iris skylake /opt/apps/resif/iris/<version>/skylake/modules/all Iris gpu /opt/apps/resif/iris/<version>/gpu/modules/all Aion epyc (default) /opt/apps/resif/aion/<version>/{epyc}/modules/all On skylake nodes, you may want to use the optimized modules for skylake On GPU nodes, you may want to use the CPU-optimized builds for skylake (in addition to the gpu -enabled softwares) ACM PEARC'21: RESIF 3.0 If you are interested to know more on the wey we setup and deploy the User Software Environment on ULHPC systems through the RESIF 3 framework, you can refer to the below article presented during the ACM PEARC'21 conference, on July 22, 2021. ACM Reference Format | ORBilu entry | OpenAccess | ULHPC blog post | slides | Github : Sebastien Varrette, Emmanuel Kieffer, Frederic Pinel, Ezhilmathi Krishnasamy, Sarah Peter, Hyacinthe Cartiaux, and Xavier Besseron. 2021. RESIF 3.0: Toward a Flexible & Automated Management of User Software Environment on HPC facility. In Practice and Experience in Advanced Research Computing (PEARC '21) . Association for Computing Machinery (ACM), New York, NY, USA, Article 33, 1\u20134. https://doi.org/10.1145/3437359.3465600","title":"ULHPC $MODULEPATH"},{"location":"environment/modules/#module-naming-schemes","text":"What is a Module Naming Scheme? The full software and module install paths for a particular software package are determined by the active module naming scheme along with the general software and modules install paths specified by the EasyBuild configuration. You can list the supported module naming schemes of Easybuild using: $ eb --avail-module-naming-schemes List of supported module naming schemes: EasyBuildMNS CategorizedHMNS MigrateFromEBToHMNS HierarchicalMNS CategorizedModuleNamingScheme See Flat vs. Hierarchical module naming scheme for an illustrated explaination of the difference between two extreme cases: flat or 3-level hierarchical. On ULHPC systems, we selected an intermediate scheme called CategorizedModuleNamingScheme . Module Naming Schemes on ULHPC system ULHPC modules are organised through the Categorized Naming Scheme Format: <category>/<name>/<version>-<toolchain><versionsuffix> This means that the typical module hierarchy has as prefix a category level, taken out from one of the supported software category or module class : $ eb --show-default-moduleclasses Default available module classes: base: Default module class astro: Astronomy, Astrophysics and Cosmology bio: Bioinformatics, biology and biomedical cae: Computer Aided Engineering (incl. CFD) chem: Chemistry, Computational Chemistry and Quantum Chemistry compiler: Compilers data: Data management & processing tools debugger: Debuggers devel: Development tools geo: Earth Sciences ide: Integrated Development Environments (e.g. editors) lang: Languages and programming aids lib: General purpose libraries math: High-level mathematical software mpi: MPI stacks numlib: Numerical Libraries perf: Performance tools quantum: Quantum Computing phys: Physics and physical systems simulations system: System utilities (e.g. highly depending on system OS and hardware) toolchain: EasyBuild toolchains tools: General purpose tools vis: Visualization, plotting, documentation and typesetting It follows that the ULHPC software modules are structured according to the organization depicted below ( click to enlarge ).","title":"Module Naming Schemes"},{"location":"environment/modules/#ulhpc-toolchains-and-software-set-versioning","text":"We offer a YEARLY release of the ULHPC Software Set based on Easybuid release of toolchains -- see Component versions ( fixed per release ) in the foss and intel toolchains. However , count at least 6 months of validation/import after EB release before ULHPC release An overview of the currently available component versions is depicted below: Name Type 2019b ( legacy ) 2020a 2020b ( prod ) 2021a 2021b ( devel ) GCCCore compiler 8.3.0 9.3.0 10.2.0 10.3.0 11.2.0 foss toolchain 2019b 2020a 2020b 2021a 2021b intel toolchain 2019b 2020a 2020b 2021a 2021b binutils 2.32 2.34 2.35 2.36 2.37 Python 3.7.4 (and 2.7.16) 3.8.2 (and 2.7.18) 3.8.6 3.9.2 3.9.6 LLVM compiler 9.0.1 10.0.1 11.0.0 11.1.0 12.0.1 OpenMPI MPI 3.1.4 4.0.3 4.0.5 4.1.1 4.1.2 Once on a node, the current version of the ULHPC Software Set in production is stored in $RESIF_VERSION_PROD . You can use the variables $MODULEPATH_{LEGACY,PROD,DEVEL} to access or set the MODULEPATH command with the appropriate value. Yet we have define utility scripts to facilitate your quick reset of the module environment, i.e., resif-load-swset-{legacy,prod,devel} and resif-reset-swset For instance, if you want to use the legacy software set, proceed as follows in your launcher scripts: resif-load-swset-legacy # Eq. of export MODULEPATH=$MODULEPATH_LEGACY # [...] # Restore production settings resif-load-swset-prod # Eq. of export MODULEPATH=$MODULEPATH_PROD If on the contrary you want to test the (new) development software set, i.e., the devel version, stored in $RESIF_VERSION_DEVEL : resif-load-swset-devel # Eq. of export MODULEPATH=$MODULEPATH_DEVEL # [...] # Restore production settings resif-reset-swset # As resif-load-swset-prod (iris only) Skylake Optimized builds Skylake optimized build can be loaded on regular nodes using resif-load-swset-skylake # Eq. of export MODULEPATH=$MODULEPATH_PROD_SKYLAKE You MUST obviously be on a Skylake node ( sbatch -C skylake [...] ) to take benefit from it. Note that this action is not required on GPU nodes. GPU Optimized builds vs. CPU software set on GPU nodes On GPU nodes, be aware that the default MODULEPATH holds two directories: GPU Optimized builds ( i.e. typically against the {foss,intel}cuda toolchains) stored under /opt/apps/resif/<cluster>/<version>/gpu/modules/all CPU Optimized builds (ex: skylake on Iris )) stored under /opt/apps/resif/<cluster>/<version>/skylake/modules/all You may want to exclude CPU builds to ensure you take the most out of the GPU accelerators. In that case, you may want to run: # /!\\ ADAPT <version> accordingly module unuse /opt/apps/resif/ ${ ULHPC_CLUSTER } / ${ RESIF_VERSION_PROD } /skylake/modules/all","title":"ULHPC Toolchains and Software Set Versioning"},{"location":"environment/modules/#using-easybuild-to-create-custom-modules","text":"Just like we do, you probably want to use Easybuild to complete the existing software set with your own modules and software builds. See Building Custom (or missing) software documentation for more details.","title":"Using Easybuild to Create Custom Modules"},{"location":"environment/modules/#creating-a-custom-module-environment","text":"You can modify your environment so that certain modules are loaded whenever you log in. Use module save [<name>] and module restore [<name>] for that purpose -- see Lmod documentation on User collections You can also create and install your own modules for your convenience or for sharing software among collaborators. See the modulefile documentation for details of the required format and available commands. These custom modulefiles can be made visible to the module command by module use /path/to/the/custom/modulefiles Warning Make sure the UNIX file permissions grant access to all users who want to use the software. Do not give write permissions to your home directory to anyone else. Note The module use command adds new directories before other module search paths (defined as $MODULEPATH ), so modules defined in a custom directory will have precedence if there are other modules with the same name in the module search paths. If you prefer to have the new directory added at the end of $MODULEPATH , use module use -a instead of module use .","title":"Creating a Custom Module Environment"},{"location":"environment/modules/#module-faq","text":"Is there an environment variable that captures loaded modules? Yes, active modules can be retrieved via $LOADEDMODULES , this environment variable is automatically changed to reflect active loaded modules that is reflected via module list . If you want to access modulefile path for loaded modules you can retrieve via $_LM_FILES","title":"Module FAQ"},{"location":"environment/workflow/","text":"ULHPC Workflow \u00b6 Your typical journey on the ULHPC facility is illustrated in the below figure. Typical workflow on UL HPC resources You daily interaction with the ULHPC facility includes the following actions: Preliminary setup Connect to the access/login servers This can be done either by ssh ( recommended ) or via the ULHPC OOD portal ( advanced users ) at this point, you probably want to create (or reattach) to a screen or tmux session Synchronize you code and/or transfer your input data using rsync/svn/git typically recall that the different storage filesystems are shared (via a high-speed interconnect network ) among the computational resources of the ULHPC facilities. In particular, it is sufficient to exchange data with the access servers to make them available on the clusters Reserve a few interactive resources with salloc -p interactive [...] recall that the module command (used to load the ULHPC User software ) is only available on the compute nodes ( eventually ) build your program, typically using gcc/icc/mpicc/nvcc.. Test your workflow / HPC analysis on a small size problem ( srun/python/sh... ) Prepare a launcher script <launcher>.{sh|py} Then you can proceed with your Real Experiments : Reserve passive resources : sbatch [...] <launcher> Grab the results and (eventually) transfer back your output results using rsync/svn/git","title":"Workflow"},{"location":"environment/workflow/#ulhpc-workflow","text":"Your typical journey on the ULHPC facility is illustrated in the below figure. Typical workflow on UL HPC resources You daily interaction with the ULHPC facility includes the following actions: Preliminary setup Connect to the access/login servers This can be done either by ssh ( recommended ) or via the ULHPC OOD portal ( advanced users ) at this point, you probably want to create (or reattach) to a screen or tmux session Synchronize you code and/or transfer your input data using rsync/svn/git typically recall that the different storage filesystems are shared (via a high-speed interconnect network ) among the computational resources of the ULHPC facilities. In particular, it is sufficient to exchange data with the access servers to make them available on the clusters Reserve a few interactive resources with salloc -p interactive [...] recall that the module command (used to load the ULHPC User software ) is only available on the compute nodes ( eventually ) build your program, typically using gcc/icc/mpicc/nvcc.. Test your workflow / HPC analysis on a small size problem ( srun/python/sh... ) Prepare a launcher script <launcher>.{sh|py} Then you can proceed with your Real Experiments : Reserve passive resources : sbatch [...] <launcher> Grab the results and (eventually) transfer back your output results using rsync/svn/git","title":"ULHPC Workflow"},{"location":"filesystems/","text":"Your journey on the ULHPC facility is illustrated in the below figure. In particular, once connected, you have access to several different File Systems (FS) which are configured for different purposes. What is a File System (FS) ? A File System (FS) is just the logical manner to store, organize & access data. There are different types of file systems available nowadays: (local) Disk FS you find on laptops and servers: FAT32 , NTFS , HFS+ , ext4 , {x,z,btr}fs ... Networked FS , such as NFS , CIFS / SMB , AFP , allowing to access a remote storage system as a NAS (Network Attached Storage) Parallel and Distributed FS : such as SpectrumScale/GPFS or Lustre . Those are typical FileSystems you meet on HPC or HTC (High Throughput Computing) facility as they exhibit several unique capabilities: data is spread across multiple storage nodes for redundancy and performance. the global capacity AND the global performances are increased with every systems added to the storage infrastructure. Storage Systems Overview \u00b6 Current statistics of the available filesystems are depicted on the side figure. The ULHPC facility relies on 2 types of Distributed/Parallel File Systems to deliver high-performant Data storage at a BigData scale: IBM Spectrum Scale , formerly known as the General Parallel File System ( GPFS ), a global high -performance clustered file system hosting your $HOME and projects data. Lustre , an open-source, parallel file system dedicated to large, local, parallel scratch storage. In addition, the following file-systems complete the ULHPC storage infrastructure: OneFS, A global low -performance Dell/EMC Isilon solution used to host project data, and serve for backup and archival purposes The ULHPC team relies on other filesystems within its internal backup infrastructure, such as xfs , a high-performant disk file-system deployed on storage/backup servers. Summary \u00b6 Several File Systems co-exist on the ULHPC facility and are configured for different purposes. Each servers and computational resources has access to at least three different file systems with different levels of performance, permanence and available space summarized below Directory Env. file system backup purging /home/users/<login> $HOME GPFS/Spectrumscale yes no /work/projects/ <name> - GPFS/Spectrumscale yes no /scratch/users/<login> $SCRATCH Lustre no yes /mnt/isilon/projects/<name> - OneFS yes* no ULHPC backup policies Quotas and purging policies ULHPC GPFS/SpectrumScale and Lustre filesystems UL Isilon/OneFS filesystems","title":"Overview"},{"location":"filesystems/#storage-systems-overview","text":"Current statistics of the available filesystems are depicted on the side figure. The ULHPC facility relies on 2 types of Distributed/Parallel File Systems to deliver high-performant Data storage at a BigData scale: IBM Spectrum Scale , formerly known as the General Parallel File System ( GPFS ), a global high -performance clustered file system hosting your $HOME and projects data. Lustre , an open-source, parallel file system dedicated to large, local, parallel scratch storage. In addition, the following file-systems complete the ULHPC storage infrastructure: OneFS, A global low -performance Dell/EMC Isilon solution used to host project data, and serve for backup and archival purposes The ULHPC team relies on other filesystems within its internal backup infrastructure, such as xfs , a high-performant disk file-system deployed on storage/backup servers.","title":"Storage Systems Overview"},{"location":"filesystems/#summary","text":"Several File Systems co-exist on the ULHPC facility and are configured for different purposes. Each servers and computational resources has access to at least three different file systems with different levels of performance, permanence and available space summarized below Directory Env. file system backup purging /home/users/<login> $HOME GPFS/Spectrumscale yes no /work/projects/ <name> - GPFS/Spectrumscale yes no /scratch/users/<login> $SCRATCH Lustre no yes /mnt/isilon/projects/<name> - OneFS yes* no ULHPC backup policies Quotas and purging policies ULHPC GPFS/SpectrumScale and Lustre filesystems UL Isilon/OneFS filesystems","title":"Summary"},{"location":"filesystems/gpfs/","text":"GPFS/SpectrumScale ( $HOME , project) \u00b6 Introduction \u00b6 IBM Spectrum Scale , formerly known as the General Parallel File System (GPFS), is global high -performance clustered file system available on all ULHPC computational systems through a DDN GridScaler/GS7K system. It allows sharing homedirs and project data between users, systems, and eventually (i.e. if needed) with the \"outside world\". In terms of raw storage capacities, it represents more than 4PB . Live status Global Home directory $HOME \u00b6 Home directories provide a convenient means for a user to have access to files such as dotfiles, source files, input files, configuration files regardless of the platform. Refer to your home directory using the environment variable $HOME whenever possible. The absolute path may change, but the value of $HOME will always be correct. $HOME quotas and backup policies See quotas for detailed information about inode, space quotas, and file system purge policies. Your HOME is backuped weekly, according to the policy detailed in the ULHPC backup policies . Global Project directory $PROJECTHOME=/work/projects/ \u00b6 Project directories are intended for sharing data within a group of researchers, under /work/projects/<name> Refer to your project base home directory using the environment variable $PROJECTHOME=/work/projects whenever possible. Global Project quotas and backup policies See quotas for detailed information about inode, space quotas, and file system purge policies. Your projects backup directories are backuped weekly, according to the policy detailed in the ULHPC backup policies . Access rights to project directory: Quota for clusterusers group in project directories is 0 !!! When a project <name> is created, a group of the same name ( <name> ) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory. Be aware that your default group as a user is clusterusers which has ( on purpose ) a quota in project directories set to 0 . You thus need to ensure you always write data in your project directory using the <name> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: chmod g+s [...] When using rsync to transfer file toward the project directory /work/projects/<name> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to: give new files the destination-default permissions with --no-p ( --no-perms ), and use the default group <name> of the destination dir with --no-g ( --no-group ) (eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using --chmod=ug=rwX Your full rsync command becomes (adapt accordingly): rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] <source> /work/projects/<name>/[...] For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory /work/projects/<name> , you want to use the sg as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"<command> [...]\" This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"eb [...] -r --rebuild -D\" # Dry-run - enforce using the '<name>' group sg <name> -c \"eb [...] -r --rebuild\" # Dry-run - enforce using the '<name>' group Storage System Implementation \u00b6 The way the ULHPC GPFS file system is implemented is depicted on the below figure. It is composed of: Two NAS protocol servers (see below One DDN GridScaler 7K system acquired as part of RFP 160019 deployed in 2017 and later extended, composed of 1x DDN GS7K enclosure (~11GB/s IO throughput) 4x SS8460 disk expansion enclosures 350x HGST disks (7.2K RPM HDD, 6TB, Self Encrypted Disks (SED) configured over 35 RAID6 (8+2) pools 28x Sandisk SSD 400GB disks Another DDN GridScaler 7K system acquired as part of RFP 190027 deployed in 2020 as part of Aion and later extended. 1x DDN GS7990-EDR embedded storage 4x SS9012 disk expansion enclosures 360x NL-SAS HDDs (6TB, Self Encrypted Disks (SED)) configured over 36 RAID6 (8+2) pools 10x 3.2TB SED SAS-SSD for metadata. There is no single point of failure within the storage solution and the setup is fully redundant. The data paths from the storage to the NSD servers are redundant and providing one link from each of the servers to each controller in the storage unit. There are redundant power supplies, redundant fans, redundant storage controller with mirrored cache and battery backup to secure the cache data when power is lost completely. The data paths to the enclosures are redundant so that links can fail, and the system will still be fully operational. Filesystem Performance \u00b6 The performance of the GS7990 storage system via native GPFS and RDMA based data transport for the HPC filesystem is expected to be in the range of at least 20GB/s for large sequential read and writes, using a filesystem block size of 16MB and scatter or cluster allocation. Performance measurement by IOR , a synthetic benchmark for testing the performance of distributed filesystems is planned upon finalization of the installation. The IOR benchmark IOR is a parallel IO benchmark that can be used to test the performance of parallel storage systems using various interfaces and access patterns. It supports a variety of different APIs to simulate IO load and is nowadays considered as a reference Parallel filesystem I/O benchmark. It recently embedded another well-known benchmark suite called MDTest, a synthetic MPI parallel benchmark for testing the metadata performance of filesystems (such as Lustre or Spectrum Scale GPFS) where each thread is operating its own working set (to create directory/files, read files, delete files or directory tree). In complement to IOR, the IO-500 benchmarking suite (see also the white paper \" Establishing the IO-500 Benchmark \") will be performed. IO-500 aims at capturing user-experienced performance with measured performance representative for: applications with well optimised I/O patterns; applications with random-like workloads; workloads involving metadata small/objects. NAS/NFS Servers \u00b6 Two NAS protocol servers are available, each connected via 2 x IB EDR links to the IB fabric and exporting the filesystem via NFS and SMB over 2 x 10GE links into the Ethernet network.","title":"GPFS/SpectrumScale"},{"location":"filesystems/gpfs/#gpfsspectrumscale-home-project","text":"","title":"GPFS/SpectrumScale ($HOME, project)"},{"location":"filesystems/gpfs/#introduction","text":"IBM Spectrum Scale , formerly known as the General Parallel File System (GPFS), is global high -performance clustered file system available on all ULHPC computational systems through a DDN GridScaler/GS7K system. It allows sharing homedirs and project data between users, systems, and eventually (i.e. if needed) with the \"outside world\". In terms of raw storage capacities, it represents more than 4PB . Live status","title":"Introduction"},{"location":"filesystems/gpfs/#global-home-directory-home","text":"Home directories provide a convenient means for a user to have access to files such as dotfiles, source files, input files, configuration files regardless of the platform. Refer to your home directory using the environment variable $HOME whenever possible. The absolute path may change, but the value of $HOME will always be correct. $HOME quotas and backup policies See quotas for detailed information about inode, space quotas, and file system purge policies. Your HOME is backuped weekly, according to the policy detailed in the ULHPC backup policies .","title":"Global Home directory $HOME"},{"location":"filesystems/gpfs/#global-project-directory-projecthomeworkprojects","text":"Project directories are intended for sharing data within a group of researchers, under /work/projects/<name> Refer to your project base home directory using the environment variable $PROJECTHOME=/work/projects whenever possible. Global Project quotas and backup policies See quotas for detailed information about inode, space quotas, and file system purge policies. Your projects backup directories are backuped weekly, according to the policy detailed in the ULHPC backup policies . Access rights to project directory: Quota for clusterusers group in project directories is 0 !!! When a project <name> is created, a group of the same name ( <name> ) is also created and researchers allowed to collaborate on the project are made members of this group,which grant them access to the project directory. Be aware that your default group as a user is clusterusers which has ( on purpose ) a quota in project directories set to 0 . You thus need to ensure you always write data in your project directory using the <name> group (instead of yoru default one.). This can be achieved by ensuring the setgid bit is set on all folders in the project directories: chmod g+s [...] When using rsync to transfer file toward the project directory /work/projects/<name> as destination, be aware that rsync will not use the correct permissions when copying files into your project directory. As indicated in the Data transfer section, you also need to: give new files the destination-default permissions with --no-p ( --no-perms ), and use the default group <name> of the destination dir with --no-g ( --no-group ) (eventually) instruct rsync to preserve whatever executable permissions existed on the source file and aren't masked at the destination using --chmod=ug=rwX Your full rsync command becomes (adapt accordingly): rsync -avz {--update | --delete} --no-p --no-g [--chmod=ug=rwX] <source> /work/projects/<name>/[...] For the same reason detailed above, in case you are using a build command or more generally any command meant to write data in your project directory /work/projects/<name> , you want to use the sg as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"<command> [...]\" This is particularly important if you are building dedicated software with Easybuild for members of the project - you typically want to do it as follows: # /!\\ ADAPT <name> accordingly sg <name> -c \"eb [...] -r --rebuild -D\" # Dry-run - enforce using the '<name>' group sg <name> -c \"eb [...] -r --rebuild\" # Dry-run - enforce using the '<name>' group","title":"Global Project directory $PROJECTHOME=/work/projects/"},{"location":"filesystems/gpfs/#storage-system-implementation","text":"The way the ULHPC GPFS file system is implemented is depicted on the below figure. It is composed of: Two NAS protocol servers (see below One DDN GridScaler 7K system acquired as part of RFP 160019 deployed in 2017 and later extended, composed of 1x DDN GS7K enclosure (~11GB/s IO throughput) 4x SS8460 disk expansion enclosures 350x HGST disks (7.2K RPM HDD, 6TB, Self Encrypted Disks (SED) configured over 35 RAID6 (8+2) pools 28x Sandisk SSD 400GB disks Another DDN GridScaler 7K system acquired as part of RFP 190027 deployed in 2020 as part of Aion and later extended. 1x DDN GS7990-EDR embedded storage 4x SS9012 disk expansion enclosures 360x NL-SAS HDDs (6TB, Self Encrypted Disks (SED)) configured over 36 RAID6 (8+2) pools 10x 3.2TB SED SAS-SSD for metadata. There is no single point of failure within the storage solution and the setup is fully redundant. The data paths from the storage to the NSD servers are redundant and providing one link from each of the servers to each controller in the storage unit. There are redundant power supplies, redundant fans, redundant storage controller with mirrored cache and battery backup to secure the cache data when power is lost completely. The data paths to the enclosures are redundant so that links can fail, and the system will still be fully operational.","title":"Storage System Implementation"},{"location":"filesystems/gpfs/#filesystem-performance","text":"The performance of the GS7990 storage system via native GPFS and RDMA based data transport for the HPC filesystem is expected to be in the range of at least 20GB/s for large sequential read and writes, using a filesystem block size of 16MB and scatter or cluster allocation. Performance measurement by IOR , a synthetic benchmark for testing the performance of distributed filesystems is planned upon finalization of the installation. The IOR benchmark IOR is a parallel IO benchmark that can be used to test the performance of parallel storage systems using various interfaces and access patterns. It supports a variety of different APIs to simulate IO load and is nowadays considered as a reference Parallel filesystem I/O benchmark. It recently embedded another well-known benchmark suite called MDTest, a synthetic MPI parallel benchmark for testing the metadata performance of filesystems (such as Lustre or Spectrum Scale GPFS) where each thread is operating its own working set (to create directory/files, read files, delete files or directory tree). In complement to IOR, the IO-500 benchmarking suite (see also the white paper \" Establishing the IO-500 Benchmark \") will be performed. IO-500 aims at capturing user-experienced performance with measured performance representative for: applications with well optimised I/O patterns; applications with random-like workloads; workloads involving metadata small/objects.","title":"Filesystem Performance"},{"location":"filesystems/gpfs/#nasnfs-servers","text":"Two NAS protocol servers are available, each connected via 2 x IB EDR links to the IB fabric and exporting the filesystem via NFS and SMB over 2 x 10GE links into the Ethernet network.","title":"NAS/NFS Servers"},{"location":"filesystems/home/","text":"Global Home directory $HOME \u00b6 Home directories provide a convenient means for a user to have access to files such as dotfiles, source files, input files, configuration files regardless of the platform. Refer to your home directory using the environment variable $HOME whenever possible. The absolute path may change, but the value of $HOME will always be correct.","title":"Home"},{"location":"filesystems/home/#global-home-directory-home","text":"Home directories provide a convenient means for a user to have access to files such as dotfiles, source files, input files, configuration files regardless of the platform. Refer to your home directory using the environment variable $HOME whenever possible. The absolute path may change, but the value of $HOME will always be correct.","title":"Global Home directory $HOME"},{"location":"filesystems/isilon/","text":"Dell EMC Isilon (Archives and cold project data) \u00b6 OneFS, A global low -performance Dell/EMC Isilon solution is used to host project data, and serve for backup and archival purposes. You will find them mounted under /mnt/isilon/projects . In 2014, the IT Department of the University , the [UL HPC] ( https://hpc.uni.lu/about/team.html ) and the LCSB join their forces (and their funding) to acquire a scalable and modular NAS solution able to sustain the need for an internal big data storage, i.e. provides space for centralized data and backups of all devices used by the UL staff and all rese arch-related data, including the one proceed on the UL HPC platform. At the end of a public call for tender released in 2014, the EMC Isilon system was finally selected with an effective deployment in 2015. It is physically hosted in the new CDC (Centre de Calcul) server room in the Maison du Savoir . Composed by a large number of disk enclosures featuring the OneFS File System, it currently offers an effective capacity of 3.360 PB. A secondary Isilon cluster, acquired in 2020 and deployed in 2021 is duplicating this setup in a redundant way.","title":"OneFS Isilon"},{"location":"filesystems/isilon/#dell-emc-isilon-archives-and-cold-project-data","text":"OneFS, A global low -performance Dell/EMC Isilon solution is used to host project data, and serve for backup and archival purposes. You will find them mounted under /mnt/isilon/projects . In 2014, the IT Department of the University , the [UL HPC] ( https://hpc.uni.lu/about/team.html ) and the LCSB join their forces (and their funding) to acquire a scalable and modular NAS solution able to sustain the need for an internal big data storage, i.e. provides space for centralized data and backups of all devices used by the UL staff and all rese arch-related data, including the one proceed on the UL HPC platform. At the end of a public call for tender released in 2014, the EMC Isilon system was finally selected with an effective deployment in 2015. It is physically hosted in the new CDC (Centre de Calcul) server room in the Maison du Savoir . Composed by a large number of disk enclosures featuring the OneFS File System, it currently offers an effective capacity of 3.360 PB. A secondary Isilon cluster, acquired in 2020 and deployed in 2021 is duplicating this setup in a redundant way.","title":"Dell EMC Isilon (Archives and cold project data)"},{"location":"filesystems/lfs/","text":"Understanding Lustre I/O \u00b6 When a client (a compute node from your job) needs to create or access a file, the client queries the metadata server (MDS) and the metadata target (MDT) for the layout and location of the file's stripes. Once the file is opened and the client obtains the striping information, the MDS is no longer involved in the file I/O process. The client interacts directly with the object storage servers (OSSes) and OSTs to perform I/O operations such as locking, disk allocation, storage, and retrieval. If multiple clients try to read and write the same part of a file at the same time, the Lustre distributed lock manager enforces coherency, so that all clients see consistent results. Discover MDTs and OSTs \u00b6 ULHPC's Lustre file systems look and act like a single logical storage, but a large files on Lustre can be divided into multiple chunks ( stripes ) and stored across over OSTs. This technique is called file striping . The stripes are distributed among the OSTs in a round-robin fashion to ensure load balancing. It is thus important to know the number of OST on your running system. As mentioned in the Lustre implementation section , the ULHPC Lustre infrastructure is composed of 2 MDS servers (2 MDT), 2 OSS servers and 16 OSTs . You can list the MDTs and OSTs with the command lfs df : $ cds # OR: cd $SCRATCH $ lfs df -h UUID bytes Used Available Use% Mounted on lscratch-MDT0000_UUID 3 .2T 15 .4G 3 .1T 1 % /mnt/lscratch [ MDT:0 ] lscratch-MDT0001_UUID 3 .2T 3 .8G 3 .2T 1 % /mnt/lscratch [ MDT:1 ] lscratch-OST0000_UUID 57 .4T 16 .7T 40 .2T 30 % /mnt/lscratch [ OST:0 ] lscratch-OST0001_UUID 57 .4T 18 .8T 38 .0T 34 % /mnt/lscratch [ OST:1 ] lscratch-OST0002_UUID 57 .4T 17 .6T 39 .3T 31 % /mnt/lscratch [ OST:2 ] lscratch-OST0003_UUID 57 .4T 16 .6T 40 .3T 30 % /mnt/lscratch [ OST:3 ] lscratch-OST0004_UUID 57 .4T 16 .5T 40 .3T 30 % /mnt/lscratch [ OST:4 ] lscratch-OST0005_UUID 57 .4T 16 .5T 40 .3T 30 % /mnt/lscratch [ OST:5 ] lscratch-OST0006_UUID 57 .4T 16 .3T 40 .6T 29 % /mnt/lscratch [ OST:6 ] lscratch-OST0007_UUID 57 .4T 17 .0T 39 .9T 30 % /mnt/lscratch [ OST:7 ] lscratch-OST0008_UUID 57 .4T 16 .8T 40 .0T 30 % /mnt/lscratch [ OST:8 ] lscratch-OST0009_UUID 57 .4T 13 .2T 43 .6T 24 % /mnt/lscratch [ OST:9 ] lscratch-OST000a_UUID 57 .4T 13 .2T 43 .7T 24 % /mnt/lscratch [ OST:10 ] lscratch-OST000b_UUID 57 .4T 13 .3T 43 .6T 24 % /mnt/lscratch [ OST:11 ] lscratch-OST000c_UUID 57 .4T 14 .0T 42 .8T 25 % /mnt/lscratch [ OST:12 ] lscratch-OST000d_UUID 57 .4T 13 .9T 43 .0T 25 % /mnt/lscratch [ OST:13 ] lscratch-OST000e_UUID 57 .4T 14 .4T 42 .5T 26 % /mnt/lscratch [ OST:14 ] lscratch-OST000f_UUID 57 .4T 12 .9T 43 .9T 23 % /mnt/lscratch [ OST:15 ] filesystem_summary: 919 .0T 247 .8T 662 .0T 28 % /mnt/lscratch File striping \u00b6 File striping permits to increase the throughput of operations by taking advantage of several OSSs and OSTs, by allowing one or more clients to read/write different parts of the same file in parallel. On the other hand, striping small files can decrease the performance. File striping allows file sizes larger than a single OST, large files MUST be striped over several OSTs in order to avoid filling a single OST and harming the performance for all users. There is default stripe configuration for ULHPC Lustre filesystems (see below). However, users can set the following stripe parameters for their own directories or files to get optimum I/O performance. You can tune file striping using 3 properties: Property Effect Default Accepted values Advised values stripe_size Size of the file stripes in bytes 1048576 (1m) > 0 > 0 stripe_count Number of OST to stripe across 1 -1 (use all the OSTs), 1-16 -1 stripe_offset Index of the OST where the first stripe of files will be written -1 (automatic) -1 , 0-15 -1 Note : With regards stripe_offset (the index of the OST where the first stripe is to be placed); the default is -1 which results in random selection and using a non-default value is NOT recommended . Note Setting stripe size and stripe count correctly for your needs may significantly affect the I/O performance. Use the lfs getstripe command for getting the stripe parameters. Use lfs setstripe for setting the stripe parameters to get optimal I/O performance. The correct stripe setting depends on your needs and file access patterns. Newly created files and directories will inherit these parameters from their parent directory. However, the parameters cannot be changed on an existing file. $ lfs getstripe dir | filename $ lfs setstripe -s <stripe_size> -c <stripe_count> -o <stripe_offset> dir | filename usage: lfs setstripe -d <directory> (to delete default striping from an existing directory) usage: lfs setstripe [--stripe-count|-c <stripe_count>] [--stripe-index|-i <start_ost_idx>] [--stripe-size|-S <stripe_size>] <directory|filename> Example: $ lfs getstripe $SCRATCH /scratch/users/<login>/ stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 [...] $ lfs setstripe -c -1 $SCRATCH $ lfs getstripe $SCRATCH /scratch/users/<login>/ stripe_count: -1 stripe_size: 1048576 pattern: raid0 stripe_offset: -1 In this example, we view the current stripe setting of the $SCRATCH directory. The stripe count is changed to all OSTs and verified. All files written to this directory will be striped over the maximum number of OSTs (16). Use lfs check osts to see the number and status of active OSTs for each filesystem on the cluster. Learn more by reading the man page: $ lfs check osts $ man lfs File stripping Examples \u00b6 Set the striping parameters for a directory containing only small files (< 20MB) $ cd $SCRATCH $ mkdir test_small_files $ lfs getstripe test_small_files test_small_files stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 pool: $ lfs setstripe --stripe-size 1M --stripe-count 1 test_small_files $ lfs getstripe test_small_files test_small_files stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 Set the striping parameters for a directory containing only large files between 100MB and 1GB $ mkdir test_large_files $ lfs setstripe --stripe-size 2M --stripe-count 2 test_large_files $ lfs getstripe test_large_files test_large_files stripe_count: 2 stripe_size: 2097152 stripe_offset: -1 Set the striping parameters for a directory containing files larger than 1GB $ mkdir test_larger_files $ lfs setstripe --stripe-size 4M --stripe-count 6 test_larger_files $ lfs getstripe test_larger_files test_larger_files stripe_count: 6 stripe_size: 4194304 stripe_offset: -1 Big Data files management on Lustre Using a large stripe size can improve performance when accessing very large files Large stripe size allows each client to have exclusive access to its own part of a file. However, it can be counterproductive in some cases if it does not match your I/O pattern. The choice of stripe size has no effect on a single-stripe file. Note that these are simple examples, the optimal settings defer depending on the application (concurrent threads accessing the same file, size of each write operation, etc). Lustre Best practices \u00b6 Parallel I/O on the same file Increase the stripe_count for parallel I/O to the same file. When multiple processes are writing blocks of data to the same file in parallel, the I/O performance for large files will improve when the stripe_count is set to a larger value. The stripe count sets the number of OSTs to which the file will be written. By default, the stripe count is set to 1. While this default setting provides for efficient access of metadata (for example to support the ls -l command), large files should use stripe counts of greater than 1. This will increase the aggregate I/O bandwidth by using multiple OSTs in parallel instead of just one. A rule of thumb is to use a stripe count approximately equal to the number of gigabytes in the file. Another good practice is to make the stripe count be an integral factor of the number of processes performing the write in parallel, so that you achieve load balance among the OSTs. For example, set the stripe count to 16 instead of 15 when you have 64 processes performing the writes. For more details, you can read the following external resources: Reference Documentation: Managing File Layout (Striping) and Free Space Lustre Wiki Lustre Best Practices - Nasa HECC I/O and Lustre Usage - NISC","title":"Scratch Data Management"},{"location":"filesystems/lfs/#understanding-lustre-io","text":"When a client (a compute node from your job) needs to create or access a file, the client queries the metadata server (MDS) and the metadata target (MDT) for the layout and location of the file's stripes. Once the file is opened and the client obtains the striping information, the MDS is no longer involved in the file I/O process. The client interacts directly with the object storage servers (OSSes) and OSTs to perform I/O operations such as locking, disk allocation, storage, and retrieval. If multiple clients try to read and write the same part of a file at the same time, the Lustre distributed lock manager enforces coherency, so that all clients see consistent results.","title":"Understanding Lustre I/O"},{"location":"filesystems/lfs/#discover-mdts-and-osts","text":"ULHPC's Lustre file systems look and act like a single logical storage, but a large files on Lustre can be divided into multiple chunks ( stripes ) and stored across over OSTs. This technique is called file striping . The stripes are distributed among the OSTs in a round-robin fashion to ensure load balancing. It is thus important to know the number of OST on your running system. As mentioned in the Lustre implementation section , the ULHPC Lustre infrastructure is composed of 2 MDS servers (2 MDT), 2 OSS servers and 16 OSTs . You can list the MDTs and OSTs with the command lfs df : $ cds # OR: cd $SCRATCH $ lfs df -h UUID bytes Used Available Use% Mounted on lscratch-MDT0000_UUID 3 .2T 15 .4G 3 .1T 1 % /mnt/lscratch [ MDT:0 ] lscratch-MDT0001_UUID 3 .2T 3 .8G 3 .2T 1 % /mnt/lscratch [ MDT:1 ] lscratch-OST0000_UUID 57 .4T 16 .7T 40 .2T 30 % /mnt/lscratch [ OST:0 ] lscratch-OST0001_UUID 57 .4T 18 .8T 38 .0T 34 % /mnt/lscratch [ OST:1 ] lscratch-OST0002_UUID 57 .4T 17 .6T 39 .3T 31 % /mnt/lscratch [ OST:2 ] lscratch-OST0003_UUID 57 .4T 16 .6T 40 .3T 30 % /mnt/lscratch [ OST:3 ] lscratch-OST0004_UUID 57 .4T 16 .5T 40 .3T 30 % /mnt/lscratch [ OST:4 ] lscratch-OST0005_UUID 57 .4T 16 .5T 40 .3T 30 % /mnt/lscratch [ OST:5 ] lscratch-OST0006_UUID 57 .4T 16 .3T 40 .6T 29 % /mnt/lscratch [ OST:6 ] lscratch-OST0007_UUID 57 .4T 17 .0T 39 .9T 30 % /mnt/lscratch [ OST:7 ] lscratch-OST0008_UUID 57 .4T 16 .8T 40 .0T 30 % /mnt/lscratch [ OST:8 ] lscratch-OST0009_UUID 57 .4T 13 .2T 43 .6T 24 % /mnt/lscratch [ OST:9 ] lscratch-OST000a_UUID 57 .4T 13 .2T 43 .7T 24 % /mnt/lscratch [ OST:10 ] lscratch-OST000b_UUID 57 .4T 13 .3T 43 .6T 24 % /mnt/lscratch [ OST:11 ] lscratch-OST000c_UUID 57 .4T 14 .0T 42 .8T 25 % /mnt/lscratch [ OST:12 ] lscratch-OST000d_UUID 57 .4T 13 .9T 43 .0T 25 % /mnt/lscratch [ OST:13 ] lscratch-OST000e_UUID 57 .4T 14 .4T 42 .5T 26 % /mnt/lscratch [ OST:14 ] lscratch-OST000f_UUID 57 .4T 12 .9T 43 .9T 23 % /mnt/lscratch [ OST:15 ] filesystem_summary: 919 .0T 247 .8T 662 .0T 28 % /mnt/lscratch","title":"Discover MDTs and OSTs"},{"location":"filesystems/lfs/#file-striping","text":"File striping permits to increase the throughput of operations by taking advantage of several OSSs and OSTs, by allowing one or more clients to read/write different parts of the same file in parallel. On the other hand, striping small files can decrease the performance. File striping allows file sizes larger than a single OST, large files MUST be striped over several OSTs in order to avoid filling a single OST and harming the performance for all users. There is default stripe configuration for ULHPC Lustre filesystems (see below). However, users can set the following stripe parameters for their own directories or files to get optimum I/O performance. You can tune file striping using 3 properties: Property Effect Default Accepted values Advised values stripe_size Size of the file stripes in bytes 1048576 (1m) > 0 > 0 stripe_count Number of OST to stripe across 1 -1 (use all the OSTs), 1-16 -1 stripe_offset Index of the OST where the first stripe of files will be written -1 (automatic) -1 , 0-15 -1 Note : With regards stripe_offset (the index of the OST where the first stripe is to be placed); the default is -1 which results in random selection and using a non-default value is NOT recommended . Note Setting stripe size and stripe count correctly for your needs may significantly affect the I/O performance. Use the lfs getstripe command for getting the stripe parameters. Use lfs setstripe for setting the stripe parameters to get optimal I/O performance. The correct stripe setting depends on your needs and file access patterns. Newly created files and directories will inherit these parameters from their parent directory. However, the parameters cannot be changed on an existing file. $ lfs getstripe dir | filename $ lfs setstripe -s <stripe_size> -c <stripe_count> -o <stripe_offset> dir | filename usage: lfs setstripe -d <directory> (to delete default striping from an existing directory) usage: lfs setstripe [--stripe-count|-c <stripe_count>] [--stripe-index|-i <start_ost_idx>] [--stripe-size|-S <stripe_size>] <directory|filename> Example: $ lfs getstripe $SCRATCH /scratch/users/<login>/ stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 [...] $ lfs setstripe -c -1 $SCRATCH $ lfs getstripe $SCRATCH /scratch/users/<login>/ stripe_count: -1 stripe_size: 1048576 pattern: raid0 stripe_offset: -1 In this example, we view the current stripe setting of the $SCRATCH directory. The stripe count is changed to all OSTs and verified. All files written to this directory will be striped over the maximum number of OSTs (16). Use lfs check osts to see the number and status of active OSTs for each filesystem on the cluster. Learn more by reading the man page: $ lfs check osts $ man lfs","title":"File striping"},{"location":"filesystems/lfs/#file-stripping-examples","text":"Set the striping parameters for a directory containing only small files (< 20MB) $ cd $SCRATCH $ mkdir test_small_files $ lfs getstripe test_small_files test_small_files stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 pool: $ lfs setstripe --stripe-size 1M --stripe-count 1 test_small_files $ lfs getstripe test_small_files test_small_files stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 Set the striping parameters for a directory containing only large files between 100MB and 1GB $ mkdir test_large_files $ lfs setstripe --stripe-size 2M --stripe-count 2 test_large_files $ lfs getstripe test_large_files test_large_files stripe_count: 2 stripe_size: 2097152 stripe_offset: -1 Set the striping parameters for a directory containing files larger than 1GB $ mkdir test_larger_files $ lfs setstripe --stripe-size 4M --stripe-count 6 test_larger_files $ lfs getstripe test_larger_files test_larger_files stripe_count: 6 stripe_size: 4194304 stripe_offset: -1 Big Data files management on Lustre Using a large stripe size can improve performance when accessing very large files Large stripe size allows each client to have exclusive access to its own part of a file. However, it can be counterproductive in some cases if it does not match your I/O pattern. The choice of stripe size has no effect on a single-stripe file. Note that these are simple examples, the optimal settings defer depending on the application (concurrent threads accessing the same file, size of each write operation, etc).","title":"File stripping Examples"},{"location":"filesystems/lfs/#lustre-best-practices","text":"Parallel I/O on the same file Increase the stripe_count for parallel I/O to the same file. When multiple processes are writing blocks of data to the same file in parallel, the I/O performance for large files will improve when the stripe_count is set to a larger value. The stripe count sets the number of OSTs to which the file will be written. By default, the stripe count is set to 1. While this default setting provides for efficient access of metadata (for example to support the ls -l command), large files should use stripe counts of greater than 1. This will increase the aggregate I/O bandwidth by using multiple OSTs in parallel instead of just one. A rule of thumb is to use a stripe count approximately equal to the number of gigabytes in the file. Another good practice is to make the stripe count be an integral factor of the number of processes performing the write in parallel, so that you achieve load balance among the OSTs. For example, set the stripe count to 16 instead of 15 when you have 64 processes performing the writes. For more details, you can read the following external resources: Reference Documentation: Managing File Layout (Striping) and Free Space Lustre Wiki Lustre Best Practices - Nasa HECC I/O and Lustre Usage - NISC","title":"Lustre Best practices"},{"location":"filesystems/lustre/","text":"Lustre ( $SCRATCH ) \u00b6 Introduction \u00b6 The Lustre file system is an open-source, parallel file system that supports many requirements of leadership class HPC simulation environments. It is available as a global high -performance file system on all ULHPC computational systems through a DDN ExaScaler system. It is meant to host temporary scratch data within your jobs. In terms of raw storage capacities, it represents more than 1.6PB . Live status Global Scratch directory $SCRATCH \u00b6 The scratch area is a Lustre -based file system designed for high performance temporary storage of large files. It is thus intended to support large I/O for jobs that are being actively computed on the ULHPC systems. We recommend that you run your jobs, especially data intensive ones, from the ULHPC scratch file system. Refer to your scratch directory using the environment variable $SCRATCH whenever possible (which expands to /scratch/users/$(whoami) ). The scratch file system is shared via the Infiniband network of the ULHPC facility and is available from all nodes while being tuned for high performance. ULHPC $SCRATCH quotas, backup and purging policies Extended ACLs are provided for sharing data with other users using fine-grained control. See quotas for detailed information about inode, space quotas, and file system purge policies. In particular, your SCRATCH directory purged on a regular basis , and is NOT backuped according to the policy detailed in the ULHPC backup policies . A short history of Lustre Lustre was initiated & funded by the U.S. Department of Energy Office of Science & National Nuclear Security Administration laboratories in mid 2000s. Developments continue through the Cluster File Systems (ClusterFS) company founded in 2001. Sun Microsystems acquired ClusterFS in 2007 with the intent to bring Lustre technologies to Sun's ZFS file system and the Solaris operating system. In 2010, Oracle bought Sun and began to manage and release Lustre, however the company was not known for HPC. In December 2010, Oracle announced that they would cease Lustre 2.x development and place Lustre 1.8 into maintenance-only support, creating uncertainty around the future development of the file system. Following this announcement, several new organizations sprang up to provide support and development in an open community development model, including Whamcloud , Open Scalable File Systems ( OpenSFS , a nonprofit organization promoting the Lustre file system to ensure Lustre remains vendor-neutral, open, and free), Xyratex or DDN. By the end of 2010, most Lustre developers had left Oracle. WhamCloud was bought by Intel in 2011 and Xyratex took over the Lustre trade mark, logo, related assets (support) from Oracle. In June 2018, the Lustre team and assets were acquired from Intel by DDN. DDN organized the new acquisition as an independent division, reviving the Whamcloud name for the new division. General Architecture \u00b6 A Lustre file system has three major functional units: One or more MetaData Servers (MDS) nodes (here two) that have one or more MetaData Target (MDT) devices per Lustre filesystem that stores namespace metadata, such as filenames, directories, access permissions, and file layout. The MDT data is stored in a local disk filesystem. However, unlike block-based distributed filesystems, such as GPFS/SpectrumScale and PanFS, where the metadata server controls all of the block allocation, the Lustre metadata server is only involved in pathname and permission checks, and is not involved in any file I/O operations, avoiding I/O scalability bottlenecks on the metadata server. One or more Object Storage Server (OSS) nodes that store file data on one or more Object Storage Target (OST) devices. The capacity of a Lustre file system is the sum of the capacities provided by the OSTs. OSSs do most of the work and thus require as much RAM as possible Rule of thumb: ~2 GB base memory + 1 GB / OST Failover configurations: ~2 GB / OST OSSs should have as much CPUs as possible, but it is not as much critical as on MDS Client(s) that access and use the data. Lustre presents all clients with a unified namespace for all of the files and data in the filesystem, using standard POSIX semantics, and allows concurrent and coherent read and write access to the files in the filesystem. Lustre general features and numbers Lustre brings a modern architecture within an Object based file system with the following features: Adaptable : supports wide range of networks and storage hardware Scalable : Distributed file object handling for 100.000 clients and more Stability : production-quality stability and failover Modular : interfaces for easy adaption Highly Available : no single point of failure when configured with HA software BIG and exapandable : allow for multiple PB in one namespace Open-source and community driven. Lustre provides a POSIX compliant layer supported on most Linux flavours. In terms of raw number capabilities for the Lustre: Max system size: about 64PB Max number of OSTs: 8150 Max number of MDTs: multiple per filesystem supported since Lustre 2.4 Files per directory: 25 Millions (**don't run ls -al ) Max stripes: 2000 since Lustre 2.2 Stripe size: Min 64kB -- Max 2TB Max object size: 16TB( ldiskfs ) 256PB (ZFS) Max file size: 31.35PB ( ldiskfs ) 8EB (ZFS) When to use Lustre? Lustre is optimized for : Large files Sequential throughput Parallel applications writing to different parts of a file Lustre will not perform well for Lots of small files High number of meta data requests, improved on new versions Waste of space on the OSTs Understanding the Lustre Filesystems Storage System Implementation \u00b6 The way the ULHPC Lustre file system is implemented is depicted on the below figure. Acquired as part of RFP 170035 , the ULHPC configuration is based upon: a set of 2x EXAScaler Lustre building blocks that each consist of: 1x DDN SS7700 base enclosure and its controller pair with 4x FDR ports 1x DDN SS8460 disk expansion enclosure (84-slot drive enclosures) OSTs: 160x SEAGATE disks (7.2K RPM HDD, 8TB, Self Encrypted Disks (SED)) configured over 16 RAID6 (8+2) pools and extra disks in spare pools MDTs: 18x HGST disks (10K RPM HDD, 1.8TB, Self Encrypted Disks (SED)) configured over 8 RAID1 pools and extra disks in spare pools Two redundant MDS servers Dell R630, 2x Intel Xeon E5-2667v4 @ 3.20GHz [8c], 128GB RAM Two redundant OSS servers Dell R630XL, 2x Intel Xeon E5-2640v4 @ 2.40GHz [10c], 128GB RAM Criteria Value Power (nominal) 6.8 KW Power (idle) 5.5 KW Weight 432 kg Rack Height 22U LNet is configured to be performed with OST based balancing. Filesystem Performance \u00b6 The performance of the ULHPC Lustre filesystem is expected to be in the range of at least 15GB/s for large sequential read and writes. IOR \u00b6 Upon release of the system, performance measurement by IOR , a synthetic benchmark for testing the performance of distributed filesystems, was run for an increasing number of clients as well as with 1kiB, 4kiB, 1MiB and 4MiB transfer sizes. As can be seen, aggregated writes and reads exceed 15 GB/s (depending on the test) which meets the minimum requirement. FIO \u00b6 Random IOPS benchmark was performed using FIO with 20 and 40 GB file size over 8 jobs, leading to the following total size of 160GB and 320 GB 320 GB is > 2 \\times \\times RAM size of the OSS node (128 GB RAM) 160 GB is > 1 \\times \\times RAM size of the OSS node (128 GB RAM) MDTEST \u00b6 Mdtest (based on the 7c0ec41 on September 11 , 2017 (based on v1.9.3)) was used to benchmark the metadata capabilities of the delivered system. HT was turned on to be able to run 32 threads. Mind the logarithmic Y-Axis. Tests on 4 clients with up to 20 threads have been included as well to show the scalability of the system. Lustre Usage \u00b6 Understanding Lustre I/O \u00b6 When a client (a compute node from your job) needs to create or access a file, the client queries the metadata server (MDS) and the metadata target (MDT) for the layout and location of the file's stripes. Once the file is opened and the client obtains the striping information, the MDS is no longer involved in the file I/O process. The client interacts directly with the object storage servers (OSSes) and OSTs to perform I/O operations such as locking, disk allocation, storage, and retrieval. If multiple clients try to read and write the same part of a file at the same time, the Lustre distributed lock manager enforces coherency, so that all clients see consistent results. Discover MDTs and OSTs \u00b6 ULHPC's Lustre file systems look and act like a single logical storage, but a large files on Lustre can be divided into multiple chunks ( stripes ) and stored across over OSTs. This technique is called file striping . The stripes are distributed among the OSTs in a round-robin fashion to ensure load balancing. It is thus important to know the number of OST on your running system. As mentioned in the Lustre implementation section , the ULHPC Lustre infrastructure is composed of 2 MDS servers (2 MDT), 2 OSS servers and 16 OSTs . You can list the MDTs and OSTs with the command lfs df : $ cds # OR: cd $SCRATCH $ lfs df -h UUID bytes Used Available Use% Mounted on lscratch-MDT0000_UUID 3 .2T 15 .4G 3 .1T 1 % /mnt/lscratch [ MDT:0 ] lscratch-MDT0001_UUID 3 .2T 3 .8G 3 .2T 1 % /mnt/lscratch [ MDT:1 ] lscratch-OST0000_UUID 57 .4T 16 .7T 40 .2T 30 % /mnt/lscratch [ OST:0 ] lscratch-OST0001_UUID 57 .4T 18 .8T 38 .0T 34 % /mnt/lscratch [ OST:1 ] lscratch-OST0002_UUID 57 .4T 17 .6T 39 .3T 31 % /mnt/lscratch [ OST:2 ] lscratch-OST0003_UUID 57 .4T 16 .6T 40 .3T 30 % /mnt/lscratch [ OST:3 ] lscratch-OST0004_UUID 57 .4T 16 .5T 40 .3T 30 % /mnt/lscratch [ OST:4 ] lscratch-OST0005_UUID 57 .4T 16 .5T 40 .3T 30 % /mnt/lscratch [ OST:5 ] lscratch-OST0006_UUID 57 .4T 16 .3T 40 .6T 29 % /mnt/lscratch [ OST:6 ] lscratch-OST0007_UUID 57 .4T 17 .0T 39 .9T 30 % /mnt/lscratch [ OST:7 ] lscratch-OST0008_UUID 57 .4T 16 .8T 40 .0T 30 % /mnt/lscratch [ OST:8 ] lscratch-OST0009_UUID 57 .4T 13 .2T 43 .6T 24 % /mnt/lscratch [ OST:9 ] lscratch-OST000a_UUID 57 .4T 13 .2T 43 .7T 24 % /mnt/lscratch [ OST:10 ] lscratch-OST000b_UUID 57 .4T 13 .3T 43 .6T 24 % /mnt/lscratch [ OST:11 ] lscratch-OST000c_UUID 57 .4T 14 .0T 42 .8T 25 % /mnt/lscratch [ OST:12 ] lscratch-OST000d_UUID 57 .4T 13 .9T 43 .0T 25 % /mnt/lscratch [ OST:13 ] lscratch-OST000e_UUID 57 .4T 14 .4T 42 .5T 26 % /mnt/lscratch [ OST:14 ] lscratch-OST000f_UUID 57 .4T 12 .9T 43 .9T 23 % /mnt/lscratch [ OST:15 ] filesystem_summary: 919 .0T 247 .8T 662 .0T 28 % /mnt/lscratch File striping \u00b6 File striping permits to increase the throughput of operations by taking advantage of several OSSs and OSTs, by allowing one or more clients to read/write different parts of the same file in parallel. On the other hand, striping small files can decrease the performance. File striping allows file sizes larger than a single OST, large files MUST be striped over several OSTs in order to avoid filling a single OST and harming the performance for all users. There is default stripe configuration for ULHPC Lustre filesystems (see below). However, users can set the following stripe parameters for their own directories or files to get optimum I/O performance. You can tune file striping using 3 properties: Property Effect Default Accepted values Advised values stripe_size Size of the file stripes in bytes 1048576 (1m) > 0 > 0 stripe_count Number of OST to stripe across 1 -1 (use all the OSTs), 1-16 -1 stripe_offset Index of the OST where the first stripe of files will be written -1 (automatic) -1 , 0-15 -1 Note : With regards stripe_offset (the index of the OST where the first stripe is to be placed); the default is -1 which results in random selection and using a non-default value is NOT recommended . Note Setting stripe size and stripe count correctly for your needs may significantly affect the I/O performance. Use the lfs getstripe command for getting the stripe parameters. Use lfs setstripe for setting the stripe parameters to get optimal I/O performance. The correct stripe setting depends on your needs and file access patterns. Newly created files and directories will inherit these parameters from their parent directory. However, the parameters cannot be changed on an existing file. $ lfs getstripe dir | filename $ lfs setstripe -s <stripe_size> -c <stripe_count> -o <stripe_offset> dir | filename usage: lfs setstripe -d <directory> (to delete default striping from an existing directory) usage: lfs setstripe [--stripe-count|-c <stripe_count>] [--stripe-index|-i <start_ost_idx>] [--stripe-size|-S <stripe_size>] <directory|filename> Example: $ lfs getstripe $SCRATCH /scratch/users/<login>/ stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 [...] $ lfs setstripe -c -1 $SCRATCH $ lfs getstripe $SCRATCH /scratch/users/<login>/ stripe_count: -1 stripe_size: 1048576 pattern: raid0 stripe_offset: -1 In this example, we view the current stripe setting of the $SCRATCH directory. The stripe count is changed to all OSTs and verified. All files written to this directory will be striped over the maximum number of OSTs (16). Use lfs check osts to see the number and status of active OSTs for each filesystem on the cluster. Learn more by reading the man page: $ lfs check osts $ man lfs File stripping Examples \u00b6 Set the striping parameters for a directory containing only small files (< 20MB) $ cd $SCRATCH $ mkdir test_small_files $ lfs getstripe test_small_files test_small_files stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 pool: $ lfs setstripe --stripe-size 1M --stripe-count 1 test_small_files $ lfs getstripe test_small_files test_small_files stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 Set the striping parameters for a directory containing only large files between 100MB and 1GB $ mkdir test_large_files $ lfs setstripe --stripe-size 2M --stripe-count 2 test_large_files $ lfs getstripe test_large_files test_large_files stripe_count: 2 stripe_size: 2097152 stripe_offset: -1 Set the striping parameters for a directory containing files larger than 1GB $ mkdir test_larger_files $ lfs setstripe --stripe-size 4M --stripe-count 6 test_larger_files $ lfs getstripe test_larger_files test_larger_files stripe_count: 6 stripe_size: 4194304 stripe_offset: -1 Big Data files management on Lustre Using a large stripe size can improve performance when accessing very large files Large stripe size allows each client to have exclusive access to its own part of a file. However, it can be counterproductive in some cases if it does not match your I/O pattern. The choice of stripe size has no effect on a single-stripe file. Note that these are simple examples, the optimal settings defer depending on the application (concurrent threads accessing the same file, size of each write operation, etc). Lustre Best practices \u00b6 Parallel I/O on the same file Increase the stripe_count for parallel I/O to the same file. When multiple processes are writing blocks of data to the same file in parallel, the I/O performance for large files will improve when the stripe_count is set to a larger value. The stripe count sets the number of OSTs to which the file will be written. By default, the stripe count is set to 1. While this default setting provides for efficient access of metadata (for example to support the ls -l command), large files should use stripe counts of greater than 1. This will increase the aggregate I/O bandwidth by using multiple OSTs in parallel instead of just one. A rule of thumb is to use a stripe count approximately equal to the number of gigabytes in the file. Another good practice is to make the stripe count be an integral factor of the number of processes performing the write in parallel, so that you achieve load balance among the OSTs. For example, set the stripe count to 16 instead of 15 when you have 64 processes performing the writes. For more details, you can read the following external resources: Reference Documentation: Managing File Layout (Striping) and Free Space Lustre Wiki Lustre Best Practices - Nasa HECC I/O and Lustre Usage - NISC","title":"Lustre"},{"location":"filesystems/lustre/#lustre-scratch","text":"","title":"Lustre ($SCRATCH)"},{"location":"filesystems/lustre/#introduction","text":"The Lustre file system is an open-source, parallel file system that supports many requirements of leadership class HPC simulation environments. It is available as a global high -performance file system on all ULHPC computational systems through a DDN ExaScaler system. It is meant to host temporary scratch data within your jobs. In terms of raw storage capacities, it represents more than 1.6PB . Live status","title":"Introduction"},{"location":"filesystems/lustre/#global-scratch-directory-scratch","text":"The scratch area is a Lustre -based file system designed for high performance temporary storage of large files. It is thus intended to support large I/O for jobs that are being actively computed on the ULHPC systems. We recommend that you run your jobs, especially data intensive ones, from the ULHPC scratch file system. Refer to your scratch directory using the environment variable $SCRATCH whenever possible (which expands to /scratch/users/$(whoami) ). The scratch file system is shared via the Infiniband network of the ULHPC facility and is available from all nodes while being tuned for high performance. ULHPC $SCRATCH quotas, backup and purging policies Extended ACLs are provided for sharing data with other users using fine-grained control. See quotas for detailed information about inode, space quotas, and file system purge policies. In particular, your SCRATCH directory purged on a regular basis , and is NOT backuped according to the policy detailed in the ULHPC backup policies . A short history of Lustre Lustre was initiated & funded by the U.S. Department of Energy Office of Science & National Nuclear Security Administration laboratories in mid 2000s. Developments continue through the Cluster File Systems (ClusterFS) company founded in 2001. Sun Microsystems acquired ClusterFS in 2007 with the intent to bring Lustre technologies to Sun's ZFS file system and the Solaris operating system. In 2010, Oracle bought Sun and began to manage and release Lustre, however the company was not known for HPC. In December 2010, Oracle announced that they would cease Lustre 2.x development and place Lustre 1.8 into maintenance-only support, creating uncertainty around the future development of the file system. Following this announcement, several new organizations sprang up to provide support and development in an open community development model, including Whamcloud , Open Scalable File Systems ( OpenSFS , a nonprofit organization promoting the Lustre file system to ensure Lustre remains vendor-neutral, open, and free), Xyratex or DDN. By the end of 2010, most Lustre developers had left Oracle. WhamCloud was bought by Intel in 2011 and Xyratex took over the Lustre trade mark, logo, related assets (support) from Oracle. In June 2018, the Lustre team and assets were acquired from Intel by DDN. DDN organized the new acquisition as an independent division, reviving the Whamcloud name for the new division.","title":"Global Scratch directory $SCRATCH"},{"location":"filesystems/lustre/#general-architecture","text":"A Lustre file system has three major functional units: One or more MetaData Servers (MDS) nodes (here two) that have one or more MetaData Target (MDT) devices per Lustre filesystem that stores namespace metadata, such as filenames, directories, access permissions, and file layout. The MDT data is stored in a local disk filesystem. However, unlike block-based distributed filesystems, such as GPFS/SpectrumScale and PanFS, where the metadata server controls all of the block allocation, the Lustre metadata server is only involved in pathname and permission checks, and is not involved in any file I/O operations, avoiding I/O scalability bottlenecks on the metadata server. One or more Object Storage Server (OSS) nodes that store file data on one or more Object Storage Target (OST) devices. The capacity of a Lustre file system is the sum of the capacities provided by the OSTs. OSSs do most of the work and thus require as much RAM as possible Rule of thumb: ~2 GB base memory + 1 GB / OST Failover configurations: ~2 GB / OST OSSs should have as much CPUs as possible, but it is not as much critical as on MDS Client(s) that access and use the data. Lustre presents all clients with a unified namespace for all of the files and data in the filesystem, using standard POSIX semantics, and allows concurrent and coherent read and write access to the files in the filesystem. Lustre general features and numbers Lustre brings a modern architecture within an Object based file system with the following features: Adaptable : supports wide range of networks and storage hardware Scalable : Distributed file object handling for 100.000 clients and more Stability : production-quality stability and failover Modular : interfaces for easy adaption Highly Available : no single point of failure when configured with HA software BIG and exapandable : allow for multiple PB in one namespace Open-source and community driven. Lustre provides a POSIX compliant layer supported on most Linux flavours. In terms of raw number capabilities for the Lustre: Max system size: about 64PB Max number of OSTs: 8150 Max number of MDTs: multiple per filesystem supported since Lustre 2.4 Files per directory: 25 Millions (**don't run ls -al ) Max stripes: 2000 since Lustre 2.2 Stripe size: Min 64kB -- Max 2TB Max object size: 16TB( ldiskfs ) 256PB (ZFS) Max file size: 31.35PB ( ldiskfs ) 8EB (ZFS) When to use Lustre? Lustre is optimized for : Large files Sequential throughput Parallel applications writing to different parts of a file Lustre will not perform well for Lots of small files High number of meta data requests, improved on new versions Waste of space on the OSTs Understanding the Lustre Filesystems","title":"General Architecture"},{"location":"filesystems/lustre/#storage-system-implementation","text":"The way the ULHPC Lustre file system is implemented is depicted on the below figure. Acquired as part of RFP 170035 , the ULHPC configuration is based upon: a set of 2x EXAScaler Lustre building blocks that each consist of: 1x DDN SS7700 base enclosure and its controller pair with 4x FDR ports 1x DDN SS8460 disk expansion enclosure (84-slot drive enclosures) OSTs: 160x SEAGATE disks (7.2K RPM HDD, 8TB, Self Encrypted Disks (SED)) configured over 16 RAID6 (8+2) pools and extra disks in spare pools MDTs: 18x HGST disks (10K RPM HDD, 1.8TB, Self Encrypted Disks (SED)) configured over 8 RAID1 pools and extra disks in spare pools Two redundant MDS servers Dell R630, 2x Intel Xeon E5-2667v4 @ 3.20GHz [8c], 128GB RAM Two redundant OSS servers Dell R630XL, 2x Intel Xeon E5-2640v4 @ 2.40GHz [10c], 128GB RAM Criteria Value Power (nominal) 6.8 KW Power (idle) 5.5 KW Weight 432 kg Rack Height 22U LNet is configured to be performed with OST based balancing.","title":"Storage System Implementation"},{"location":"filesystems/lustre/#filesystem-performance","text":"The performance of the ULHPC Lustre filesystem is expected to be in the range of at least 15GB/s for large sequential read and writes.","title":"Filesystem Performance"},{"location":"filesystems/lustre/#ior","text":"Upon release of the system, performance measurement by IOR , a synthetic benchmark for testing the performance of distributed filesystems, was run for an increasing number of clients as well as with 1kiB, 4kiB, 1MiB and 4MiB transfer sizes. As can be seen, aggregated writes and reads exceed 15 GB/s (depending on the test) which meets the minimum requirement.","title":"IOR"},{"location":"filesystems/lustre/#fio","text":"Random IOPS benchmark was performed using FIO with 20 and 40 GB file size over 8 jobs, leading to the following total size of 160GB and 320 GB 320 GB is > 2 \\times \\times RAM size of the OSS node (128 GB RAM) 160 GB is > 1 \\times \\times RAM size of the OSS node (128 GB RAM)","title":"FIO"},{"location":"filesystems/lustre/#mdtest","text":"Mdtest (based on the 7c0ec41 on September 11 , 2017 (based on v1.9.3)) was used to benchmark the metadata capabilities of the delivered system. HT was turned on to be able to run 32 threads. Mind the logarithmic Y-Axis. Tests on 4 clients with up to 20 threads have been included as well to show the scalability of the system.","title":"MDTEST"},{"location":"filesystems/lustre/#lustre-usage","text":"","title":"Lustre Usage"},{"location":"filesystems/lustre/#understanding-lustre-io","text":"When a client (a compute node from your job) needs to create or access a file, the client queries the metadata server (MDS) and the metadata target (MDT) for the layout and location of the file's stripes. Once the file is opened and the client obtains the striping information, the MDS is no longer involved in the file I/O process. The client interacts directly with the object storage servers (OSSes) and OSTs to perform I/O operations such as locking, disk allocation, storage, and retrieval. If multiple clients try to read and write the same part of a file at the same time, the Lustre distributed lock manager enforces coherency, so that all clients see consistent results.","title":"Understanding Lustre I/O"},{"location":"filesystems/lustre/#discover-mdts-and-osts","text":"ULHPC's Lustre file systems look and act like a single logical storage, but a large files on Lustre can be divided into multiple chunks ( stripes ) and stored across over OSTs. This technique is called file striping . The stripes are distributed among the OSTs in a round-robin fashion to ensure load balancing. It is thus important to know the number of OST on your running system. As mentioned in the Lustre implementation section , the ULHPC Lustre infrastructure is composed of 2 MDS servers (2 MDT), 2 OSS servers and 16 OSTs . You can list the MDTs and OSTs with the command lfs df : $ cds # OR: cd $SCRATCH $ lfs df -h UUID bytes Used Available Use% Mounted on lscratch-MDT0000_UUID 3 .2T 15 .4G 3 .1T 1 % /mnt/lscratch [ MDT:0 ] lscratch-MDT0001_UUID 3 .2T 3 .8G 3 .2T 1 % /mnt/lscratch [ MDT:1 ] lscratch-OST0000_UUID 57 .4T 16 .7T 40 .2T 30 % /mnt/lscratch [ OST:0 ] lscratch-OST0001_UUID 57 .4T 18 .8T 38 .0T 34 % /mnt/lscratch [ OST:1 ] lscratch-OST0002_UUID 57 .4T 17 .6T 39 .3T 31 % /mnt/lscratch [ OST:2 ] lscratch-OST0003_UUID 57 .4T 16 .6T 40 .3T 30 % /mnt/lscratch [ OST:3 ] lscratch-OST0004_UUID 57 .4T 16 .5T 40 .3T 30 % /mnt/lscratch [ OST:4 ] lscratch-OST0005_UUID 57 .4T 16 .5T 40 .3T 30 % /mnt/lscratch [ OST:5 ] lscratch-OST0006_UUID 57 .4T 16 .3T 40 .6T 29 % /mnt/lscratch [ OST:6 ] lscratch-OST0007_UUID 57 .4T 17 .0T 39 .9T 30 % /mnt/lscratch [ OST:7 ] lscratch-OST0008_UUID 57 .4T 16 .8T 40 .0T 30 % /mnt/lscratch [ OST:8 ] lscratch-OST0009_UUID 57 .4T 13 .2T 43 .6T 24 % /mnt/lscratch [ OST:9 ] lscratch-OST000a_UUID 57 .4T 13 .2T 43 .7T 24 % /mnt/lscratch [ OST:10 ] lscratch-OST000b_UUID 57 .4T 13 .3T 43 .6T 24 % /mnt/lscratch [ OST:11 ] lscratch-OST000c_UUID 57 .4T 14 .0T 42 .8T 25 % /mnt/lscratch [ OST:12 ] lscratch-OST000d_UUID 57 .4T 13 .9T 43 .0T 25 % /mnt/lscratch [ OST:13 ] lscratch-OST000e_UUID 57 .4T 14 .4T 42 .5T 26 % /mnt/lscratch [ OST:14 ] lscratch-OST000f_UUID 57 .4T 12 .9T 43 .9T 23 % /mnt/lscratch [ OST:15 ] filesystem_summary: 919 .0T 247 .8T 662 .0T 28 % /mnt/lscratch","title":"Discover MDTs and OSTs"},{"location":"filesystems/lustre/#file-striping","text":"File striping permits to increase the throughput of operations by taking advantage of several OSSs and OSTs, by allowing one or more clients to read/write different parts of the same file in parallel. On the other hand, striping small files can decrease the performance. File striping allows file sizes larger than a single OST, large files MUST be striped over several OSTs in order to avoid filling a single OST and harming the performance for all users. There is default stripe configuration for ULHPC Lustre filesystems (see below). However, users can set the following stripe parameters for their own directories or files to get optimum I/O performance. You can tune file striping using 3 properties: Property Effect Default Accepted values Advised values stripe_size Size of the file stripes in bytes 1048576 (1m) > 0 > 0 stripe_count Number of OST to stripe across 1 -1 (use all the OSTs), 1-16 -1 stripe_offset Index of the OST where the first stripe of files will be written -1 (automatic) -1 , 0-15 -1 Note : With regards stripe_offset (the index of the OST where the first stripe is to be placed); the default is -1 which results in random selection and using a non-default value is NOT recommended . Note Setting stripe size and stripe count correctly for your needs may significantly affect the I/O performance. Use the lfs getstripe command for getting the stripe parameters. Use lfs setstripe for setting the stripe parameters to get optimal I/O performance. The correct stripe setting depends on your needs and file access patterns. Newly created files and directories will inherit these parameters from their parent directory. However, the parameters cannot be changed on an existing file. $ lfs getstripe dir | filename $ lfs setstripe -s <stripe_size> -c <stripe_count> -o <stripe_offset> dir | filename usage: lfs setstripe -d <directory> (to delete default striping from an existing directory) usage: lfs setstripe [--stripe-count|-c <stripe_count>] [--stripe-index|-i <start_ost_idx>] [--stripe-size|-S <stripe_size>] <directory|filename> Example: $ lfs getstripe $SCRATCH /scratch/users/<login>/ stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 [...] $ lfs setstripe -c -1 $SCRATCH $ lfs getstripe $SCRATCH /scratch/users/<login>/ stripe_count: -1 stripe_size: 1048576 pattern: raid0 stripe_offset: -1 In this example, we view the current stripe setting of the $SCRATCH directory. The stripe count is changed to all OSTs and verified. All files written to this directory will be striped over the maximum number of OSTs (16). Use lfs check osts to see the number and status of active OSTs for each filesystem on the cluster. Learn more by reading the man page: $ lfs check osts $ man lfs","title":"File striping"},{"location":"filesystems/lustre/#file-stripping-examples","text":"Set the striping parameters for a directory containing only small files (< 20MB) $ cd $SCRATCH $ mkdir test_small_files $ lfs getstripe test_small_files test_small_files stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 pool: $ lfs setstripe --stripe-size 1M --stripe-count 1 test_small_files $ lfs getstripe test_small_files test_small_files stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 Set the striping parameters for a directory containing only large files between 100MB and 1GB $ mkdir test_large_files $ lfs setstripe --stripe-size 2M --stripe-count 2 test_large_files $ lfs getstripe test_large_files test_large_files stripe_count: 2 stripe_size: 2097152 stripe_offset: -1 Set the striping parameters for a directory containing files larger than 1GB $ mkdir test_larger_files $ lfs setstripe --stripe-size 4M --stripe-count 6 test_larger_files $ lfs getstripe test_larger_files test_larger_files stripe_count: 6 stripe_size: 4194304 stripe_offset: -1 Big Data files management on Lustre Using a large stripe size can improve performance when accessing very large files Large stripe size allows each client to have exclusive access to its own part of a file. However, it can be counterproductive in some cases if it does not match your I/O pattern. The choice of stripe size has no effect on a single-stripe file. Note that these are simple examples, the optimal settings defer depending on the application (concurrent threads accessing the same file, size of each write operation, etc).","title":"File stripping Examples"},{"location":"filesystems/lustre/#lustre-best-practices","text":"Parallel I/O on the same file Increase the stripe_count for parallel I/O to the same file. When multiple processes are writing blocks of data to the same file in parallel, the I/O performance for large files will improve when the stripe_count is set to a larger value. The stripe count sets the number of OSTs to which the file will be written. By default, the stripe count is set to 1. While this default setting provides for efficient access of metadata (for example to support the ls -l command), large files should use stripe counts of greater than 1. This will increase the aggregate I/O bandwidth by using multiple OSTs in parallel instead of just one. A rule of thumb is to use a stripe count approximately equal to the number of gigabytes in the file. Another good practice is to make the stripe count be an integral factor of the number of processes performing the write in parallel, so that you achieve load balance among the OSTs. For example, set the stripe count to 16 instead of 15 when you have 64 processes performing the writes. For more details, you can read the following external resources: Reference Documentation: Managing File Layout (Striping) and Free Space Lustre Wiki Lustre Best Practices - Nasa HECC I/O and Lustre Usage - NISC","title":"Lustre Best practices"},{"location":"filesystems/overview/","text":"ULHPC File Systems Overview \u00b6 Several File Systems co-exist on the ULHPC facility and are configured for different purposes. Each servers and computational resources has access to at least three different file systems with different levels of performance, permanence and available space summarized below Directory Env. file system backup purging /home/users/<login> $HOME GPFS/Spectrumscale yes no /work/projects/ <name> - GPFS/Spectrumscale yes no /scratch/users/<login> $SCRATCH Lustre no yes /mnt/isilon/projects/<name> - OneFS yes* no","title":"Overview"},{"location":"filesystems/overview/#ulhpc-file-systems-overview","text":"Several File Systems co-exist on the ULHPC facility and are configured for different purposes. Each servers and computational resources has access to at least three different file systems with different levels of performance, permanence and available space summarized below Directory Env. file system backup purging /home/users/<login> $HOME GPFS/Spectrumscale yes no /work/projects/ <name> - GPFS/Spectrumscale yes no /scratch/users/<login> $SCRATCH Lustre no yes /mnt/isilon/projects/<name> - OneFS yes* no","title":"ULHPC File Systems Overview"},{"location":"filesystems/projecthome/","text":"Global Project directory $PROJECTHOME=/work/projects/ \u00b6 Project directories are intended for sharing data within a group of researchers, under /work/projects/<name> Refer to your project base home directory using the environment variable $PROJECTHOME=/work/projects whenever possible.","title":"Projecthome"},{"location":"filesystems/projecthome/#global-project-directory-projecthomeworkprojects","text":"Project directories are intended for sharing data within a group of researchers, under /work/projects/<name> Refer to your project base home directory using the environment variable $PROJECTHOME=/work/projects whenever possible.","title":"Global Project directory $PROJECTHOME=/work/projects/"},{"location":"filesystems/quotas/","text":"Quotas and Purging \u00b6 Overview \u00b6 Directory Default space quota Default inode quota Purge time $HOME 500 GB 1 M - $SCRATCH 10 TB 1 M 60 days /work/projects/... 1 TB 1 M - /mnt/isilon/projects/... 1.14 PB globally - - Quotas \u00b6 Warning When a quota is reached writes to that directory will fail. Note On Isilon everyone shares one global quota. Unfortunately it is not possible to see the quota status on the cluster. Current usage \u00b6 We provide the df-ulhpc command on the cluster login nodes, which displays current usage, soft quota, hard quota and grace period. Any directories that have exceeded the quota will be highlighted in red. Once you reach the soft quota you can still write data until the grace period expires (7 days) or you reach the hard quota. After you reach the end of the grace period or the hard quota, you have to reduce your usage to below the soft quota to be able to write data again. Check current space quota status: df-ulhpc Check current inode quota status: df-ulhpc -i Check free space on all file systems: df -h Check free space on current file system: df -h . Increases \u00b6 If you or your project needs additional space or inodes for your scratch or project directory you may request it via ServiceNow (HPC \u2192 Storage & projects \u2192 Extend quota). Quotas on the home directory cannot be increased. Troubleshooting \u00b6 The quotas on project directories are based on the group. Be aware that the quota for the default user group clusterusers is 0. If you get a quota error, but df-ulhpc and df-ulhpc -i confirm that the quota is not expired, you are most likely trying to write a file with the group clusterusers instead of the project group. To avoid this issue, check out the newgrp command or set the s mode bit (\"set group ID\") on the directory with chmod g+s <directory> . The s bit means that any file or folder created below will inherit the group. To transfer data with rsync into a project directory, please check the data transfer documentation . Purging \u00b6 Files in the scratch ( $SCRATCH , /mnt/lscratch ) directories are purged. This means files that have not been read (i.e. atime has not been updated) within the last 60 days are automatically removed every month . Files in /tmp on the compute nodes are removed at the end of the job.","title":"Quotas and Purging"},{"location":"filesystems/quotas/#quotas-and-purging","text":"","title":"Quotas and Purging"},{"location":"filesystems/quotas/#overview","text":"Directory Default space quota Default inode quota Purge time $HOME 500 GB 1 M - $SCRATCH 10 TB 1 M 60 days /work/projects/... 1 TB 1 M - /mnt/isilon/projects/... 1.14 PB globally - -","title":"Overview"},{"location":"filesystems/quotas/#quotas","text":"Warning When a quota is reached writes to that directory will fail. Note On Isilon everyone shares one global quota. Unfortunately it is not possible to see the quota status on the cluster.","title":"Quotas"},{"location":"filesystems/quotas/#current-usage","text":"We provide the df-ulhpc command on the cluster login nodes, which displays current usage, soft quota, hard quota and grace period. Any directories that have exceeded the quota will be highlighted in red. Once you reach the soft quota you can still write data until the grace period expires (7 days) or you reach the hard quota. After you reach the end of the grace period or the hard quota, you have to reduce your usage to below the soft quota to be able to write data again. Check current space quota status: df-ulhpc Check current inode quota status: df-ulhpc -i Check free space on all file systems: df -h Check free space on current file system: df -h .","title":"Current usage"},{"location":"filesystems/quotas/#increases","text":"If you or your project needs additional space or inodes for your scratch or project directory you may request it via ServiceNow (HPC \u2192 Storage & projects \u2192 Extend quota). Quotas on the home directory cannot be increased.","title":"Increases"},{"location":"filesystems/quotas/#troubleshooting","text":"The quotas on project directories are based on the group. Be aware that the quota for the default user group clusterusers is 0. If you get a quota error, but df-ulhpc and df-ulhpc -i confirm that the quota is not expired, you are most likely trying to write a file with the group clusterusers instead of the project group. To avoid this issue, check out the newgrp command or set the s mode bit (\"set group ID\") on the directory with chmod g+s <directory> . The s bit means that any file or folder created below will inherit the group. To transfer data with rsync into a project directory, please check the data transfer documentation .","title":"Troubleshooting"},{"location":"filesystems/quotas/#purging","text":"Files in the scratch ( $SCRATCH , /mnt/lscratch ) directories are purged. This means files that have not been read (i.e. atime has not been updated) within the last 60 days are automatically removed every month . Files in /tmp on the compute nodes are removed at the end of the job.","title":"Purging"},{"location":"filesystems/scratch/","text":"Global Scratch directory $SCRATCH \u00b6 The scratch area is a Lustre -based file system designed for high performance temporary storage of large files. It is thus intended to support large I/O for jobs that are being actively computed on the ULHPC systems. We recommend that you run your jobs, especially data intensive ones, from the ULHPC scratch file system. Refer to your scratch directory using the environment variable $SCRATCH whenever possible (which expands to /scratch/users/$(whoami) ). The scratch file system is shared via the Infiniband network of the ULHPC facility and is available from all nodes while being tuned for high performance.","title":"Scratch"},{"location":"filesystems/scratch/#global-scratch-directory-scratch","text":"The scratch area is a Lustre -based file system designed for high performance temporary storage of large files. It is thus intended to support large I/O for jobs that are being actively computed on the ULHPC systems. We recommend that you run your jobs, especially data intensive ones, from the ULHPC scratch file system. Refer to your scratch directory using the environment variable $SCRATCH whenever possible (which expands to /scratch/users/$(whoami) ). The scratch file system is shared via the Infiniband network of the ULHPC facility and is available from all nodes while being tuned for high performance.","title":"Global Scratch directory $SCRATCH"},{"location":"filesystems/unix-file-permissions/","text":"Unix File Permissions \u00b6 Brief Overview \u00b6 Every file (and directory) has an owner, an associated Unix group, and a set of permission flags that specify separate read, write, and execute permissions for the \"user\" (owner), \"group\", and \"other\". Group permissions apply to all users who belong to the group associated with the file. \"Other\" is also sometimes known as \"world\" permissions, and applies to all users who can login to the system. The command ls -l displays the permissions and associated group for any file. Here is an example of the output of this command: drwx------ 2 elvis elvis 2048 Jun 12 2012 private -rw------- 2 elvis elvis 1327 Apr 9 2012 try.f90 -rwx------ 2 elvis elvis 12040 Apr 9 2012 a.out drwxr-x--- 2 elvis bigsci 2048 Oct 17 2011 share drwxr-xr-x 3 elvis bigsci 2048 Nov 13 2011 public From left to right, the fields above represent: set of ten permission flags link count (irrelevant to this topic) owner associated group size date of last modification name of file The permission flags from left to right are: Position Meaning 1 \"d\" if a directory, \"-\" if a normal file 2, 3, 4 read, write, execute permission for user (owner) of file 5, 6, 7 read, write, execute permission for group 8, 9, 10 read, write, execute permission for other (world) and have the following meanings: Value Meaning - Flag is not set. r File is readable. w File is writable. For directories, files may be created or removed. x File is executable. For directories, files may be listed. s Set group ID (sgid). For directories, files created therein will be associated with the same group as the directory, rather than default group of the user. Subdirectories created therein will not only have the same group, but will also inherit the sgid setting. These definitions can be used to interpret the example output of ls -l presented above: drwx------ 2 elvis elvis 2048 Jun 12 2012 private This is a directory named \"private\", owned by user elvis and associated with Unix group elvis. The directory has read, write, and execute permissions for the owner, and no permissions for any other user. -rw------- 2 elvis elvis 1327 Apr 9 2012 try.f90 This is a normal file named \"try.f90\", owned by user elvis and associated with group elvis. It is readable and writable by the owner, but is not accessible to any other user. -rwx------ 2 elvis elvis 12040 Apr 9 2012 a.out This is a normal file named \"a.out\", owned by user elvis and associated with group elvis. It is executable, as well as readable and writable, for the owner only. drwxr-x--- 2 elvis bigsci 2048 Oct 17 2011 share This is a directory named \"share\", owned by user elvis and associated with group bigsci. The owner can read and write the directory; all members of the file group bigsci can list the contents of the directory. Presumably, this directory would contain files that also have \"group read\" permissions. drwxr-xr-x 3 elvis bigsci 2048 Nov 13 2011 public This is a directory named \"public\", owned by user elvis and associated with group bigsci. The owner can read and write the directory; all other users can only read the contents of the directory. A directory such as this would most likely contain files that have \"world read\" permissions. Useful File Permission Commands \u00b6 umask \u00b6 When a file is created, the permission flags are set according to the file mode creation mask, which can be set using the umask command. The file mode creation mask (sometimes referred to as \"the umask\") is a three-digit octal value whose nine bits correspond to fields 2-10 of the permission flags. The resulting permissions are calculated via the bitwise AND of the unary complement of the argument (using bitwise NOT) and the default permissions specified by the shell (typically 666 for files and 777 for directories). Common useful values are: umask value File Permissions Directory Permissions 002 -rw-rw-r-- drwxrwxr-x 007 -rw-rw---- drwxrwx--- 022 -rw-r--r-- drwxr-xr-x 027 -rw-r----- drwxr-x--- 077 -rw------- drwx------ Note that at ULHPC, the default umask is left unchanged (022), yet it can be redefined in your ~/.bash_profile configuration file if needed. chmod \u00b6 The chmod (\"change mode\") command is used to change the permission flags on existing files. It can be applied recursively using the \"-R\" option. It can be invoked with either octal values representing the permission flags, or with symbolic representations of the flags. The octal values have the following meaning: Octal Digit Binary Representation ( rwx ) Permission 0 000 none 1 001 execute only 2 010 write only 3 011 write and execute 4 100 read only 5 101 read and execute 6 110 read and write 7 111 read, write, and execute (full permissions) Here is an example of chmod using octal values: $ umask 0022 $ touch foo $ ls -l foo -rw-r--r--. 1 elvis elvis 0 Nov 19 14:49 foo $ chmod 755 foo $ ls -l foo -rwxr-xr-x. 1 elvis elvis 0 Nov 19 14:49 foo In the above example, the umask for user elvis results in a file that is read-write for the user, and read for group and other. The chmod command specifies read-write-execute permissions for the user, and read-execute permissions for group and other. Here is the format of the chmod command when using symbolic values: chmod [-R] [classes][operator][modes] file ... The classes determine to which combination of user/group/other the operation will apply, the operator specifies whether permissions are being added or removed, and the modes specify the permissions to be added or removed. Classes are formed by combining one or more of the following letters: Letter Class Description u user Owner of the file g group Users who are members of the file's group o other Users who are not the owner of the file or members of the file's group a all All of the above (equivalent to ugo ) The following operators are supported: Operator Description + Add the specified modes to the specified classes. - Remove the specified modes from the specified classes. = The specified modes are made the exact modes for the specified classes. The modes specify which permissions are to be added to or removed from the specified classes. There are three primary values which correspond to the basic permissions, and two less frequently-used values that are useful in specific circumstances: Mode Name Description r read Read a file or list a directory's contents. w write Write to a file or directory. x execute Execute a file or traverse a directory. X \"special\" execute This is a slightly more restrictive version of \"x\". It applies execute permissions to directories in all cases, and to files only if at least one execute permission bit is already set. It is typically used with the \"+\" operator and the \"-R\" option, to give group and/or other access to a large directory tree, without setting execute permissions on normal (non-executable) files (e.g., text files). For example, chmod -R go+rx bigdir would set read and execute permissions on every file (including text files) and directory in the bigdir directory, recursively, for group and other. The command chmod -R go+rX bigdir would set read and execute permissions on every directory, and would set group and other read and execute permissions on files that were already executable by the owner. s setgid or sgid This setting is typically applied to directories. If set, any file created in that directory will be associated with the directory's group, rather than with the default file group of the owner. This is useful in setting up directories where many users share access. This setting is sometimes referred to as the \"sticky bit\", although that phrase has a historical meaning unrelated to this context. Sets of class/operator/mode may separated by commas. Using the above definitions, the previous (octal notation) example can be done symbolically: $ umask 0022 $ touch foo $ ls -l foo -rw-r--r--. 1 elvis elvis 0 Nov 19 14:49 foo $ chmod u+x,go+rx foo $ ls -l foo -rwxr-xr-x. 1 elvis elvis 0 Nov 19 14:49 foo Unix File Groups \u00b6 Unix file groups provide a means to control access to shared data on disk and tape. Overview of Unix Groups \u00b6 Every user on a Unix system is a member of one or more Unix groups, including their primary or default group. Every file (or directory) on the system has an owner and an associated group. When a user creates a file, the file's associated group will be the user's default group. The user (owner) has the ability to change the associated group to any of the groups to which the user belongs. Unix groups can be defined that allow users to share data with other users who belong to the same group. Unix Groups at ULHPC \u00b6 All user's default group is clusterusers . Users usually belong to several other groups, including groups associated with specific research projects. Groups are used to shared file between project members, and can be created on request. See the page about Project Data Management for more information. Useful Unix Group Commands \u00b6 Command Description groups username List group membership id username List group membership with group ids ls -l List group associated with file or directory chgrp Change group associated with file or directory newgrp Create new shell with different default group sg Execute command with different default group","title":"Unix File Permissions"},{"location":"filesystems/unix-file-permissions/#unix-file-permissions","text":"","title":"Unix File Permissions"},{"location":"filesystems/unix-file-permissions/#brief-overview","text":"Every file (and directory) has an owner, an associated Unix group, and a set of permission flags that specify separate read, write, and execute permissions for the \"user\" (owner), \"group\", and \"other\". Group permissions apply to all users who belong to the group associated with the file. \"Other\" is also sometimes known as \"world\" permissions, and applies to all users who can login to the system. The command ls -l displays the permissions and associated group for any file. Here is an example of the output of this command: drwx------ 2 elvis elvis 2048 Jun 12 2012 private -rw------- 2 elvis elvis 1327 Apr 9 2012 try.f90 -rwx------ 2 elvis elvis 12040 Apr 9 2012 a.out drwxr-x--- 2 elvis bigsci 2048 Oct 17 2011 share drwxr-xr-x 3 elvis bigsci 2048 Nov 13 2011 public From left to right, the fields above represent: set of ten permission flags link count (irrelevant to this topic) owner associated group size date of last modification name of file The permission flags from left to right are: Position Meaning 1 \"d\" if a directory, \"-\" if a normal file 2, 3, 4 read, write, execute permission for user (owner) of file 5, 6, 7 read, write, execute permission for group 8, 9, 10 read, write, execute permission for other (world) and have the following meanings: Value Meaning - Flag is not set. r File is readable. w File is writable. For directories, files may be created or removed. x File is executable. For directories, files may be listed. s Set group ID (sgid). For directories, files created therein will be associated with the same group as the directory, rather than default group of the user. Subdirectories created therein will not only have the same group, but will also inherit the sgid setting. These definitions can be used to interpret the example output of ls -l presented above: drwx------ 2 elvis elvis 2048 Jun 12 2012 private This is a directory named \"private\", owned by user elvis and associated with Unix group elvis. The directory has read, write, and execute permissions for the owner, and no permissions for any other user. -rw------- 2 elvis elvis 1327 Apr 9 2012 try.f90 This is a normal file named \"try.f90\", owned by user elvis and associated with group elvis. It is readable and writable by the owner, but is not accessible to any other user. -rwx------ 2 elvis elvis 12040 Apr 9 2012 a.out This is a normal file named \"a.out\", owned by user elvis and associated with group elvis. It is executable, as well as readable and writable, for the owner only. drwxr-x--- 2 elvis bigsci 2048 Oct 17 2011 share This is a directory named \"share\", owned by user elvis and associated with group bigsci. The owner can read and write the directory; all members of the file group bigsci can list the contents of the directory. Presumably, this directory would contain files that also have \"group read\" permissions. drwxr-xr-x 3 elvis bigsci 2048 Nov 13 2011 public This is a directory named \"public\", owned by user elvis and associated with group bigsci. The owner can read and write the directory; all other users can only read the contents of the directory. A directory such as this would most likely contain files that have \"world read\" permissions.","title":"Brief Overview"},{"location":"filesystems/unix-file-permissions/#useful-file-permission-commands","text":"","title":"Useful File Permission Commands"},{"location":"filesystems/unix-file-permissions/#umask","text":"When a file is created, the permission flags are set according to the file mode creation mask, which can be set using the umask command. The file mode creation mask (sometimes referred to as \"the umask\") is a three-digit octal value whose nine bits correspond to fields 2-10 of the permission flags. The resulting permissions are calculated via the bitwise AND of the unary complement of the argument (using bitwise NOT) and the default permissions specified by the shell (typically 666 for files and 777 for directories). Common useful values are: umask value File Permissions Directory Permissions 002 -rw-rw-r-- drwxrwxr-x 007 -rw-rw---- drwxrwx--- 022 -rw-r--r-- drwxr-xr-x 027 -rw-r----- drwxr-x--- 077 -rw------- drwx------ Note that at ULHPC, the default umask is left unchanged (022), yet it can be redefined in your ~/.bash_profile configuration file if needed.","title":"umask"},{"location":"filesystems/unix-file-permissions/#chmod","text":"The chmod (\"change mode\") command is used to change the permission flags on existing files. It can be applied recursively using the \"-R\" option. It can be invoked with either octal values representing the permission flags, or with symbolic representations of the flags. The octal values have the following meaning: Octal Digit Binary Representation ( rwx ) Permission 0 000 none 1 001 execute only 2 010 write only 3 011 write and execute 4 100 read only 5 101 read and execute 6 110 read and write 7 111 read, write, and execute (full permissions) Here is an example of chmod using octal values: $ umask 0022 $ touch foo $ ls -l foo -rw-r--r--. 1 elvis elvis 0 Nov 19 14:49 foo $ chmod 755 foo $ ls -l foo -rwxr-xr-x. 1 elvis elvis 0 Nov 19 14:49 foo In the above example, the umask for user elvis results in a file that is read-write for the user, and read for group and other. The chmod command specifies read-write-execute permissions for the user, and read-execute permissions for group and other. Here is the format of the chmod command when using symbolic values: chmod [-R] [classes][operator][modes] file ... The classes determine to which combination of user/group/other the operation will apply, the operator specifies whether permissions are being added or removed, and the modes specify the permissions to be added or removed. Classes are formed by combining one or more of the following letters: Letter Class Description u user Owner of the file g group Users who are members of the file's group o other Users who are not the owner of the file or members of the file's group a all All of the above (equivalent to ugo ) The following operators are supported: Operator Description + Add the specified modes to the specified classes. - Remove the specified modes from the specified classes. = The specified modes are made the exact modes for the specified classes. The modes specify which permissions are to be added to or removed from the specified classes. There are three primary values which correspond to the basic permissions, and two less frequently-used values that are useful in specific circumstances: Mode Name Description r read Read a file or list a directory's contents. w write Write to a file or directory. x execute Execute a file or traverse a directory. X \"special\" execute This is a slightly more restrictive version of \"x\". It applies execute permissions to directories in all cases, and to files only if at least one execute permission bit is already set. It is typically used with the \"+\" operator and the \"-R\" option, to give group and/or other access to a large directory tree, without setting execute permissions on normal (non-executable) files (e.g., text files). For example, chmod -R go+rx bigdir would set read and execute permissions on every file (including text files) and directory in the bigdir directory, recursively, for group and other. The command chmod -R go+rX bigdir would set read and execute permissions on every directory, and would set group and other read and execute permissions on files that were already executable by the owner. s setgid or sgid This setting is typically applied to directories. If set, any file created in that directory will be associated with the directory's group, rather than with the default file group of the owner. This is useful in setting up directories where many users share access. This setting is sometimes referred to as the \"sticky bit\", although that phrase has a historical meaning unrelated to this context. Sets of class/operator/mode may separated by commas. Using the above definitions, the previous (octal notation) example can be done symbolically: $ umask 0022 $ touch foo $ ls -l foo -rw-r--r--. 1 elvis elvis 0 Nov 19 14:49 foo $ chmod u+x,go+rx foo $ ls -l foo -rwxr-xr-x. 1 elvis elvis 0 Nov 19 14:49 foo","title":"chmod"},{"location":"filesystems/unix-file-permissions/#unix-file-groups","text":"Unix file groups provide a means to control access to shared data on disk and tape.","title":"Unix File Groups"},{"location":"filesystems/unix-file-permissions/#overview-of-unix-groups","text":"Every user on a Unix system is a member of one or more Unix groups, including their primary or default group. Every file (or directory) on the system has an owner and an associated group. When a user creates a file, the file's associated group will be the user's default group. The user (owner) has the ability to change the associated group to any of the groups to which the user belongs. Unix groups can be defined that allow users to share data with other users who belong to the same group.","title":"Overview of Unix Groups"},{"location":"filesystems/unix-file-permissions/#unix-groups-at-ulhpc","text":"All user's default group is clusterusers . Users usually belong to several other groups, including groups associated with specific research projects. Groups are used to shared file between project members, and can be created on request. See the page about Project Data Management for more information.","title":"Unix Groups at ULHPC"},{"location":"filesystems/unix-file-permissions/#useful-unix-group-commands","text":"Command Description groups username List group membership id username List group membership with group ids ls -l List group associated with file or directory chgrp Change group associated with file or directory newgrp Create new shell with different default group sg Execute command with different default group","title":"Useful Unix Group Commands"},{"location":"help/","text":"Support \u00b6 ULHPC strives to support in a user friendly way your [super]computing needs. Note however that we are not here to make your PhD at your place ;) Service Now HPC Support Portal FAQ/Troubleshooting \u00b6 Password reset Connection issues File Permissions Access rights to project directory Quotas and Purging Read the Friendly Manual \u00b6 We have always maintained an extensive documentation and tutorials available online, which aims at being the most up-to-date and comprehensive. So please, read the documentation first if you have a question of problem -- we probably provide detailed instructions here Help Desk \u00b6 The online help desk Service is the preferred method for contacting ULHPC. Tips Before reporting a problem or and issue, kindly remember that: Your issue is probably documented here on the ULHPC Technical documentation An event may be on-going: check the ULHPC Live status page Planned maintenance are announced at least 2 weeks in advance - -- see Maintenance and Downtime Policy The proper SSH banner is displayed during planned downtime check the state of your nodes and jobs Joining/monitoring running jobs Monitoring post-mortem Job status and efficiency Service Now HPC Support Portal You can make code snippets, shell outputs, etc in your ticket much more readable by inserting a line with: [code]<pre> before the snippet, and another line with: </pre>[/code] after it. For a full list of formatting options, see this ServiceNow article . Be as precise and complete as possible ULHPC team handle thousands of support requests per year. In order to ensure efficient timely resolution of issues, ensure that: you select the appropriate category (left menu) you include as much of the following as possible when making a request: Who? - Name and user id (login), eventually project name When? - When did the problem occur? Where? - Which cluster ? Which node ? Which job ? Really include Job IDs Location of relevant files input/output, job launcher scripts, source code, executables etc. What? - What happened? What exactly were you doing or trying to do ? include Error messages - kindly report system or software messages literally and exactly . output of module list any steps you have tried Steps to reproduce Any part of this technical documentation you checked before opening the ticket Access to the online help system requires logging in with your Uni.lu username, password, and eventually one-time password. If you are an existing user unable to log in, you can send us an email . Availability and Response Time HPC support is provided on a volunteer basis by UL HPC staff and associated UL experts working at normal business hours. We offer no guarantee on response time except with paid support contracts. Email support \u00b6 You can contact us by mail to the ULHPC Team Email ( ONLY if you cannot login/access the HPC Support helpdesk portal : hpc-team@uni.lu You may also ask the help of other ULHPC users using the HPC User community mailing list: (moderated): hpc-users@uni.lu","title":"Support"},{"location":"help/#support","text":"ULHPC strives to support in a user friendly way your [super]computing needs. Note however that we are not here to make your PhD at your place ;) Service Now HPC Support Portal","title":"Support"},{"location":"help/#faqtroubleshooting","text":"Password reset Connection issues File Permissions Access rights to project directory Quotas and Purging","title":"FAQ/Troubleshooting"},{"location":"help/#read-the-friendly-manual","text":"We have always maintained an extensive documentation and tutorials available online, which aims at being the most up-to-date and comprehensive. So please, read the documentation first if you have a question of problem -- we probably provide detailed instructions here","title":"Read the Friendly Manual"},{"location":"help/#help-desk","text":"The online help desk Service is the preferred method for contacting ULHPC. Tips Before reporting a problem or and issue, kindly remember that: Your issue is probably documented here on the ULHPC Technical documentation An event may be on-going: check the ULHPC Live status page Planned maintenance are announced at least 2 weeks in advance - -- see Maintenance and Downtime Policy The proper SSH banner is displayed during planned downtime check the state of your nodes and jobs Joining/monitoring running jobs Monitoring post-mortem Job status and efficiency Service Now HPC Support Portal You can make code snippets, shell outputs, etc in your ticket much more readable by inserting a line with: [code]<pre> before the snippet, and another line with: </pre>[/code] after it. For a full list of formatting options, see this ServiceNow article . Be as precise and complete as possible ULHPC team handle thousands of support requests per year. In order to ensure efficient timely resolution of issues, ensure that: you select the appropriate category (left menu) you include as much of the following as possible when making a request: Who? - Name and user id (login), eventually project name When? - When did the problem occur? Where? - Which cluster ? Which node ? Which job ? Really include Job IDs Location of relevant files input/output, job launcher scripts, source code, executables etc. What? - What happened? What exactly were you doing or trying to do ? include Error messages - kindly report system or software messages literally and exactly . output of module list any steps you have tried Steps to reproduce Any part of this technical documentation you checked before opening the ticket Access to the online help system requires logging in with your Uni.lu username, password, and eventually one-time password. If you are an existing user unable to log in, you can send us an email . Availability and Response Time HPC support is provided on a volunteer basis by UL HPC staff and associated UL experts working at normal business hours. We offer no guarantee on response time except with paid support contracts.","title":"Help Desk"},{"location":"help/#email-support","text":"You can contact us by mail to the ULHPC Team Email ( ONLY if you cannot login/access the HPC Support helpdesk portal : hpc-team@uni.lu You may also ask the help of other ULHPC users using the HPC User community mailing list: (moderated): hpc-users@uni.lu","title":"Email support"},{"location":"help/professional-services/","text":"Consulting and Professional HPC Support Services \u00b6 Expert-level service and support can be provided by the University HPC staff in the form of pools of 4-hour specialized help, at 480\u20ac VAT excluded (equivalent to 120\u20ac per hour expert rate EA). Service and support activities include but are not limited to HPC training on facilities utilization, software installation, HPC workflow development. Such support service is typically handled by an HPC service agreement signed between the ULHPC and the company requesting consultant services. Note that such expert-level support is natively embedded on all HPC Resource allocation Service Contract for external and private partners . Contact us for more details.","title":"Consulting and Professional HPC Support Services"},{"location":"help/professional-services/#consulting-and-professional-hpc-support-services","text":"Expert-level service and support can be provided by the University HPC staff in the form of pools of 4-hour specialized help, at 480\u20ac VAT excluded (equivalent to 120\u20ac per hour expert rate EA). Service and support activities include but are not limited to HPC training on facilities utilization, software installation, HPC workflow development. Such support service is typically handled by an HPC service agreement signed between the ULHPC and the company requesting consultant services. Note that such expert-level support is natively embedded on all HPC Resource allocation Service Contract for external and private partners . Contact us for more details.","title":"Consulting and Professional HPC Support Services"},{"location":"interconnect/ethernet/","text":"Ethernet Network \u00b6 Having a single high-bandwidth and low-latency network as the local Fast IB interconnect network to support efficient HPC and Big data workloads would not provide the necessary flexibility brought by the Ethernet protocol. Especially applications that are not able to employ the native protocol foreseen for that network and thus forced to use an IP emulation layer will benefit from the flexibility of Ethernet-based networks. An additional, Ethernet-based network offers the robustness and resiliency needed for management tasks inside the system in such cases Outside the Fast IB interconnect network used inside the clusters, we maintain an Ethernet network organized as a 2-layer topology: one upper level ( Gateway Layer ) with routing, switching features, network isolation and filtering (ACL) rules and meant to interconnect only switches. This layer is handled by a redundant set of site routers (ULHPC gateway routers). it allows to interface the University network for both internal (LAN) and external (WAN) communications one bottom level ( Switching Layer ) composed by the [stacked] core switches as well as the TOR (Top-the-rack) switches, meant to interface the HPC servers and compute nodes. An overview of this topology is provided in the below figure. ACM PEARC'22 article If you are interested to get more details on the implemented Ethernet network, you can refer to the following article published to the ACM PEARC'22 conference (Practice and Experience in Advanced Research Computing) in Boston, USA on July 13, 2022. ACM Reference Format | ORBilu entry | OpenAccess | ULHPC blog post | slides Sebastien Varrette, Hyacinthe Cartiaux, Teddy Valette, and Abatcha Olloh. 2022. Aggregating and Consolidating two High Performant Network Topologies: The ULHPC Experience. In Practice and Experience in Advanced Research Computing (PEARC '22) . Association for Computing Machinery, New York, NY, USA, Article 61, 1\u20136. https://doi.org/10.1145/3491418.3535159","title":"Ethernet Interconnect"},{"location":"interconnect/ethernet/#ethernet-network","text":"Having a single high-bandwidth and low-latency network as the local Fast IB interconnect network to support efficient HPC and Big data workloads would not provide the necessary flexibility brought by the Ethernet protocol. Especially applications that are not able to employ the native protocol foreseen for that network and thus forced to use an IP emulation layer will benefit from the flexibility of Ethernet-based networks. An additional, Ethernet-based network offers the robustness and resiliency needed for management tasks inside the system in such cases Outside the Fast IB interconnect network used inside the clusters, we maintain an Ethernet network organized as a 2-layer topology: one upper level ( Gateway Layer ) with routing, switching features, network isolation and filtering (ACL) rules and meant to interconnect only switches. This layer is handled by a redundant set of site routers (ULHPC gateway routers). it allows to interface the University network for both internal (LAN) and external (WAN) communications one bottom level ( Switching Layer ) composed by the [stacked] core switches as well as the TOR (Top-the-rack) switches, meant to interface the HPC servers and compute nodes. An overview of this topology is provided in the below figure. ACM PEARC'22 article If you are interested to get more details on the implemented Ethernet network, you can refer to the following article published to the ACM PEARC'22 conference (Practice and Experience in Advanced Research Computing) in Boston, USA on July 13, 2022. ACM Reference Format | ORBilu entry | OpenAccess | ULHPC blog post | slides Sebastien Varrette, Hyacinthe Cartiaux, Teddy Valette, and Abatcha Olloh. 2022. Aggregating and Consolidating two High Performant Network Topologies: The ULHPC Experience. In Practice and Experience in Advanced Research Computing (PEARC '22) . Association for Computing Machinery, New York, NY, USA, Article 61, 1\u20136. https://doi.org/10.1145/3491418.3535159","title":"Ethernet Network"},{"location":"interconnect/ib/","text":"Fast Local Interconnect Network \u00b6 High Performance Computing (HPC) encompasses advanced computation over parallel processing, enabling faster execution of highly compute intensive tasks. The execution time of a given simulation depends upon many factors, such as the number of CPU/GPU cores and their utilisation factor and the interconnect performance, efficiency, and scalability. HPC interconnect technologies can be nowadays divided into three categories: Ethernet, InfiniBand, and vendor specific interconnects. While Ethernet is established as the dominant interconnect standard for mainstream commercial computing requirements, the underlying protocol has inherent limitations preventing low-latency deployments expected in real HPC environment. When in need of high-bandwidth and low-latency as required in efficient high performance computing systems, better options have emerged and are considered: InfiniBand technologies 1 . See Introduction to High-Speed InfiniBand Interconnect . Vendor specific interconnects, which currently correspond to the technology provided by three main HPC vendors: Cray/HPC Slingshot , Intel's EOL Omni-Path Architecture (OPA) or, to a minor measure, Bull BXI . Within the ULHPC facility, the InfiniBand solution was preferred as the predominant interconnect technology in the HPC market, tested against the largest set of HPC workloads. In practice: Iris relies on a EDR Infiniband (IB) Fabric in a Fat-Tree Topology Aion relies on a HDR100 Infiniband (IB) Fabric in a Fat-Tree Topology ACM PEARC'22 article If you are interested to understand the architecture and the solutions designed upon Aion acquisition to expand and consolidate the previously existing IB networks beyond its seminal capacity limits (while keeping at best their Bisection bandwidth), you can refer to the following article published to the ACM PEARC'22 conference (Practice and Experience in Advanced Research Computing) in Boston, USA on July 13, 2022. ACM Reference Format | ORBilu entry | OpenAccess | ULHPC blog post | slides Sebastien Varrette, Hyacinthe Cartiaux, Teddy Valette, and Abatcha Olloh. 2022. Aggregating and Consolidating two High Performant Network Topologies: The ULHPC Experience. In Practice and Experience in Advanced Research Computing (PEARC '22) . Association for Computing Machinery, New York, NY, USA, Article 61, 1\u20136. https://doi.org/10.1145/3491418.3535159 ULHPC IB Topology \u00b6 One of the most significant differentiators between HPC systems and lesser performing systems is, apart from the interconnect technology deployed, the supporting topology. There are several topologies commonly used in large-scale HPC deployments ( Fat-Tree , 3D-Torus , Dragonfly+ etc.). Fat-tree remains the widely used topology in HPC clusters due to its versatility, high bisection bandwidth and well understood routing. For this reason, each production clusters of the ULHPC facility rely on Fat-Tree topology. To minimize the number of switches per nodes while keeping a good Bisection bandwidth and allowing to interconnect Iris and Aion IB networks, the following configuration has been implemented: For more details: Iris IB Interconnect Aion IB Interconnect The tight integration of I/O and compute in the ULHPC supercomputer architecture gives a very robust, time critical production systems. The selected routing algorithms also provides a dedicated and fast path to the IO targets, avoiding congestion on the high-speed network and mitigating the risk of runtime \"jitter\" for time critical jobs. IB Fabric Diagnostic Utilities \u00b6 An InfiniBand fabric is composed of switches and channel adapter (HCA/Connect-X cards) devices. To identify devices in a fabric (or even in one switch system), each device is given a GUID (a MAC address equivalent). Since a GUID is a non-user-friendly string of characters, we alias it to a meaningful, user-given name. There are a few IB diagnostic tools (typically installed by the infiniband-diags package) using these names. The ULHPC team is using them to diagnose Infiniband Fabric Information 2 -- see also InfiniBand Guide by Atos/Bull (PDF) Tools Description ibnodes Show Infiniband nodes in topology ibhosts Show InfiniBand host nodes in topology ibswitches Show InfiniBand switch nodes in topology ibnetdiscover Discover InfiniBand topology ibdiag Scans the fabric using directed route packets and extracts all the available information (connectivity, devices) perfquery find errors on a particular or number of HCA\u2019s and switch ports sminfo Get InfiniBand Subnet Manager Info Mellanox Equipment FW Update \u00b6 An InfiniBand fabric is composed of switches and channel adapter (HCA/Connect-X cards) devices. Both should be kept up-to-date to mitigate potential security issues. Mellanox ConnectX HCA cards \u00b6 The Mellanox HCA firmware updater tool: mlxup , can be downloaded from mellanox.com . A Typical workflow applied within the ULHPC team to update the firmware of the Connect-X cards : Query specific device or all devices (if no device were supplied) mlxup --query Go to https://support.mellanox.com/s/downloads-center then click on Adapter > ConnectX-<N> > All downloads (select any OS, it will redirect you to the same page) Click on \"Firmware\" tab and enter the PSID number obtained from mlxup --query Download the latest firmware version wget http://content.mellanox.com/firmware/fw-ConnectX[...].bin.zip Unzip the downloaded file: unzip [...] Burn device with latest firmware mlxup -d <PCI-device-name> -i <unzipped-image-file>.bin Reboot Mellanox IB Switches \u00b6 Reference documentation You need to download from Mellanox Download Center BEWARE of the processor architecture (X86 vs. PPC) when selecting the images select the switch model and download the proposed images -- Pay attention to the download path Originated in 1999 to specifically address workload requirements that were not adequately addressed by Ethernet and designed for scalability, using a switched fabric network topology together with Remote Direct Memory Access (RDMA) to reduce CPU overhead. Although InfiniBand is backed by a standards organisation ( InfiniBand Trade Association with formal and open multi-vendor processes, the InfiniBand market is currently dominated by a single significant vendor Mellanox recently acquired by NVidia , which also dominates the non-Ethernet market segment across HPC deployments. \u21a9 Most require priviledged (root) right and thus are not available for ULHPC end users. \u21a9","title":"Fast Infiniband Interconnect"},{"location":"interconnect/ib/#fast-local-interconnect-network","text":"High Performance Computing (HPC) encompasses advanced computation over parallel processing, enabling faster execution of highly compute intensive tasks. The execution time of a given simulation depends upon many factors, such as the number of CPU/GPU cores and their utilisation factor and the interconnect performance, efficiency, and scalability. HPC interconnect technologies can be nowadays divided into three categories: Ethernet, InfiniBand, and vendor specific interconnects. While Ethernet is established as the dominant interconnect standard for mainstream commercial computing requirements, the underlying protocol has inherent limitations preventing low-latency deployments expected in real HPC environment. When in need of high-bandwidth and low-latency as required in efficient high performance computing systems, better options have emerged and are considered: InfiniBand technologies 1 . See Introduction to High-Speed InfiniBand Interconnect . Vendor specific interconnects, which currently correspond to the technology provided by three main HPC vendors: Cray/HPC Slingshot , Intel's EOL Omni-Path Architecture (OPA) or, to a minor measure, Bull BXI . Within the ULHPC facility, the InfiniBand solution was preferred as the predominant interconnect technology in the HPC market, tested against the largest set of HPC workloads. In practice: Iris relies on a EDR Infiniband (IB) Fabric in a Fat-Tree Topology Aion relies on a HDR100 Infiniband (IB) Fabric in a Fat-Tree Topology ACM PEARC'22 article If you are interested to understand the architecture and the solutions designed upon Aion acquisition to expand and consolidate the previously existing IB networks beyond its seminal capacity limits (while keeping at best their Bisection bandwidth), you can refer to the following article published to the ACM PEARC'22 conference (Practice and Experience in Advanced Research Computing) in Boston, USA on July 13, 2022. ACM Reference Format | ORBilu entry | OpenAccess | ULHPC blog post | slides Sebastien Varrette, Hyacinthe Cartiaux, Teddy Valette, and Abatcha Olloh. 2022. Aggregating and Consolidating two High Performant Network Topologies: The ULHPC Experience. In Practice and Experience in Advanced Research Computing (PEARC '22) . Association for Computing Machinery, New York, NY, USA, Article 61, 1\u20136. https://doi.org/10.1145/3491418.3535159","title":"Fast Local Interconnect Network"},{"location":"interconnect/ib/#ulhpc-ib-topology","text":"One of the most significant differentiators between HPC systems and lesser performing systems is, apart from the interconnect technology deployed, the supporting topology. There are several topologies commonly used in large-scale HPC deployments ( Fat-Tree , 3D-Torus , Dragonfly+ etc.). Fat-tree remains the widely used topology in HPC clusters due to its versatility, high bisection bandwidth and well understood routing. For this reason, each production clusters of the ULHPC facility rely on Fat-Tree topology. To minimize the number of switches per nodes while keeping a good Bisection bandwidth and allowing to interconnect Iris and Aion IB networks, the following configuration has been implemented: For more details: Iris IB Interconnect Aion IB Interconnect The tight integration of I/O and compute in the ULHPC supercomputer architecture gives a very robust, time critical production systems. The selected routing algorithms also provides a dedicated and fast path to the IO targets, avoiding congestion on the high-speed network and mitigating the risk of runtime \"jitter\" for time critical jobs.","title":"ULHPC IB Topology"},{"location":"interconnect/ib/#ib-fabric-diagnostic-utilities","text":"An InfiniBand fabric is composed of switches and channel adapter (HCA/Connect-X cards) devices. To identify devices in a fabric (or even in one switch system), each device is given a GUID (a MAC address equivalent). Since a GUID is a non-user-friendly string of characters, we alias it to a meaningful, user-given name. There are a few IB diagnostic tools (typically installed by the infiniband-diags package) using these names. The ULHPC team is using them to diagnose Infiniband Fabric Information 2 -- see also InfiniBand Guide by Atos/Bull (PDF) Tools Description ibnodes Show Infiniband nodes in topology ibhosts Show InfiniBand host nodes in topology ibswitches Show InfiniBand switch nodes in topology ibnetdiscover Discover InfiniBand topology ibdiag Scans the fabric using directed route packets and extracts all the available information (connectivity, devices) perfquery find errors on a particular or number of HCA\u2019s and switch ports sminfo Get InfiniBand Subnet Manager Info","title":"IB Fabric Diagnostic Utilities"},{"location":"interconnect/ib/#mellanox-equipment-fw-update","text":"An InfiniBand fabric is composed of switches and channel adapter (HCA/Connect-X cards) devices. Both should be kept up-to-date to mitigate potential security issues.","title":"Mellanox Equipment FW Update"},{"location":"interconnect/ib/#mellanox-connectx-hca-cards","text":"The Mellanox HCA firmware updater tool: mlxup , can be downloaded from mellanox.com . A Typical workflow applied within the ULHPC team to update the firmware of the Connect-X cards : Query specific device or all devices (if no device were supplied) mlxup --query Go to https://support.mellanox.com/s/downloads-center then click on Adapter > ConnectX-<N> > All downloads (select any OS, it will redirect you to the same page) Click on \"Firmware\" tab and enter the PSID number obtained from mlxup --query Download the latest firmware version wget http://content.mellanox.com/firmware/fw-ConnectX[...].bin.zip Unzip the downloaded file: unzip [...] Burn device with latest firmware mlxup -d <PCI-device-name> -i <unzipped-image-file>.bin Reboot","title":"Mellanox ConnectX HCA cards"},{"location":"interconnect/ib/#mellanox-ib-switches","text":"Reference documentation You need to download from Mellanox Download Center BEWARE of the processor architecture (X86 vs. PPC) when selecting the images select the switch model and download the proposed images -- Pay attention to the download path Originated in 1999 to specifically address workload requirements that were not adequately addressed by Ethernet and designed for scalability, using a switched fabric network topology together with Remote Direct Memory Access (RDMA) to reduce CPU overhead. Although InfiniBand is backed by a standards organisation ( InfiniBand Trade Association with formal and open multi-vendor processes, the InfiniBand market is currently dominated by a single significant vendor Mellanox recently acquired by NVidia , which also dominates the non-Ethernet market segment across HPC deployments. \u21a9 Most require priviledged (root) right and thus are not available for ULHPC end users. \u21a9","title":"Mellanox IB Switches"},{"location":"jobs/best-effort/","text":"Best-effort Jobs \u00b6 Node Type Slurm command regular sbatch [-A <project>] -p batch --qos besteffort [-C {broadwell,skylake}] [...] gpu sbatch [-A <project>] -p gpu --qos besteffort [-C volta[32]] -G 1 [...] bigmem sbatch [-A <project>] -p bigmem --qos besteffort [...] Best-effort (preemptible) jobs allow an efficient usage of the platform by filling available computing nodes until regular jobs are submitted. sbatch -p {batch | gpu | bigmem} --qos besteffort [...] What means job preemption? Job preemption is the the act of \"stopping\" one or more \"low-priority\" jobs to let a \"high-priority\" job run. Job preemption is implemented as a variation of Slurm's Gang Scheduling logic. When a non -best-effort job is allocated resources that are already allocated to one or more best-effort jobs, the preemptable job(s) (thus on QOS besteffort ) are preempted. On ULHPC facilities, the preempted job(s) can be requeued (if possible) or canceling them. **For jobs to be requeued, they MUST have the \" --requeue \" sbatch option set. The besteffort QOS have less constraints than the other QOS (for instance, you can submit more jobs etc. ) As a general rule users should ensure that they track successful completion of best-effort jobs (which may be interrupted by other jobs at any time) and use them in combination with mechanisms such as Checkpoint-Restart that allow applications to stop and resume safely.","title":"Best-effort Jobs"},{"location":"jobs/best-effort/#best-effort-jobs","text":"Node Type Slurm command regular sbatch [-A <project>] -p batch --qos besteffort [-C {broadwell,skylake}] [...] gpu sbatch [-A <project>] -p gpu --qos besteffort [-C volta[32]] -G 1 [...] bigmem sbatch [-A <project>] -p bigmem --qos besteffort [...] Best-effort (preemptible) jobs allow an efficient usage of the platform by filling available computing nodes until regular jobs are submitted. sbatch -p {batch | gpu | bigmem} --qos besteffort [...] What means job preemption? Job preemption is the the act of \"stopping\" one or more \"low-priority\" jobs to let a \"high-priority\" job run. Job preemption is implemented as a variation of Slurm's Gang Scheduling logic. When a non -best-effort job is allocated resources that are already allocated to one or more best-effort jobs, the preemptable job(s) (thus on QOS besteffort ) are preempted. On ULHPC facilities, the preempted job(s) can be requeued (if possible) or canceling them. **For jobs to be requeued, they MUST have the \" --requeue \" sbatch option set. The besteffort QOS have less constraints than the other QOS (for instance, you can submit more jobs etc. ) As a general rule users should ensure that they track successful completion of best-effort jobs (which may be interrupted by other jobs at any time) and use them in combination with mechanisms such as Checkpoint-Restart that allow applications to stop and resume safely.","title":"Best-effort Jobs"},{"location":"jobs/billing/","text":"Job Accounting and Billing \u00b6 Usage Charging Policy ULHPC Resource Allocation Policy (PDF) Billing rates \u00b6 Trackable RESources (TRES) Billing Weights \u00b6 The above policy is in practice implemented through the Slurm Trackable RESources (TRES) and remains an important factor for the Fairsharing score calculation. As explained in the ULHPC Usage Charging Policy , we set TRES for CPU, GPU, and Memory usage according to weights defined as follows: Weight Description \\alpha_{cpu} \\alpha_{cpu} Normalized relative performance of CPU processor core (ref.: skylake 73.6 GFlops/core) \\alpha_{mem} \\alpha_{mem} Inverse of the average available memory size per core \\alpha_{GPU} \\alpha_{GPU} Weight per GPU accelerator Each partition has its own weights (combined into TRESBillingWeight ) you can check with # /!\\ ADAPT <partition> accordingly scontrol show partition <partition>","title":"Job Accounting and Billing"},{"location":"jobs/billing/#job-accounting-and-billing","text":"Usage Charging Policy ULHPC Resource Allocation Policy (PDF)","title":"Job Accounting and Billing"},{"location":"jobs/billing/#billing-rates","text":"","title":"Billing rates"},{"location":"jobs/billing/#trackable-resources-tres-billing-weights","text":"The above policy is in practice implemented through the Slurm Trackable RESources (TRES) and remains an important factor for the Fairsharing score calculation. As explained in the ULHPC Usage Charging Policy , we set TRES for CPU, GPU, and Memory usage according to weights defined as follows: Weight Description \\alpha_{cpu} \\alpha_{cpu} Normalized relative performance of CPU processor core (ref.: skylake 73.6 GFlops/core) \\alpha_{mem} \\alpha_{mem} Inverse of the average available memory size per core \\alpha_{GPU} \\alpha_{GPU} Weight per GPU accelerator Each partition has its own weights (combined into TRESBillingWeight ) you can check with # /!\\ ADAPT <partition> accordingly scontrol show partition <partition>","title":"Trackable RESources (TRES) Billing Weights"},{"location":"jobs/gpu/","text":"ULHPC GPU Nodes \u00b6 Each GPU node provided as part of the gpu partition feature 4x Nvidia V100 SXM2 (with either 16G or 32G memory) interconnected by the NVLink 2.0 architecture NVlink was designed as an alternative solution to PCI Express with higher bandwidth and additional features (e.g., shared memory) specifically designed to be compatible with Nvidia's own GPU ISA for multi-GPU systems -- see wikichip article . Because of the hardware organization, you MUST follow the below recommendations: Do not run jobs on GPU nodes if you have no use of GPU accelerators! , i.e. if you are not using any of the software compiled against the {foss,intel}cuda toolchain. Avoid using more than 4 GPUs, ideally within the same node Dedicated \u00bc of the available CPU cores for the management of each GPU card reserved. Thus your typical GPU launcher would match the AI/DL launcher example: #!/bin/bash -l ### Request one GPU tasks for 4 hours - dedicate 1/4 of available cores for its management #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 7 #SBATCH -G 1 #SBATCH --time=04:00:00 #SBATCH -p gpu print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load numlib/cuDNN # Example with cuDNN [ ... ] You can quickly access a GPU node for interactive jobs using si-gpu .","title":"GPU Jobs"},{"location":"jobs/gpu/#ulhpc-gpu-nodes","text":"Each GPU node provided as part of the gpu partition feature 4x Nvidia V100 SXM2 (with either 16G or 32G memory) interconnected by the NVLink 2.0 architecture NVlink was designed as an alternative solution to PCI Express with higher bandwidth and additional features (e.g., shared memory) specifically designed to be compatible with Nvidia's own GPU ISA for multi-GPU systems -- see wikichip article . Because of the hardware organization, you MUST follow the below recommendations: Do not run jobs on GPU nodes if you have no use of GPU accelerators! , i.e. if you are not using any of the software compiled against the {foss,intel}cuda toolchain. Avoid using more than 4 GPUs, ideally within the same node Dedicated \u00bc of the available CPU cores for the management of each GPU card reserved. Thus your typical GPU launcher would match the AI/DL launcher example: #!/bin/bash -l ### Request one GPU tasks for 4 hours - dedicate 1/4 of available cores for its management #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 7 #SBATCH -G 1 #SBATCH --time=04:00:00 #SBATCH -p gpu print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load numlib/cuDNN # Example with cuDNN [ ... ] You can quickly access a GPU node for interactive jobs using si-gpu .","title":"ULHPC GPU Nodes"},{"location":"jobs/interactive/","text":"Interactive Jobs \u00b6 The interactive ( floating ) partition (exclusively associated to the debug QOS ) is to be used for code development, testing, and debugging . Important Production runs are not permitted in interactive jobs . User accounts are subject to suspension if they are determined to be using the interactive partition and the debug QOS for production computing. In particular, interactive job \"chaining\" is not allowed. Chaining is defined as using a batch script to submit another batch script. You can access the different node classes available using the -C <class> flag (see also List of Slurm features on ULHPC nodes ), or ( better ) through the custom helper functions defined for each category of nodes, i.e. si , si-gpu or si-bigmem : Regular Dual-CPU node ### Quick interative job for the default time $ si # salloc -p interactive --qos debug -C batch ### Explicitly ask for a skylake node $ si -C skylake # salloc -p interactive --qos debug -C batch -C skylake ### Use 1 full node for 28 tasks $ si --ntasks-per-node 28 # salloc -p interactive --qos debug -C batch --ntasks-per-node 28 ### interactive job for 2 hours $ si -t 02 :00:00 # salloc -p interactive --qos debug -C batch -t 02:00:00 ### interactive job on 2 nodes, 1 multithreaded tasks per node $ si -N 2 --ntasks-per-node 1 -c 4 si -N 2 --ntasks-per-node 1 -c 4 # salloc -p interactive --qos debug -C batch -N 2 --ntasks-per-node 1 -c 4 GPU node ### Quick interative job for the default time $ si-gpu # /!\\ WARNING: append -G 1 to really reserve a GPU # salloc -p interactive --qos debug -C gpu -G 1 ### (Better) Allocate 1/4 of available CPU cores per GPU to manage $ si-gpu -G 1 -c 7 $ si-gpu -G 2 -c 14 $ si-gpu -G 4 -c 28 Large-Memory node ### Quick interative job for the default time $ si-bigmem # salloc -p interactive --qos debug -C bigmem ### interactive job with 1 multithreaded task per socket available (4 in total) $ si-bigmem --ntasks-per-node 4 --ntasks-per-socket 1 -c 28 # salloc -p interactive --qos debug -C bigmem --ntasks-per-node 4 --ntasks-per-socket 1 -c 4 ### interactive job for 1 task but 512G of memory $ si-bigmem --mem 512G # salloc -p interactive --qos debug -C bigmem --mem 512G If you prefer to rely on the regular srun , the below table proposes the equivalent commands run by the helper scripts si* : Node Type Slurm command regular si [...] salloc -p interactive --qos debug -C batch [...] salloc -p interactive --qos debug -C batch,broadwell [...] salloc -p interactive --qos debug -C batch,skylake [...] gpu si-gpu [...] salloc -p interactive --qos debug -C gpu [-C volta[32]] -G 1 [...] bigmem si-bigmem [...] salloc -p interactive --qos debug -C bigmem [...] Impact of Interactive jobs implementation over a floating partition We have recently changed the way interactive jobs are served. Since the interactive partition is no longer dedicated but floating above the other partitions, there is NO guarantee to have an interactive job running if the surrounding partition ( batch , gpu or bigmem ) is full. However , the backfill scheduling in place together with the partition priority set ensure that interactive jobs will be first served upon resource release.","title":"Interactive Job"},{"location":"jobs/interactive/#interactive-jobs","text":"The interactive ( floating ) partition (exclusively associated to the debug QOS ) is to be used for code development, testing, and debugging . Important Production runs are not permitted in interactive jobs . User accounts are subject to suspension if they are determined to be using the interactive partition and the debug QOS for production computing. In particular, interactive job \"chaining\" is not allowed. Chaining is defined as using a batch script to submit another batch script. You can access the different node classes available using the -C <class> flag (see also List of Slurm features on ULHPC nodes ), or ( better ) through the custom helper functions defined for each category of nodes, i.e. si , si-gpu or si-bigmem : Regular Dual-CPU node ### Quick interative job for the default time $ si # salloc -p interactive --qos debug -C batch ### Explicitly ask for a skylake node $ si -C skylake # salloc -p interactive --qos debug -C batch -C skylake ### Use 1 full node for 28 tasks $ si --ntasks-per-node 28 # salloc -p interactive --qos debug -C batch --ntasks-per-node 28 ### interactive job for 2 hours $ si -t 02 :00:00 # salloc -p interactive --qos debug -C batch -t 02:00:00 ### interactive job on 2 nodes, 1 multithreaded tasks per node $ si -N 2 --ntasks-per-node 1 -c 4 si -N 2 --ntasks-per-node 1 -c 4 # salloc -p interactive --qos debug -C batch -N 2 --ntasks-per-node 1 -c 4 GPU node ### Quick interative job for the default time $ si-gpu # /!\\ WARNING: append -G 1 to really reserve a GPU # salloc -p interactive --qos debug -C gpu -G 1 ### (Better) Allocate 1/4 of available CPU cores per GPU to manage $ si-gpu -G 1 -c 7 $ si-gpu -G 2 -c 14 $ si-gpu -G 4 -c 28 Large-Memory node ### Quick interative job for the default time $ si-bigmem # salloc -p interactive --qos debug -C bigmem ### interactive job with 1 multithreaded task per socket available (4 in total) $ si-bigmem --ntasks-per-node 4 --ntasks-per-socket 1 -c 28 # salloc -p interactive --qos debug -C bigmem --ntasks-per-node 4 --ntasks-per-socket 1 -c 4 ### interactive job for 1 task but 512G of memory $ si-bigmem --mem 512G # salloc -p interactive --qos debug -C bigmem --mem 512G If you prefer to rely on the regular srun , the below table proposes the equivalent commands run by the helper scripts si* : Node Type Slurm command regular si [...] salloc -p interactive --qos debug -C batch [...] salloc -p interactive --qos debug -C batch,broadwell [...] salloc -p interactive --qos debug -C batch,skylake [...] gpu si-gpu [...] salloc -p interactive --qos debug -C gpu [-C volta[32]] -G 1 [...] bigmem si-bigmem [...] salloc -p interactive --qos debug -C bigmem [...] Impact of Interactive jobs implementation over a floating partition We have recently changed the way interactive jobs are served. Since the interactive partition is no longer dedicated but floating above the other partitions, there is NO guarantee to have an interactive job running if the surrounding partition ( batch , gpu or bigmem ) is full. However , the backfill scheduling in place together with the partition priority set ensure that interactive jobs will be first served upon resource release.","title":"Interactive Jobs"},{"location":"jobs/long/","text":"Long Jobs \u00b6 If you are confident that your jobs will last more than 2 days while efficiently using the allocated resources , you can use --qos long QOS. sbatch -p {batch | gpu | bigmem} --qos long [...] Following EuroHPC/PRACE Recommendations, the long QOS allow for an extended Max walltime ( MaxWall ) set to 14 days . Node Type Slurm command regular sbatch [-A <project>] -p batch --qos long [-C {broadwell,skylake}] [...] gpu sbatch [-A <project>] -p gpu --qos long [-C volta[32]] -G 1 [...] bigmem sbatch [-A <project>] -p bigmem --qos long [...] Important Be aware however that special restrictions applies for this kind of jobs. There is a limit to the maximum number of concurrent nodes involved in long jobs (see sqos for details). No more than 4 long jobs per User ( MaxJobsPU ) are allowed, using no more than 2 nodes per jobs.","title":"Long Jobs"},{"location":"jobs/long/#long-jobs","text":"If you are confident that your jobs will last more than 2 days while efficiently using the allocated resources , you can use --qos long QOS. sbatch -p {batch | gpu | bigmem} --qos long [...] Following EuroHPC/PRACE Recommendations, the long QOS allow for an extended Max walltime ( MaxWall ) set to 14 days . Node Type Slurm command regular sbatch [-A <project>] -p batch --qos long [-C {broadwell,skylake}] [...] gpu sbatch [-A <project>] -p gpu --qos long [-C volta[32]] -G 1 [...] bigmem sbatch [-A <project>] -p bigmem --qos long [...] Important Be aware however that special restrictions applies for this kind of jobs. There is a limit to the maximum number of concurrent nodes involved in long jobs (see sqos for details). No more than 4 long jobs per User ( MaxJobsPU ) are allowed, using no more than 2 nodes per jobs.","title":"Long Jobs"},{"location":"jobs/priority/","text":"ULHPC Job Prioritization Factors \u00b6 The ULHPC Slurm configuration rely on the Multifactor Priority Plugin and the Fair tree algorithm to preform Fairsharing among the users 1 Priority Factors \u00b6 There are several factors enabled on ULHPC supercomputers that influence job priority: Age : length of time a job has been waiting (PD state) in the queue Fairshare : difference between the portion of the computing resource that has been promised and the amount of resources that has been consumed - see Fairsharing . Partition : factor associated with each node partition , for instance to privilege interactive over batch partitions QOS A factor associated with each Quality Of Service ( low \\longrightarrow \\longrightarrow urgent ) The job's priority at any given time will be a weighted sum of all the factors that have been enabled. Job priority can be expressed as: Job_priority = PriorityWeightAge * age_factor + PriorityWeightFairshare * fair-share_factor+ PriorityWeightPartition * partition_factor + PriorityWeightQOS * QOS_factor + - nice_factor All of the factors in this formula are floating point numbers that range from 0.0 to 1.0. The weights are unsigned, 32 bit integers, you can get with: $ sprio -w # OR, from slurm.conf $ scontrol show config | grep -i PriorityWeight You can use the sprio to view the factors that comprise a job's scheduling priority and were your (pending) jobs stands in the priority queue. sprio Utility usage Show current weights sprio -w List pending jobs, sorted by jobid sprio [ -n ] # OR: sp List pending jobs, sorted by priority sprio [-n] -S+Y sprio [-n] | sort -k 3 -n sprio [-n] -l | sort -k 4 -n Getting the priority given to a job can be done either with squeue : # /!\\ ADAPT <jobid> accordingly squeue -o %Q -j <jobid> Backfill Scheduling \u00b6 Backfill is a mechanism by which lower priority jobs can start earlier to fill the idle slots provided they are finished before the next high priority jobs is expected to start based on resource availability. If your job is sufficiently small, it can be backfilled and scheduled in the shadow of a larger, higher-priority job For more details, see official Slurm documentation All users from a higher priority account receive a higher fair share factor than all users from a lower priority account \u21a9","title":"Job Priority and Backfilling"},{"location":"jobs/priority/#ulhpc-job-prioritization-factors","text":"The ULHPC Slurm configuration rely on the Multifactor Priority Plugin and the Fair tree algorithm to preform Fairsharing among the users 1","title":"ULHPC Job Prioritization Factors"},{"location":"jobs/priority/#priority-factors","text":"There are several factors enabled on ULHPC supercomputers that influence job priority: Age : length of time a job has been waiting (PD state) in the queue Fairshare : difference between the portion of the computing resource that has been promised and the amount of resources that has been consumed - see Fairsharing . Partition : factor associated with each node partition , for instance to privilege interactive over batch partitions QOS A factor associated with each Quality Of Service ( low \\longrightarrow \\longrightarrow urgent ) The job's priority at any given time will be a weighted sum of all the factors that have been enabled. Job priority can be expressed as: Job_priority = PriorityWeightAge * age_factor + PriorityWeightFairshare * fair-share_factor+ PriorityWeightPartition * partition_factor + PriorityWeightQOS * QOS_factor + - nice_factor All of the factors in this formula are floating point numbers that range from 0.0 to 1.0. The weights are unsigned, 32 bit integers, you can get with: $ sprio -w # OR, from slurm.conf $ scontrol show config | grep -i PriorityWeight You can use the sprio to view the factors that comprise a job's scheduling priority and were your (pending) jobs stands in the priority queue. sprio Utility usage Show current weights sprio -w List pending jobs, sorted by jobid sprio [ -n ] # OR: sp List pending jobs, sorted by priority sprio [-n] -S+Y sprio [-n] | sort -k 3 -n sprio [-n] -l | sort -k 4 -n Getting the priority given to a job can be done either with squeue : # /!\\ ADAPT <jobid> accordingly squeue -o %Q -j <jobid>","title":"Priority Factors"},{"location":"jobs/priority/#backfill-scheduling","text":"Backfill is a mechanism by which lower priority jobs can start earlier to fill the idle slots provided they are finished before the next high priority jobs is expected to start based on resource availability. If your job is sufficiently small, it can be backfilled and scheduled in the shadow of a larger, higher-priority job For more details, see official Slurm documentation All users from a higher priority account receive a higher fair share factor than all users from a lower priority account \u21a9","title":"Backfill Scheduling"},{"location":"jobs/reason-codes/","text":"Job Status and Reason Codes \u00b6 The squeue command details a variety of information on an active job\u2019s status with state and reason codes. Job state codes describe a job\u2019s current state in queue (e.g. pending, completed). Job reason codes describe the reason why the job is in its current state. The following tables outline a variety of job state and reason codes you may encounter when using squeue to check on your jobs. Job State Codes \u00b6 Status Code Explaination CANCELLED CA The job was explicitly cancelled by the user or system administrator. COMPLETED CD The job has completed successfully. COMPLETING CG The job is finishing but some processes are still active. DEADLINE DL The job terminated on deadline FAILED F The job terminated with a non-zero exit code and failed to execute. NODE_FAIL NF The job terminated due to failure of one or more allocated nodes OUT_OF_MEMORY OOM The Job experienced an out of memory error. PENDING PD The job is waiting for resource allocation. It will eventually run. PREEMPTED PR The job was terminated because of preemption by another job. RUNNING R The job currently is allocated to a node and is running. SUSPENDED S A running job has been stopped with its cores released to other jobs. STOPPED ST A running job has been stopped with its cores retained. TIMEOUT TO Job terminated upon reaching its time limit. A full list of these Job State codes can be found in squeue documentation. or sacct documentation . Job Reason Codes \u00b6 Reason Code Explaination Priority One or more higher priority jobs is in queue for running. Your job will eventually run. Dependency This job is waiting for a dependent job to complete and will run afterwards. Resources The job is waiting for resources to become available and will eventually run. InvalidAccount The job\u2019s account is invalid. Cancel the job and rerun with correct account. InvaldQoS The job\u2019s QoS is invalid. Cancel the job and rerun with correct account. QOSGrpCpuLimit All CPUs assigned to your job\u2019s specified QoS are in use; job will run eventually. QOSGrpMaxJobsLimit Maximum number of jobs for your job\u2019s QoS have been met; job will run eventually. QOSGrpNodeLimit All nodes assigned to your job\u2019s specified QoS are in use; job will run eventually. PartitionCpuLimit All CPUs assigned to your job\u2019s specified partition are in use; job will run eventually. PartitionMaxJobsLimit Maximum number of jobs for your job\u2019s partition have been met; job will run eventually. PartitionNodeLimit All nodes assigned to your job\u2019s specified partition are in use; job will run eventually. AssociationCpuLimit All CPUs assigned to your job\u2019s specified association are in use; job will run eventually. AssociationMaxJobsLimit Maximum number of jobs for your job\u2019s association have been met; job will run eventually. AssociationNodeLimit All nodes assigned to your job\u2019s specified association are in use; job will run eventually. A full list of these Job Reason Codes can be found in Slurm\u2019s documentation. Running Job Statistics Metrics \u00b6 The sstat command allows users to easily pull up status information about their currently running jobs. This includes information about CPU usage , task information , node information , resident set size (RSS) , and virtual memory (VM) . We can invoke the sstat command as such: # /!\\ ADAPT <jobid> accordingly $ sstat --jobs = <jobid> By default, sstat will pull up significantly more information than what would be needed in the commands default output. To remedy this, we can use the --format flag to choose what we want in our output. A chart of some these variables are listed in the table below: Variable Description avecpu Average CPU time of all tasks in job. averss Average resident set size of all tasks. avevmsize Average virtual memory of all tasks in a job. jobid The id of the Job. maxrss Maximum number of bytes read by all tasks in the job. maxvsize Maximum number of bytes written by all tasks in the job. ntasks Number of tasks in a job. For an example, let's print out a job's average job id, cpu time, max rss, and number of tasks. We can do this by typing out the command: # /!\\ ADAPT <jobid> accordingly sstat --jobs = <jobid> --format = jobid,cputime,maxrss,ntasks A full list of variables that specify data handled by sstat can be found with the --helpformat flag or by visiting the slurm page on sstat . Past Job Statistics Metrics \u00b6 You can use the custom susage function in /etc/profile.d/slurm.sh to collect statistics information. $ susage -h Usage: susage [-m] [-Y] [-S YYYY-MM-DD] [-E YYYT-MM-DD] For a specific user (if accounting rights granted): susage [...] -u <user> For a specific account (if accounting rights granted): susage [...] -A <account> Display past job usage summary But by default, you should use the sacct command allows users to pull up status information about past jobs. This command is very similar to sstat , but is used on jobs that have been previously run on the system instead of currently running jobs. # /!\\ ADAPT <jobid> accordingly $ sacct [ -X ] --jobs = <jobid> [ --format = metric1,... ] # OR, for a user, eventually between a Start and End date $ sacct [ -X ] -u $USER [ -S YYYY-MM-DD ] [ -E YYYY-MM-DD ] [ --format = metric1,... ] # OR, for an account - ADAPT <account> accordingly $ sacct [ -X ] -A <account> [ --format = metric1,... ] Use -X to aggregate the statistics relevant to the job allocation itself, not taking job steps into consideration. The main metrics code you may be interested to review are listed below. Variable Description account Account the job ran under. avecpu Average CPU time of all tasks in job. averss Average resident set size of all tasks in the job. cputime Formatted (Elapsed time * CPU) count used by a job or step. elapsed Jobs elapsed time formated as DD-HH:MM:SS. exitcode The exit code returned by the job script or salloc. jobid The id of the Job. jobname The name of the Job. maxdiskread Maximum number of bytes read by all tasks in the job. maxdiskwrite Maximum number of bytes written by all tasks in the job. maxrss Maximum resident set size of all tasks in the job. ncpus Amount of allocated CPUs. nnodes The number of nodes used in a job. ntasks Number of tasks in a job. priority Slurm priority. qos Quality of service. reqcpu Required number of CPUs reqmem Required amount of memory for a job. reqtres Required Trackable RESources (TRES) user Userna A full list of variables that specify data handled by sacct can be found with the --helpformat flag or by visiting the slurm page on sacct .","title":"Job State and Reason Code"},{"location":"jobs/reason-codes/#job-status-and-reason-codes","text":"The squeue command details a variety of information on an active job\u2019s status with state and reason codes. Job state codes describe a job\u2019s current state in queue (e.g. pending, completed). Job reason codes describe the reason why the job is in its current state. The following tables outline a variety of job state and reason codes you may encounter when using squeue to check on your jobs.","title":"Job Status and Reason Codes"},{"location":"jobs/reason-codes/#job-state-codes","text":"Status Code Explaination CANCELLED CA The job was explicitly cancelled by the user or system administrator. COMPLETED CD The job has completed successfully. COMPLETING CG The job is finishing but some processes are still active. DEADLINE DL The job terminated on deadline FAILED F The job terminated with a non-zero exit code and failed to execute. NODE_FAIL NF The job terminated due to failure of one or more allocated nodes OUT_OF_MEMORY OOM The Job experienced an out of memory error. PENDING PD The job is waiting for resource allocation. It will eventually run. PREEMPTED PR The job was terminated because of preemption by another job. RUNNING R The job currently is allocated to a node and is running. SUSPENDED S A running job has been stopped with its cores released to other jobs. STOPPED ST A running job has been stopped with its cores retained. TIMEOUT TO Job terminated upon reaching its time limit. A full list of these Job State codes can be found in squeue documentation. or sacct documentation .","title":"Job State Codes"},{"location":"jobs/reason-codes/#job-reason-codes","text":"Reason Code Explaination Priority One or more higher priority jobs is in queue for running. Your job will eventually run. Dependency This job is waiting for a dependent job to complete and will run afterwards. Resources The job is waiting for resources to become available and will eventually run. InvalidAccount The job\u2019s account is invalid. Cancel the job and rerun with correct account. InvaldQoS The job\u2019s QoS is invalid. Cancel the job and rerun with correct account. QOSGrpCpuLimit All CPUs assigned to your job\u2019s specified QoS are in use; job will run eventually. QOSGrpMaxJobsLimit Maximum number of jobs for your job\u2019s QoS have been met; job will run eventually. QOSGrpNodeLimit All nodes assigned to your job\u2019s specified QoS are in use; job will run eventually. PartitionCpuLimit All CPUs assigned to your job\u2019s specified partition are in use; job will run eventually. PartitionMaxJobsLimit Maximum number of jobs for your job\u2019s partition have been met; job will run eventually. PartitionNodeLimit All nodes assigned to your job\u2019s specified partition are in use; job will run eventually. AssociationCpuLimit All CPUs assigned to your job\u2019s specified association are in use; job will run eventually. AssociationMaxJobsLimit Maximum number of jobs for your job\u2019s association have been met; job will run eventually. AssociationNodeLimit All nodes assigned to your job\u2019s specified association are in use; job will run eventually. A full list of these Job Reason Codes can be found in Slurm\u2019s documentation.","title":"Job Reason Codes"},{"location":"jobs/reason-codes/#running-job-statistics-metrics","text":"The sstat command allows users to easily pull up status information about their currently running jobs. This includes information about CPU usage , task information , node information , resident set size (RSS) , and virtual memory (VM) . We can invoke the sstat command as such: # /!\\ ADAPT <jobid> accordingly $ sstat --jobs = <jobid> By default, sstat will pull up significantly more information than what would be needed in the commands default output. To remedy this, we can use the --format flag to choose what we want in our output. A chart of some these variables are listed in the table below: Variable Description avecpu Average CPU time of all tasks in job. averss Average resident set size of all tasks. avevmsize Average virtual memory of all tasks in a job. jobid The id of the Job. maxrss Maximum number of bytes read by all tasks in the job. maxvsize Maximum number of bytes written by all tasks in the job. ntasks Number of tasks in a job. For an example, let's print out a job's average job id, cpu time, max rss, and number of tasks. We can do this by typing out the command: # /!\\ ADAPT <jobid> accordingly sstat --jobs = <jobid> --format = jobid,cputime,maxrss,ntasks A full list of variables that specify data handled by sstat can be found with the --helpformat flag or by visiting the slurm page on sstat .","title":"Running Job Statistics Metrics"},{"location":"jobs/reason-codes/#past-job-statistics-metrics","text":"You can use the custom susage function in /etc/profile.d/slurm.sh to collect statistics information. $ susage -h Usage: susage [-m] [-Y] [-S YYYY-MM-DD] [-E YYYT-MM-DD] For a specific user (if accounting rights granted): susage [...] -u <user> For a specific account (if accounting rights granted): susage [...] -A <account> Display past job usage summary But by default, you should use the sacct command allows users to pull up status information about past jobs. This command is very similar to sstat , but is used on jobs that have been previously run on the system instead of currently running jobs. # /!\\ ADAPT <jobid> accordingly $ sacct [ -X ] --jobs = <jobid> [ --format = metric1,... ] # OR, for a user, eventually between a Start and End date $ sacct [ -X ] -u $USER [ -S YYYY-MM-DD ] [ -E YYYY-MM-DD ] [ --format = metric1,... ] # OR, for an account - ADAPT <account> accordingly $ sacct [ -X ] -A <account> [ --format = metric1,... ] Use -X to aggregate the statistics relevant to the job allocation itself, not taking job steps into consideration. The main metrics code you may be interested to review are listed below. Variable Description account Account the job ran under. avecpu Average CPU time of all tasks in job. averss Average resident set size of all tasks in the job. cputime Formatted (Elapsed time * CPU) count used by a job or step. elapsed Jobs elapsed time formated as DD-HH:MM:SS. exitcode The exit code returned by the job script or salloc. jobid The id of the Job. jobname The name of the Job. maxdiskread Maximum number of bytes read by all tasks in the job. maxdiskwrite Maximum number of bytes written by all tasks in the job. maxrss Maximum resident set size of all tasks in the job. ncpus Amount of allocated CPUs. nnodes The number of nodes used in a job. ntasks Number of tasks in a job. priority Slurm priority. qos Quality of service. reqcpu Required number of CPUs reqmem Required amount of memory for a job. reqtres Required Trackable RESources (TRES) user Userna A full list of variables that specify data handled by sacct can be found with the --helpformat flag or by visiting the slurm page on sacct .","title":"Past Job Statistics Metrics"},{"location":"jobs/submit/","text":"Regular Jobs \u00b6 Node Type Slurm command regular sbatch [-A <project>] -p batch [--qos {high,urgent}] [-C {broadwell,skylake}] [...] gpu sbatch [-A <project>] -p gpu [--qos {high,urgent}] [-C volta[32]] -G 1 [...] bigmem sbatch [-A <project>] -p bigmem [--qos {high,urgent}] [...] Main Slurm commands Resource Allocation guide sbatch [...] /path/to/launcher \u00b6 sbatch is used to submit a batch launcher script for later execution, corresponding to batch/passive submission mode . The script will typically contain one or more srun commands to launch parallel tasks. Upon submission with sbatch , Slurm will: allocate resources (nodes, tasks, partition, constraints, etc.) runs a single copy of the batch script on the first allocated node in particular, if you depend on other scripts, ensure you have refer to them with the complete path toward them. When you submit the job, Slurm responds with the job's ID, which will be used to identify this job in reports from Slurm. # /!\\ ADAPT path to launcher accordingly $ sbatch <path/to/launcher>.sh Submitted batch job 864933 Job Submission Option \u00b6 There are several useful environment variables set be Slurm within an allocated job. The most important ones are detailed in the below table which summarizes the main job submission options offered with {sbatch | srun | salloc} [...] : Command-line option Description Example -N <N> <N> Nodes request -N 2 --ntasks-per-node=<n> <n> Tasks-per-node request --ntasks-per-node=28 --ntasks-per-socket=<s> <s> Tasks-per-socket request --ntasks-per-socket=14 -c <c> <c> Cores-per-task request (multithreading) -c 1 --mem=<m>GB <m> GB memory per node request --mem 0 -t [DD-]HH[:MM:SS]> Walltime request -t 4:00:00 -G <gpu> <gpu> GPU(s) request -G 4 -C <feature> Feature request ( broadwell,skylake... ) -C skylake -p <partition> Specify job partition/queue --qos <qos> Specify job qos -A <account> Specify account -J <name> Job name -J MyApp -d <specification> Job dependency -d singleton --mail-user=<email> Specify email address --mail-type=<type> Notify user by email when certain event types occur. --mail-type=END,FAIL At a minimum a job submission script must include number of nodes, time, type of partition and nodes (resource allocation constraint and features), and quality of service (QOS). If a script does not specify any of these options then a default may be applied. The full list of directives is documented in the man pages for the sbatch command (see. man sbatch ). Within a job, you aim at running a certain number of tasks , and Slurm allow for a fine-grain control of the resource allocation that must be satisfied for each task. Beware of Slurm terminology in Multicore Architecture ! Slurm Node = Physical node , specified with -N <#nodes> Advice : always explicit number of expected number of tasks per node using --ntasks-per-node <n> . This way you control the node footprint of your job. Slurm Socket = Physical Socket/CPU/Processor Advice : if possible, explicit also the number of expected number of tasks per socket (processor) using --ntasks-per-socket <s> . relations between <s> and <n> must be aligned with the physical NUMA characteristics of the node. For instance on aion nodes, <n> = 8*<s> For instance on iris regular nodes, <n>=2*<s> when on iris bigmem nodes, <n>=4*<s> . ( the most confusing ): Slurm CPU = Physical CORE use -c <#threads> to specify the number of cores reserved per task. Hyper-Threading (HT) Technology is disabled on all ULHPC compute nodes. In particular: assume #cores = #threads , thus when using -c <threads> , you can safely set OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } # Default to 1 if SLURM_CPUS_PER_TASK not set to automatically abstract from the job context you have interest to match the physical NUMA characteristics of the compute node you're running at (Ex: target 16 threads per socket on Aion nodes (as there are 8 virtual sockets per nodes, 14 threads per socket on Iris regular nodes). The total number of tasks defined in a given job is stored in the $SLURM_NTASKS environment variable. This is very convenient to abstract from the job context to run MPI tasks/processes in parallel using for instance: srun -n ${ SLURM_NTASKS } [ ... ] We encourage you to always explicitly specify upon resource allocation the number of tasks you want per node/socket ( --ntasks-per-node <n> --ntasks-per-socket <s> ), to easily scale on multiple nodes with -N <N> . Adapt the number of threads and the settings to match the physical NUMA characteristics of the nodes Aion 16 cores per socket and 8 (virtual) sockets (CPUs) per aion node. {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <8n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 8\\times \\times 8\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 16 Ex: -N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4 ( Total : 64 tasks) Iris (default Dual-CPU) 14 cores per socket and 2 sockets (physical CPUs) per regular iris . {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <2n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 2\\times \\times 2\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 14 Ex: -N 2 --ntasks-per-node 4 --ntasks-per-socket 2 -c 7 ( Total : 8 tasks) Iris (Bigmem) 28 cores per socket and 4 sockets (physical CPUs) per bigmem iris {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <4n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 4\\times \\times 4\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 28 Ex: -N 2 --ntasks-per-node 8 --ntasks-per-socket 2 -c 14 ( Total : 16 tasks) Careful Monitoring of your Jobs \u00b6 Bug DON'T LEAVE your jobs running WITHOUT monitoring them and ensure they are not abusing of the computational resources allocated for you!!! ULHPC Tutorial / Getting Started You will find below several ways to monitor the effective usage of the resources allocated (for running jobs) as well as the general efficiency (Average Walltime Accuracy, CPU/Memory efficiency etc.) for past jobs. Joining/monitoring running jobs \u00b6 sjoin \u00b6 At any moment of time, you can join a running job using the custom helper functions sjoin in another terminal (or another screen/tmux tab/window). The format is as follows: sjoin <jobid> [ -w <node> ] # Use <tab> to automatically complete <jobid> among your jobs Using sjoin to htop your processes # check your running job ( access ) $> sq # squeue -u $(whoami) JOBID PARTIT QOS NAME USER NODE CPUS ST TIME TIME_LEFT PRIORITY NODELIST ( REASON ) 2171206 [ ... ] # Connect to your running job, identified by its Job ID ( access ) $> sjoin 2171206 # /!\\ ADAPT <jobid> accordingly, use <TAB> to have it autocatically completed # Equivalent of: srun --jobid 2171206 --gres=gpu:0 --pty bash -i ( node ) $> htop # view of all processes # F5: tree view # u <name>: filter by process of <name> # q: quit On the [impossibility] to monitor passive GPU jobs over sjoin If you use sjoin to join a GPU job, you WON'T be able to see the allocated GPU activity with nvidia-smi and all the monitoring tools provided by NVidia. The reason is that currently, there is no way to perform an over-allocation of a Slurm Generic Resource (GRES) as our GPU cards, that means you can't create ( e.g. with sjoin or srun --jobid [...] ) job steps with access to GPUs which are bound to another step. To keep sjoin working with gres job, you MUST add \" --gres=none \" You can use a direct connection with ssh <node> or clush -w @job:<jobid> for that (see below) but be aware that confined context is NOT maintained that way and that you will see the GPU processes on all 4 GPU cards. ClusterShell \u00b6 Danger Only for VERY Advanced users!!! . You should know what you are doing when using ClusterShell as you can mistakenly generate a huge amount of remote commands across the cluster which, while they will likely fail, still induce an unexpected load that may disturb the system. ClusterShell is a useful Python package for executing arbitrary commands across multiple hosts. On the ULHPC clusters, it provides a relatively simple way for you to run commands on nodes your jobs are running on, and collect the results. Info You can only ssh to, and therefore run clush on, nodes where you have active/running jobs. nodeset \u00b6 The nodeset command enables the easy manipulation of node sets, as well as node groups, at the command line level. It uses sinfo underneath but has slightly different syntax. You can use it to ask about node states and nodes your job is running on. The nice difference is you can ask for folded (e.g. iris-[075,078,091-092] ) or expanded (e.g. iris-075 iris-078 iris-091 iris-092 ) forms of the node lists. Command description nodeset -L[LL] List all groups available nodeset -c [...] show number of nodes in nodeset(s) nodeset -e [...] expand nodeset(s) to separate nodes nodeset -f [...] fold nodeset(s) (or separate nodes) into one nodeset Nodeset expansion and folding nodeset -e (expand) # Get list of nodes with issues $ sinfo -R --noheader -o \"%N\" iris- [ 005 -008,017,161-162 ] # ... and expand that list $ sinfo -R --noheader -o \"%N\" | nodeset -e iris-005 iris-006 iris-007 iris-008 iris-017 iris-161 iris-162 # Actually equivalent of (see below) $ nodeset -e @state:drained nodeset -f (fold) # List nodes in IDLE state $> sinfo -t IDLE --noheader interactive up 4 :00:00 4 idle iris- [ 003 -005,007 ] long up 30 -00:00:0 2 idle iris- [ 015 -016 ] batch* up 5 -00:00:00 1 idle iris-134 gpu up 5 -00:00:00 9 idle iris- [ 170 ,173,175-178,181 ] bigmem up 5 -00:00:00 0 n/a # make out a synthetic list $> sinfo -t IDLE --noheader | awk '{ print $6 }' | nodeset -f iris- [ 003 -005,007,015-016,134,170,173,175-178,181 ] # ... actually done when restricting the column to nodelist only $> sinfo -t IDLE --noheader -o \"%N\" iris- [ 003 -005,007,015-016,134,170,173,175-178,181 ] # Actually equivalent of (see below) $ nodeset -f @state:idle Exclusion / intersection of nodeset Option Description -x <nodeset> exclude from working set <nodeset> -i <nodeset> intersection from working set with <nodeset> -X <nodeset> ( --xor ) elements that are in exactly one of the working set and <nodeset> # Exclusion $> nodeset -f iris- [ 001 -010 ] -x iris- [ 003 -005,007,015-016 ] iris- [ 001 -002,006,008-010 ] # Intersection $> nodeset -f iris- [ 001 -010 ] -i iris- [ 003 -005,007,015-016 ] iris- [ 003 -005,007 ] # \"XOR\" (one occurrence only) $> nodeset -f iris- [ 001 -010 ] -x iris-006 -X iris- [ 005 -007 ] iris- [ 001 -004,006,008-010 ] The groups useful to you that we have configured are @user , @job and @state . List available groups $ nodeset -LLL # convenient partition groups @batch iris- [ 001 -168 ] 168 @bigmem iris- [ 187 -190 ] 4 @gpu iris- [ 169 -186,191-196 ] 24 @interactive iris- [ 001 -196 ] 196 # conveniente state groups @state:allocated [ ... ] @state:idle [ ... ] @state:mixed [ ... ] @state:reserved [ ... ] # your individual jobs @job:2252046 iris-076 1 @job:2252050 iris- [ 191 -196 ] 6 # all the jobs under your username @user:svarrette iris- [ 076 ,191-196 ] 7 User group List expanded node names where you have jobs running # Similar to: squeue -h -u $USER -o \"%N\"|nodeset -e $ nodeset -e @user: $USER Job group List folded nodes where your job 1234567 is running (use sq to quickly list your jobs): $ similar to squeue -h -j 1234567 -o \"%N\" nodeset -f @job:1234567 State group List expanded node names that are idle according to slurm # Similar to: sinfo -t IDLE -o \"%N\" nodeset -e @state:idle clush \u00b6 clush can run commands on multiple nodes at once for instance to monitor you jobs. It uses the node grouping syntax from [ nodeset ](( https://clustershell.readthedocs.io/en/latest/tools/nodeset.html ) to allow you to run commands on those nodes. clush uses ssh to connect to each of these nodes. You can use the -b option to gather output from nodes with same output into the same lines. Leaving this out will report on each node separately. Option Description -b gathering output (as when piping to dshbak -c ) -w <nodelist> specify remote hosts, incl. node groups with @group special syntax -g <group> similar to -w @<group> , restrict commands to the hosts group <group> --diff show differences between common outputs Monitor CPU usage Show %cpu, memory usage, and command for all nodes running any of your jobs. clush -bw @user: $USER ps -u $USER -o%cpu,rss,cmd As above, but only for the nodes reserved with your job <jobid> clush -bw @job:<jobid> ps -u $USER -o%cpu,rss,cmd Monitor GPU usage Show what's running on all the GPUs on the nodes associated with your job 654321 . clush -bw @job:654321 bash -l -c 'nvidia-smi --format=csv --query-compute-apps=process_name,used_gpu_memory' As above but for all your jobs (assuming you have only GPU nodes with all GPUs) clush -bw @user: $USER bash -l -c 'nvidia-smi --format=csv --query-compute-apps=process_name,used_gpu_memory' This may be convenient for passive jobs since the sjoin utility does NOT permit to run nvidia-smi (see explaination ). However that way you will see unfortunately ALL processes running on the 4 GPU cards -- including from other users sharing your nodes. It's a known bug, not a feature. pestat : CPU/Mem usage report \u00b6 We have deployed the (excellent) Slurm tool pestat (Processor Element status) of Ole Holm Nielsen that you can use to quickly check the CPU/Memory usage of your jobs. Information deserving investigation (too low/high CPU or Memory usage compared to allocation) will be flagged in Red or Magenta pestat [-p <partition>] [-G] [-f] pestat output (official sample output) General Guidelines \u00b6 As mentionned before, always check your node activity with at least htop on the all allocated nodes to ensure you use them as expected. Several cases might apply to your job workflow: Single Node, single core You are dealing with an embarrasingly parallel job campaign and this approach is bad and overload the scheduler unnecessarily. You will also quickly cross the limits set in terms of maximum number of jobs. You must aggregate multiples tasks within a single job to exploit fully a complete node. In particular, you MUST consider using GNU Parallel and our generic GNU launcher launcher.parallel.sh . ULHPC Tutorial / HPC Management of Embarrassingly Parallel Jobs Single Node, multi-core If you asked for more than a core in your job (> 1 tasks, -c <threads> where <threads> > 1), there are 3 typical situations you MUST analysed (and pestat or htop are of great help for that): You cannot see the expected activity (only 1 core seems to be active at 100%), then you should review your workflow as you are under -exploiting (and thus probably waste ) the allocated resources. you have the expected activity on the requested cores (Ex: the 28 cores were requested, and htop reports a significant usage of all cores) BUT the CPU load of the system exceed the core capacity of the computing node . That means you are forking too many processes and overloading/harming the systems. For instance on regular iris (resp. aion ) node, a CPU load above 28 (resp. 128) is suspect. Note that we use LBNL Node Health Check (NHC) to automatically drain nodes for which the load exceed twice the core capacity An analogy for a single core load with the amont of cars possible in a single-lane brige or tunnel is illustrated below ( source ). Like the bridge/tunnel operator, you'd like your cars/processes to never be waiting, otherwise you are harming the system. Imagine this analogy for the amount of cores available on a computing node to better reporesent the situtation on a single core. you have the expected activity on the requested cores and the load match your allocation without harming the system: you're good to go! Multi-node If you asked for more than ONE node , ensure that you have consider the following questions. You are running an MPI job : you generally know what you're doing, YET ensure your followed the single node monitoring checks ( htop etc. yet across all nodes) to review your core activity on ALL nodes (see 3. below) . Consider also parallel profilers like Arm Forge You are running an embarrasingly parallel job campaign . You should first ensure you correctly exploit a single node using GNU Parallel before attempting to cross multiple nodes You run a distributed framework able to exploit multiple nodes (typically with a master/slave model as for Spark cluster ). You MUST assert that your [slave] processes are really run on the over nodes using # check you running job $ sq # Join **another** node than the first one listed $ sjoin <jobid> -w <node> $ htop # view of all processes # F5: tree view # u <name>: filter by process of <name> # q: quit Monitoring past jobs efficiency \u00b6 Walltime estimation and Job efficiency By default, none of the regular jobs you submit can exceed a walltime of 2 days ( 2-00:00:00 ). You have a strong interest to estimate accurately the walltime of your jobs. While it is not always possible, or quite hard to guess at the beginning of a given job campaign where you'll probably ask for the maximum walltime possible, you should look back as your historical usage for the past efficiency and elapsed time of your previously completed jobs using seff or susage utilities . Update the time constraint [#SBATCH] -t [...] of your jobs accordingly. There are two immediate benefits for you: Short jobs are scheduled faster, and may even be elligible for backfilling You will be more likely elligible for a raw share upgrade of your user account -- see Fairsharing The below utilities will help you track the CPU/Memory efficiency ( seff ) or the Average Walltime Accuracy ( susage , sacct ) of your past jobs seff \u00b6 Use seff to double check a past job CPU/Memory efficiency. Below examples should be self-speaking: Good CPU Eff. $ seff 2171749 Job ID: 2171749 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 28 CPU Utilized: 41-01:38:14 CPU Efficiency: 99.64% of 41-05:09:44 core-walltime Job Wall-clock time: 1-11:19:38 Memory Utilized: 2.73 GB Memory Efficiency: 2.43% of 112.00 GB Good Memory Eff. $ seff 2117620 Job ID: 2117620 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 16 CPU Utilized: 14:24:49 CPU Efficiency: 23.72% of 2-12:46:24 core-walltime Job Wall-clock time: 03:47:54 Memory Utilized: 193.04 GB Memory Efficiency: 80.43% of 240.00 GB Good CPU and Memory Eff. $ seff 2138087 Job ID: 2138087 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 64 CPU Utilized: 87-16:58:22 CPU Efficiency: 86.58% of 101-07:16:16 core-walltime Job Wall-clock time: 1-13:59:19 Memory Utilized: 1.64 TB Memory Efficiency: 99.29% of 1.65 TB [Very] Bad efficiency This illustrates a very bad job in terms of CPU/memory efficiency (below 4%), which illustrate a case where basically the user wasted 4 hours of computation while mobilizing a full node and its 28 cores. $ seff 2199497 Job ID: 2199497 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 28 CPU Utilized: 00:08:33 CPU Efficiency: 3.55% of 04:00:48 core-walltime Job Wall-clock time: 00:08:36 Memory Utilized: 55.84 MB Memory Efficiency: 0.05% of 112.00 GB This is typical of a single-core task can could be drastically improved via GNU Parallel . Note however that demonstrating a CPU good efficiency with seff may not be enough! You may still induce an abnormal load on the reserved nodes if you spawn more processes than allowed by the Slurm reservation. To avoid that, always try to prefix your executions with srun within your launchers. See also Specific Resource Allocations . susage \u00b6 Use susage to check your past jobs walltime accuracy ( Timelimit vs. Elapsed ) $ susage -h Usage: susage [-m] [-Y] [-S YYYY-MM-DD] [-E YYYT-MM-DD] For a specific user (if accounting rights granted): susage [...] -u <user> For a specific account (if accounting rights granted): susage [...] -A <account> Display past job usage summary In all cases, if you are confident that your jobs will last more than 2 days while efficiently using the allocated resources , you can use --qos long QOS. Be aware that special restrictions applies for this kind of jobs.","title":"Passive/Batch Job"},{"location":"jobs/submit/#regular-jobs","text":"Node Type Slurm command regular sbatch [-A <project>] -p batch [--qos {high,urgent}] [-C {broadwell,skylake}] [...] gpu sbatch [-A <project>] -p gpu [--qos {high,urgent}] [-C volta[32]] -G 1 [...] bigmem sbatch [-A <project>] -p bigmem [--qos {high,urgent}] [...] Main Slurm commands Resource Allocation guide","title":"Regular Jobs"},{"location":"jobs/submit/#sbatch-pathtolauncher","text":"sbatch is used to submit a batch launcher script for later execution, corresponding to batch/passive submission mode . The script will typically contain one or more srun commands to launch parallel tasks. Upon submission with sbatch , Slurm will: allocate resources (nodes, tasks, partition, constraints, etc.) runs a single copy of the batch script on the first allocated node in particular, if you depend on other scripts, ensure you have refer to them with the complete path toward them. When you submit the job, Slurm responds with the job's ID, which will be used to identify this job in reports from Slurm. # /!\\ ADAPT path to launcher accordingly $ sbatch <path/to/launcher>.sh Submitted batch job 864933","title":"sbatch [...] /path/to/launcher"},{"location":"jobs/submit/#job-submission-option","text":"There are several useful environment variables set be Slurm within an allocated job. The most important ones are detailed in the below table which summarizes the main job submission options offered with {sbatch | srun | salloc} [...] : Command-line option Description Example -N <N> <N> Nodes request -N 2 --ntasks-per-node=<n> <n> Tasks-per-node request --ntasks-per-node=28 --ntasks-per-socket=<s> <s> Tasks-per-socket request --ntasks-per-socket=14 -c <c> <c> Cores-per-task request (multithreading) -c 1 --mem=<m>GB <m> GB memory per node request --mem 0 -t [DD-]HH[:MM:SS]> Walltime request -t 4:00:00 -G <gpu> <gpu> GPU(s) request -G 4 -C <feature> Feature request ( broadwell,skylake... ) -C skylake -p <partition> Specify job partition/queue --qos <qos> Specify job qos -A <account> Specify account -J <name> Job name -J MyApp -d <specification> Job dependency -d singleton --mail-user=<email> Specify email address --mail-type=<type> Notify user by email when certain event types occur. --mail-type=END,FAIL At a minimum a job submission script must include number of nodes, time, type of partition and nodes (resource allocation constraint and features), and quality of service (QOS). If a script does not specify any of these options then a default may be applied. The full list of directives is documented in the man pages for the sbatch command (see. man sbatch ). Within a job, you aim at running a certain number of tasks , and Slurm allow for a fine-grain control of the resource allocation that must be satisfied for each task. Beware of Slurm terminology in Multicore Architecture ! Slurm Node = Physical node , specified with -N <#nodes> Advice : always explicit number of expected number of tasks per node using --ntasks-per-node <n> . This way you control the node footprint of your job. Slurm Socket = Physical Socket/CPU/Processor Advice : if possible, explicit also the number of expected number of tasks per socket (processor) using --ntasks-per-socket <s> . relations between <s> and <n> must be aligned with the physical NUMA characteristics of the node. For instance on aion nodes, <n> = 8*<s> For instance on iris regular nodes, <n>=2*<s> when on iris bigmem nodes, <n>=4*<s> . ( the most confusing ): Slurm CPU = Physical CORE use -c <#threads> to specify the number of cores reserved per task. Hyper-Threading (HT) Technology is disabled on all ULHPC compute nodes. In particular: assume #cores = #threads , thus when using -c <threads> , you can safely set OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } # Default to 1 if SLURM_CPUS_PER_TASK not set to automatically abstract from the job context you have interest to match the physical NUMA characteristics of the compute node you're running at (Ex: target 16 threads per socket on Aion nodes (as there are 8 virtual sockets per nodes, 14 threads per socket on Iris regular nodes). The total number of tasks defined in a given job is stored in the $SLURM_NTASKS environment variable. This is very convenient to abstract from the job context to run MPI tasks/processes in parallel using for instance: srun -n ${ SLURM_NTASKS } [ ... ] We encourage you to always explicitly specify upon resource allocation the number of tasks you want per node/socket ( --ntasks-per-node <n> --ntasks-per-socket <s> ), to easily scale on multiple nodes with -N <N> . Adapt the number of threads and the settings to match the physical NUMA characteristics of the nodes Aion 16 cores per socket and 8 (virtual) sockets (CPUs) per aion node. {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <8n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 8\\times \\times 8\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 16 Ex: -N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4 ( Total : 64 tasks) Iris (default Dual-CPU) 14 cores per socket and 2 sockets (physical CPUs) per regular iris . {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <2n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 2\\times \\times 2\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 14 Ex: -N 2 --ntasks-per-node 4 --ntasks-per-socket 2 -c 7 ( Total : 8 tasks) Iris (Bigmem) 28 cores per socket and 4 sockets (physical CPUs) per bigmem iris {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <4n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 4\\times \\times 4\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 28 Ex: -N 2 --ntasks-per-node 8 --ntasks-per-socket 2 -c 14 ( Total : 16 tasks)","title":"Job Submission Option"},{"location":"jobs/submit/#careful-monitoring-of-your-jobs","text":"Bug DON'T LEAVE your jobs running WITHOUT monitoring them and ensure they are not abusing of the computational resources allocated for you!!! ULHPC Tutorial / Getting Started You will find below several ways to monitor the effective usage of the resources allocated (for running jobs) as well as the general efficiency (Average Walltime Accuracy, CPU/Memory efficiency etc.) for past jobs.","title":"Careful Monitoring of your Jobs"},{"location":"jobs/submit/#joiningmonitoring-running-jobs","text":"","title":"Joining/monitoring running jobs"},{"location":"jobs/submit/#sjoin","text":"At any moment of time, you can join a running job using the custom helper functions sjoin in another terminal (or another screen/tmux tab/window). The format is as follows: sjoin <jobid> [ -w <node> ] # Use <tab> to automatically complete <jobid> among your jobs Using sjoin to htop your processes # check your running job ( access ) $> sq # squeue -u $(whoami) JOBID PARTIT QOS NAME USER NODE CPUS ST TIME TIME_LEFT PRIORITY NODELIST ( REASON ) 2171206 [ ... ] # Connect to your running job, identified by its Job ID ( access ) $> sjoin 2171206 # /!\\ ADAPT <jobid> accordingly, use <TAB> to have it autocatically completed # Equivalent of: srun --jobid 2171206 --gres=gpu:0 --pty bash -i ( node ) $> htop # view of all processes # F5: tree view # u <name>: filter by process of <name> # q: quit On the [impossibility] to monitor passive GPU jobs over sjoin If you use sjoin to join a GPU job, you WON'T be able to see the allocated GPU activity with nvidia-smi and all the monitoring tools provided by NVidia. The reason is that currently, there is no way to perform an over-allocation of a Slurm Generic Resource (GRES) as our GPU cards, that means you can't create ( e.g. with sjoin or srun --jobid [...] ) job steps with access to GPUs which are bound to another step. To keep sjoin working with gres job, you MUST add \" --gres=none \" You can use a direct connection with ssh <node> or clush -w @job:<jobid> for that (see below) but be aware that confined context is NOT maintained that way and that you will see the GPU processes on all 4 GPU cards.","title":"sjoin"},{"location":"jobs/submit/#clustershell","text":"Danger Only for VERY Advanced users!!! . You should know what you are doing when using ClusterShell as you can mistakenly generate a huge amount of remote commands across the cluster which, while they will likely fail, still induce an unexpected load that may disturb the system. ClusterShell is a useful Python package for executing arbitrary commands across multiple hosts. On the ULHPC clusters, it provides a relatively simple way for you to run commands on nodes your jobs are running on, and collect the results. Info You can only ssh to, and therefore run clush on, nodes where you have active/running jobs.","title":"ClusterShell"},{"location":"jobs/submit/#nodeset","text":"The nodeset command enables the easy manipulation of node sets, as well as node groups, at the command line level. It uses sinfo underneath but has slightly different syntax. You can use it to ask about node states and nodes your job is running on. The nice difference is you can ask for folded (e.g. iris-[075,078,091-092] ) or expanded (e.g. iris-075 iris-078 iris-091 iris-092 ) forms of the node lists. Command description nodeset -L[LL] List all groups available nodeset -c [...] show number of nodes in nodeset(s) nodeset -e [...] expand nodeset(s) to separate nodes nodeset -f [...] fold nodeset(s) (or separate nodes) into one nodeset Nodeset expansion and folding nodeset -e (expand) # Get list of nodes with issues $ sinfo -R --noheader -o \"%N\" iris- [ 005 -008,017,161-162 ] # ... and expand that list $ sinfo -R --noheader -o \"%N\" | nodeset -e iris-005 iris-006 iris-007 iris-008 iris-017 iris-161 iris-162 # Actually equivalent of (see below) $ nodeset -e @state:drained nodeset -f (fold) # List nodes in IDLE state $> sinfo -t IDLE --noheader interactive up 4 :00:00 4 idle iris- [ 003 -005,007 ] long up 30 -00:00:0 2 idle iris- [ 015 -016 ] batch* up 5 -00:00:00 1 idle iris-134 gpu up 5 -00:00:00 9 idle iris- [ 170 ,173,175-178,181 ] bigmem up 5 -00:00:00 0 n/a # make out a synthetic list $> sinfo -t IDLE --noheader | awk '{ print $6 }' | nodeset -f iris- [ 003 -005,007,015-016,134,170,173,175-178,181 ] # ... actually done when restricting the column to nodelist only $> sinfo -t IDLE --noheader -o \"%N\" iris- [ 003 -005,007,015-016,134,170,173,175-178,181 ] # Actually equivalent of (see below) $ nodeset -f @state:idle Exclusion / intersection of nodeset Option Description -x <nodeset> exclude from working set <nodeset> -i <nodeset> intersection from working set with <nodeset> -X <nodeset> ( --xor ) elements that are in exactly one of the working set and <nodeset> # Exclusion $> nodeset -f iris- [ 001 -010 ] -x iris- [ 003 -005,007,015-016 ] iris- [ 001 -002,006,008-010 ] # Intersection $> nodeset -f iris- [ 001 -010 ] -i iris- [ 003 -005,007,015-016 ] iris- [ 003 -005,007 ] # \"XOR\" (one occurrence only) $> nodeset -f iris- [ 001 -010 ] -x iris-006 -X iris- [ 005 -007 ] iris- [ 001 -004,006,008-010 ] The groups useful to you that we have configured are @user , @job and @state . List available groups $ nodeset -LLL # convenient partition groups @batch iris- [ 001 -168 ] 168 @bigmem iris- [ 187 -190 ] 4 @gpu iris- [ 169 -186,191-196 ] 24 @interactive iris- [ 001 -196 ] 196 # conveniente state groups @state:allocated [ ... ] @state:idle [ ... ] @state:mixed [ ... ] @state:reserved [ ... ] # your individual jobs @job:2252046 iris-076 1 @job:2252050 iris- [ 191 -196 ] 6 # all the jobs under your username @user:svarrette iris- [ 076 ,191-196 ] 7 User group List expanded node names where you have jobs running # Similar to: squeue -h -u $USER -o \"%N\"|nodeset -e $ nodeset -e @user: $USER Job group List folded nodes where your job 1234567 is running (use sq to quickly list your jobs): $ similar to squeue -h -j 1234567 -o \"%N\" nodeset -f @job:1234567 State group List expanded node names that are idle according to slurm # Similar to: sinfo -t IDLE -o \"%N\" nodeset -e @state:idle","title":"nodeset"},{"location":"jobs/submit/#clush","text":"clush can run commands on multiple nodes at once for instance to monitor you jobs. It uses the node grouping syntax from [ nodeset ](( https://clustershell.readthedocs.io/en/latest/tools/nodeset.html ) to allow you to run commands on those nodes. clush uses ssh to connect to each of these nodes. You can use the -b option to gather output from nodes with same output into the same lines. Leaving this out will report on each node separately. Option Description -b gathering output (as when piping to dshbak -c ) -w <nodelist> specify remote hosts, incl. node groups with @group special syntax -g <group> similar to -w @<group> , restrict commands to the hosts group <group> --diff show differences between common outputs Monitor CPU usage Show %cpu, memory usage, and command for all nodes running any of your jobs. clush -bw @user: $USER ps -u $USER -o%cpu,rss,cmd As above, but only for the nodes reserved with your job <jobid> clush -bw @job:<jobid> ps -u $USER -o%cpu,rss,cmd Monitor GPU usage Show what's running on all the GPUs on the nodes associated with your job 654321 . clush -bw @job:654321 bash -l -c 'nvidia-smi --format=csv --query-compute-apps=process_name,used_gpu_memory' As above but for all your jobs (assuming you have only GPU nodes with all GPUs) clush -bw @user: $USER bash -l -c 'nvidia-smi --format=csv --query-compute-apps=process_name,used_gpu_memory' This may be convenient for passive jobs since the sjoin utility does NOT permit to run nvidia-smi (see explaination ). However that way you will see unfortunately ALL processes running on the 4 GPU cards -- including from other users sharing your nodes. It's a known bug, not a feature.","title":"clush"},{"location":"jobs/submit/#pestat-cpumem-usage-report","text":"We have deployed the (excellent) Slurm tool pestat (Processor Element status) of Ole Holm Nielsen that you can use to quickly check the CPU/Memory usage of your jobs. Information deserving investigation (too low/high CPU or Memory usage compared to allocation) will be flagged in Red or Magenta pestat [-p <partition>] [-G] [-f] pestat output (official sample output)","title":"pestat: CPU/Mem usage report"},{"location":"jobs/submit/#general-guidelines","text":"As mentionned before, always check your node activity with at least htop on the all allocated nodes to ensure you use them as expected. Several cases might apply to your job workflow: Single Node, single core You are dealing with an embarrasingly parallel job campaign and this approach is bad and overload the scheduler unnecessarily. You will also quickly cross the limits set in terms of maximum number of jobs. You must aggregate multiples tasks within a single job to exploit fully a complete node. In particular, you MUST consider using GNU Parallel and our generic GNU launcher launcher.parallel.sh . ULHPC Tutorial / HPC Management of Embarrassingly Parallel Jobs Single Node, multi-core If you asked for more than a core in your job (> 1 tasks, -c <threads> where <threads> > 1), there are 3 typical situations you MUST analysed (and pestat or htop are of great help for that): You cannot see the expected activity (only 1 core seems to be active at 100%), then you should review your workflow as you are under -exploiting (and thus probably waste ) the allocated resources. you have the expected activity on the requested cores (Ex: the 28 cores were requested, and htop reports a significant usage of all cores) BUT the CPU load of the system exceed the core capacity of the computing node . That means you are forking too many processes and overloading/harming the systems. For instance on regular iris (resp. aion ) node, a CPU load above 28 (resp. 128) is suspect. Note that we use LBNL Node Health Check (NHC) to automatically drain nodes for which the load exceed twice the core capacity An analogy for a single core load with the amont of cars possible in a single-lane brige or tunnel is illustrated below ( source ). Like the bridge/tunnel operator, you'd like your cars/processes to never be waiting, otherwise you are harming the system. Imagine this analogy for the amount of cores available on a computing node to better reporesent the situtation on a single core. you have the expected activity on the requested cores and the load match your allocation without harming the system: you're good to go! Multi-node If you asked for more than ONE node , ensure that you have consider the following questions. You are running an MPI job : you generally know what you're doing, YET ensure your followed the single node monitoring checks ( htop etc. yet across all nodes) to review your core activity on ALL nodes (see 3. below) . Consider also parallel profilers like Arm Forge You are running an embarrasingly parallel job campaign . You should first ensure you correctly exploit a single node using GNU Parallel before attempting to cross multiple nodes You run a distributed framework able to exploit multiple nodes (typically with a master/slave model as for Spark cluster ). You MUST assert that your [slave] processes are really run on the over nodes using # check you running job $ sq # Join **another** node than the first one listed $ sjoin <jobid> -w <node> $ htop # view of all processes # F5: tree view # u <name>: filter by process of <name> # q: quit","title":"General Guidelines"},{"location":"jobs/submit/#monitoring-past-jobs-efficiency","text":"Walltime estimation and Job efficiency By default, none of the regular jobs you submit can exceed a walltime of 2 days ( 2-00:00:00 ). You have a strong interest to estimate accurately the walltime of your jobs. While it is not always possible, or quite hard to guess at the beginning of a given job campaign where you'll probably ask for the maximum walltime possible, you should look back as your historical usage for the past efficiency and elapsed time of your previously completed jobs using seff or susage utilities . Update the time constraint [#SBATCH] -t [...] of your jobs accordingly. There are two immediate benefits for you: Short jobs are scheduled faster, and may even be elligible for backfilling You will be more likely elligible for a raw share upgrade of your user account -- see Fairsharing The below utilities will help you track the CPU/Memory efficiency ( seff ) or the Average Walltime Accuracy ( susage , sacct ) of your past jobs","title":"Monitoring past jobs efficiency"},{"location":"jobs/submit/#seff","text":"Use seff to double check a past job CPU/Memory efficiency. Below examples should be self-speaking: Good CPU Eff. $ seff 2171749 Job ID: 2171749 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 28 CPU Utilized: 41-01:38:14 CPU Efficiency: 99.64% of 41-05:09:44 core-walltime Job Wall-clock time: 1-11:19:38 Memory Utilized: 2.73 GB Memory Efficiency: 2.43% of 112.00 GB Good Memory Eff. $ seff 2117620 Job ID: 2117620 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 16 CPU Utilized: 14:24:49 CPU Efficiency: 23.72% of 2-12:46:24 core-walltime Job Wall-clock time: 03:47:54 Memory Utilized: 193.04 GB Memory Efficiency: 80.43% of 240.00 GB Good CPU and Memory Eff. $ seff 2138087 Job ID: 2138087 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 64 CPU Utilized: 87-16:58:22 CPU Efficiency: 86.58% of 101-07:16:16 core-walltime Job Wall-clock time: 1-13:59:19 Memory Utilized: 1.64 TB Memory Efficiency: 99.29% of 1.65 TB [Very] Bad efficiency This illustrates a very bad job in terms of CPU/memory efficiency (below 4%), which illustrate a case where basically the user wasted 4 hours of computation while mobilizing a full node and its 28 cores. $ seff 2199497 Job ID: 2199497 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 28 CPU Utilized: 00:08:33 CPU Efficiency: 3.55% of 04:00:48 core-walltime Job Wall-clock time: 00:08:36 Memory Utilized: 55.84 MB Memory Efficiency: 0.05% of 112.00 GB This is typical of a single-core task can could be drastically improved via GNU Parallel . Note however that demonstrating a CPU good efficiency with seff may not be enough! You may still induce an abnormal load on the reserved nodes if you spawn more processes than allowed by the Slurm reservation. To avoid that, always try to prefix your executions with srun within your launchers. See also Specific Resource Allocations .","title":"seff"},{"location":"jobs/submit/#susage","text":"Use susage to check your past jobs walltime accuracy ( Timelimit vs. Elapsed ) $ susage -h Usage: susage [-m] [-Y] [-S YYYY-MM-DD] [-E YYYT-MM-DD] For a specific user (if accounting rights granted): susage [...] -u <user> For a specific account (if accounting rights granted): susage [...] -A <account> Display past job usage summary In all cases, if you are confident that your jobs will last more than 2 days while efficiently using the allocated resources , you can use --qos long QOS. Be aware that special restrictions applies for this kind of jobs.","title":"susage"},{"location":"policies/aup/","text":"Acceptable Use Policy (AUP) 2.1 \u00b6 The University of Luxembourg operates since 2007 a large academic HPC facility which remains the reference implementation within the country, offering a cutting-edge research infrastructure to Luxembourg public research while serving as edge access to the upcoming Euro-HPC Luxembourg supercomputer. The University extends access to its HPC resources (including facilities, services and HPC experts) to its students, staff, research partners (including scientific staff of national public organizations and external partners for the duration of joint research projects) and to industrial partners. UL HPC AUP \u00b6 There are a number of policies which apply to ULHPC users. UL HPC Acceptable Use Policy (AUP) [pdf] Important All users of UL HPC resources and PIs must abide by the UL HPC Acceptable Use Policy (AUP) . You should read and keep a signed copy of this document before using the facility. Access and/or usage of any ULHPC system assumes the tacit acknowledgement to this policy. The purpose of this document is to define the rules and terms governing acceptable use of resources (core hours, license hours, data storage capacity as well as network connectivity and technical support), including access, utilization and security of the resources and data. Crediting ULHPC in your research \u00b6 One of the requirements stemming from the AUP , is to credit and acknowle the usage of the University of Luxembourg HPC facility for ALL publications and contributions having results and/or contents obtained or derived from that usage. Publication tagging \u00b6 You are also requested to tag the publication(s) you have produced thanks to the usage of the UL HPC platform upon their registration on Orbilu : Login on MyOrbiLu Select your publication entry and select the \"Modify/review the reference\" icon Select the \"2. Additional info\" section in the \"Research Center\" field, enter ULHPC select the \"University of Luxembourg: High Performance Computing (ULHPC)\" entry in the proposed list. This tag is a very important indicator for us to quantify the concrete impact of the HPC facility on the research performed at the University. List of publications generated thanks to the UL HPC Platform","title":"Acceptable Use Policy (AUP)"},{"location":"policies/aup/#acceptable-use-policy-aup-21","text":"The University of Luxembourg operates since 2007 a large academic HPC facility which remains the reference implementation within the country, offering a cutting-edge research infrastructure to Luxembourg public research while serving as edge access to the upcoming Euro-HPC Luxembourg supercomputer. The University extends access to its HPC resources (including facilities, services and HPC experts) to its students, staff, research partners (including scientific staff of national public organizations and external partners for the duration of joint research projects) and to industrial partners.","title":"Acceptable Use Policy (AUP) 2.1"},{"location":"policies/aup/#ul-hpc-aup","text":"There are a number of policies which apply to ULHPC users. UL HPC Acceptable Use Policy (AUP) [pdf] Important All users of UL HPC resources and PIs must abide by the UL HPC Acceptable Use Policy (AUP) . You should read and keep a signed copy of this document before using the facility. Access and/or usage of any ULHPC system assumes the tacit acknowledgement to this policy. The purpose of this document is to define the rules and terms governing acceptable use of resources (core hours, license hours, data storage capacity as well as network connectivity and technical support), including access, utilization and security of the resources and data.","title":"UL HPC AUP"},{"location":"policies/aup/#crediting-ulhpc-in-your-research","text":"One of the requirements stemming from the AUP , is to credit and acknowle the usage of the University of Luxembourg HPC facility for ALL publications and contributions having results and/or contents obtained or derived from that usage.","title":"Crediting ULHPC in your research"},{"location":"policies/aup/#publication-tagging","text":"You are also requested to tag the publication(s) you have produced thanks to the usage of the UL HPC platform upon their registration on Orbilu : Login on MyOrbiLu Select your publication entry and select the \"Modify/review the reference\" icon Select the \"2. Additional info\" section in the \"Research Center\" field, enter ULHPC select the \"University of Luxembourg: High Performance Computing (ULHPC)\" entry in the proposed list. This tag is a very important indicator for us to quantify the concrete impact of the HPC facility on the research performed at the University. List of publications generated thanks to the UL HPC Platform","title":"Publication tagging"},{"location":"policies/maintenance/","text":"Maintenance and Downtime Policy \u00b6 Scheduled Maintenance \u00b6 The ULHPC team will schedule maintenance in one of three manners: Rolling reboots Whenever possible, ULHPC will apply updates and do other maintenance in a rolling fashion in such a manner as to have either no or as little impact as possible to ULHPC services Partial outages We will do these as needed but in a manner that impacts only some ULHPC services at a time Full outages These are outages that will affect all ULHPC services, such as outages of core datacenter networking services, datacenter power of HVAC/cooling system maintenance or global GPFS/Spectrumscale filesystem updates . Such maintenance windows typically happen on a quarterly basis . It should be noted that we are not always able to anticipate when these outages are needed . ULHPC's goal for these downtimes is to have them completed as fast as possible. However, validation and qualification of the full platform takes typically one working day, and unforeseen or unusual circumstances may occur. So count for such outages a multiple-day downtime . Notifications \u00b6 We normally inform users of cluster maintenance at least 3 weeks in advance by mail using the HPC User community mailing list (moderated): hpc-users@uni.lu . A second reminder is sent a few days prior to actual downtime. The news of the downtimes is also posted on the Live status page. Finally, a colored \" message of the day \" (motd) banner is displayed on all access/login servers such that you can quickly be informed of any incoming maintenance operation upon connection to the cluster. You can see this when you login or (again),any time by issuing the command: cat /etc/motd Detecting maintenance... During the maintenance During the maintenance period, access to the involved cluster access/login serveur is DENIED and any users still logged-in are disconnected at the beginning of the maintenance you will receive a written message in your terminal if for some reason during the maintenance you urgently need to collect data from your account, please contact the UL HPC Team by sending a mail to: hpc-team@uni.lu . We will notify you of the end of the maintenance with a summary of the performed operations. Exceptional \"EMERGENCY\" maintenance \u00b6 Unscheduled downtimes can occur for any number of reasons, including: Loss of cooling and/or power in the data center. Loss of supporting infrastructure (i.e. hardware). Critical need to make changes to hardware or software that negatively impacts performance or access. Application of critical patches that can't wait until the next scheduled maintenance. For safety or security issues that require immediate action. We will try to notify users in the advent of such event by email. Danger The ULHPC team reserves the right to intervene in user activity without notice when such activity may destabilize the platform and/or is at the expense of other users, and/or to monitor/verify/debug ongoing system activity.","title":"Downtime and Maintenance"},{"location":"policies/maintenance/#maintenance-and-downtime-policy","text":"","title":"Maintenance and Downtime Policy"},{"location":"policies/maintenance/#scheduled-maintenance","text":"The ULHPC team will schedule maintenance in one of three manners: Rolling reboots Whenever possible, ULHPC will apply updates and do other maintenance in a rolling fashion in such a manner as to have either no or as little impact as possible to ULHPC services Partial outages We will do these as needed but in a manner that impacts only some ULHPC services at a time Full outages These are outages that will affect all ULHPC services, such as outages of core datacenter networking services, datacenter power of HVAC/cooling system maintenance or global GPFS/Spectrumscale filesystem updates . Such maintenance windows typically happen on a quarterly basis . It should be noted that we are not always able to anticipate when these outages are needed . ULHPC's goal for these downtimes is to have them completed as fast as possible. However, validation and qualification of the full platform takes typically one working day, and unforeseen or unusual circumstances may occur. So count for such outages a multiple-day downtime .","title":"Scheduled Maintenance"},{"location":"policies/maintenance/#notifications","text":"We normally inform users of cluster maintenance at least 3 weeks in advance by mail using the HPC User community mailing list (moderated): hpc-users@uni.lu . A second reminder is sent a few days prior to actual downtime. The news of the downtimes is also posted on the Live status page. Finally, a colored \" message of the day \" (motd) banner is displayed on all access/login servers such that you can quickly be informed of any incoming maintenance operation upon connection to the cluster. You can see this when you login or (again),any time by issuing the command: cat /etc/motd Detecting maintenance... During the maintenance During the maintenance period, access to the involved cluster access/login serveur is DENIED and any users still logged-in are disconnected at the beginning of the maintenance you will receive a written message in your terminal if for some reason during the maintenance you urgently need to collect data from your account, please contact the UL HPC Team by sending a mail to: hpc-team@uni.lu . We will notify you of the end of the maintenance with a summary of the performed operations.","title":"Notifications"},{"location":"policies/maintenance/#exceptional-emergency-maintenance","text":"Unscheduled downtimes can occur for any number of reasons, including: Loss of cooling and/or power in the data center. Loss of supporting infrastructure (i.e. hardware). Critical need to make changes to hardware or software that negatively impacts performance or access. Application of critical patches that can't wait until the next scheduled maintenance. For safety or security issues that require immediate action. We will try to notify users in the advent of such event by email. Danger The ULHPC team reserves the right to intervene in user activity without notice when such activity may destabilize the platform and/or is at the expense of other users, and/or to monitor/verify/debug ongoing system activity.","title":"Exceptional \"EMERGENCY\"  maintenance"},{"location":"policies/passwords/","text":"Password and Account Protection \u00b6 A user is given a username (also known as a login name) and associated password that permits her/him to access ULHPC resources. This username/password pair may be used by a single individual only: passwords must not be shared with any other person . Users who share their passwords will have their access to ULHPC disabled. Do not confuse your UL[HPC] password/passphrase and your SSH passphrase We sometimes receive requests to reset your SSH passphrase, which is something you control upon SSH key generation - see SSH documentation . Passwords must be changed as soon as possible after exposure or suspected compromise. Exposure of passwords and suspected compromises must immediately be reported to ULHPC and the University CISO (see below). In all cases, recommendations for the creation of strong passwords is proposed below . Password Manager \u00b6 You are strongly encouraged also to rely on password manager applications to store your different passwords. You may want to use your browser embedded solution but it's not the safest option. Here is a list of recommended applications: BitWarden - free with no limits ($10 per year for families) - Github Dashlane - free for up to 50 passwords - 40\u20ac per year for premium (60\u20ac for families) LastPass NordPass - free version limited to one device with unlimited number of passwords; 36$ per year for premium plan 1Password - paid version only (yet worth it) with 30-day free trial, 36$ per year (60$ for families) Self-Hosted solutions : KeepassXC pass : the Standard Unix Password Manager . Forgotten Passwords \u00b6 If you forget your password or if it has recently expired, you can simply contact us to initiate the process of resetting your password. Login Failures \u00b6 Your login privileges will be disabled if you have several login failures while entering your password on a ULHPC resource. You do not need a new password in this situation. The login failures will be automatically cleared after a couple of minutes. No additional actions are necessary. How To Change Your Password on IPA \u00b6 See IPA documentation Tip Passwords must be changed under any one of the following circumstances: Immediately after someone else has obtained your password (do NOT give your password to anyone else). As soon as possible, but at least within one business day after a password has been compromised or after you suspect that a password has been compromised. On direction from ULHPC staff, or by IPA password policy requesting to frequently change your password. Your new password must adhere to ULHPC's password requirements. Password Requirements and Guidelines \u00b6 One of the potentially weakest links in computer security is the individual password. Despite the University's and ULHPC's efforts to keep hackers out of your personal files and away from University resources (e.g., email, web files, licensed software), easily-guessed passwords are still a big problem so you should really pay attention to the following guidelines and recommendations. Recently, the National Institute of Standards and Technology (NIST) has updated their Digital Identity Guidelines in Special Publication 800-63B . We have updated our password policy to bring it in closer alignment with this guidelines. In particular, the updated guidance is counter to the long-held philosophy that passwords must be long and complex. In contrast, the new guidelines recommend that passwords should be \" easy to remember \" but \" hard to guess \", allowing for usability and security to go hand-in-hand. Inpired with other password policies and guidelines ( Stanford , NERSC ), ULHPC thus recommends the usage of \" pass phrases \" instead of passwords. Pass phrases are longer, but easier to remember than complex passwords, and if well-chosen can provide better protection against hackers. In addition, the following rules based on password length and usage of Multi-Factor Authentication (MFA) must be satisfied: The enforced minimum length for accounts with MFA enabled is 8 characters. If MFA is not enabled for your account the minimum password length is 14 characters. The ability to use all special characters according to the following guidelines (see also the Stanford Password Requirements Quick Guide ) depending on the password length: 8-11: mixed case letters, numbers, & symbols 12-15: mixed case letters & numbers 16-19: mixed case letters 20+: no restrictions illustrating image Restrict sequential and repetitive characters (e.g. 12345 or aaaaaa ) Restrict context specific passwords (e.g. the name of the site, etc.) Restrict commonly used passwords (e.g. p@ssw0rd , etc.) and dictionary words Restrict passwords obtained from previous breach corpuses Passwords must be changed every six months. If you are struggling to come up with a good password, you can inspire from the following approach: Creating a pass phrase (source: Stanford password policy ) A pass phrase is basically just a series of words, which can include spaces, that you employ instead of a single pass \"word.\" Pass phrases should be at least 16 to 25 characters in length (spaces count as characters), but no less. Longer is better because, though pass phrases look simple, the increased length provides so many possible permutations that a standard password-cracking program will not be effective. It is always a good thing to disguise that simplicity by throwing in elements of weirdness, nonsense, or randomness. Here, for example, are a couple pass phrase candidates: pizza with crispy spaniels mangled persimmon therapy Punctuate and capitalize your phrase: Pizza with crispy Spaniels! mangled Persimmon Therapy? Toss in a few numbers or symbols from the top row of the keyboard, plus some deliberately misspelled words, and you'll create an almost unguessable key to your account: Pizza w/ 6 krispy Spaniels! mangl3d Persimmon Th3rapy?","title":"Password Policy"},{"location":"policies/passwords/#password-and-account-protection","text":"A user is given a username (also known as a login name) and associated password that permits her/him to access ULHPC resources. This username/password pair may be used by a single individual only: passwords must not be shared with any other person . Users who share their passwords will have their access to ULHPC disabled. Do not confuse your UL[HPC] password/passphrase and your SSH passphrase We sometimes receive requests to reset your SSH passphrase, which is something you control upon SSH key generation - see SSH documentation . Passwords must be changed as soon as possible after exposure or suspected compromise. Exposure of passwords and suspected compromises must immediately be reported to ULHPC and the University CISO (see below). In all cases, recommendations for the creation of strong passwords is proposed below .","title":"Password and Account Protection"},{"location":"policies/passwords/#password-manager","text":"You are strongly encouraged also to rely on password manager applications to store your different passwords. You may want to use your browser embedded solution but it's not the safest option. Here is a list of recommended applications: BitWarden - free with no limits ($10 per year for families) - Github Dashlane - free for up to 50 passwords - 40\u20ac per year for premium (60\u20ac for families) LastPass NordPass - free version limited to one device with unlimited number of passwords; 36$ per year for premium plan 1Password - paid version only (yet worth it) with 30-day free trial, 36$ per year (60$ for families) Self-Hosted solutions : KeepassXC pass : the Standard Unix Password Manager .","title":"Password Manager"},{"location":"policies/passwords/#forgotten-passwords","text":"If you forget your password or if it has recently expired, you can simply contact us to initiate the process of resetting your password.","title":"Forgotten Passwords"},{"location":"policies/passwords/#login-failures","text":"Your login privileges will be disabled if you have several login failures while entering your password on a ULHPC resource. You do not need a new password in this situation. The login failures will be automatically cleared after a couple of minutes. No additional actions are necessary.","title":"Login Failures"},{"location":"policies/passwords/#how-to-change-your-password-on-ipa","text":"See IPA documentation Tip Passwords must be changed under any one of the following circumstances: Immediately after someone else has obtained your password (do NOT give your password to anyone else). As soon as possible, but at least within one business day after a password has been compromised or after you suspect that a password has been compromised. On direction from ULHPC staff, or by IPA password policy requesting to frequently change your password. Your new password must adhere to ULHPC's password requirements.","title":"How To Change Your Password on IPA"},{"location":"policies/passwords/#password-requirements-and-guidelines","text":"One of the potentially weakest links in computer security is the individual password. Despite the University's and ULHPC's efforts to keep hackers out of your personal files and away from University resources (e.g., email, web files, licensed software), easily-guessed passwords are still a big problem so you should really pay attention to the following guidelines and recommendations. Recently, the National Institute of Standards and Technology (NIST) has updated their Digital Identity Guidelines in Special Publication 800-63B . We have updated our password policy to bring it in closer alignment with this guidelines. In particular, the updated guidance is counter to the long-held philosophy that passwords must be long and complex. In contrast, the new guidelines recommend that passwords should be \" easy to remember \" but \" hard to guess \", allowing for usability and security to go hand-in-hand. Inpired with other password policies and guidelines ( Stanford , NERSC ), ULHPC thus recommends the usage of \" pass phrases \" instead of passwords. Pass phrases are longer, but easier to remember than complex passwords, and if well-chosen can provide better protection against hackers. In addition, the following rules based on password length and usage of Multi-Factor Authentication (MFA) must be satisfied: The enforced minimum length for accounts with MFA enabled is 8 characters. If MFA is not enabled for your account the minimum password length is 14 characters. The ability to use all special characters according to the following guidelines (see also the Stanford Password Requirements Quick Guide ) depending on the password length: 8-11: mixed case letters, numbers, & symbols 12-15: mixed case letters & numbers 16-19: mixed case letters 20+: no restrictions illustrating image Restrict sequential and repetitive characters (e.g. 12345 or aaaaaa ) Restrict context specific passwords (e.g. the name of the site, etc.) Restrict commonly used passwords (e.g. p@ssw0rd , etc.) and dictionary words Restrict passwords obtained from previous breach corpuses Passwords must be changed every six months. If you are struggling to come up with a good password, you can inspire from the following approach: Creating a pass phrase (source: Stanford password policy ) A pass phrase is basically just a series of words, which can include spaces, that you employ instead of a single pass \"word.\" Pass phrases should be at least 16 to 25 characters in length (spaces count as characters), but no less. Longer is better because, though pass phrases look simple, the increased length provides so many possible permutations that a standard password-cracking program will not be effective. It is always a good thing to disguise that simplicity by throwing in elements of weirdness, nonsense, or randomness. Here, for example, are a couple pass phrase candidates: pizza with crispy spaniels mangled persimmon therapy Punctuate and capitalize your phrase: Pizza with crispy Spaniels! mangled Persimmon Therapy? Toss in a few numbers or symbols from the top row of the keyboard, plus some deliberately misspelled words, and you'll create an almost unguessable key to your account: Pizza w/ 6 krispy Spaniels! mangl3d Persimmon Th3rapy?","title":"Password Requirements and Guidelines"},{"location":"policies/usage-charging/","text":"ULHPC Usage Charging Policy \u00b6 The advertised prices are for internal partners only The price list and all other information of this page are meant for internal partners, i.e., not for external companies. If you are not an internal partner, please contact us at hpc-partnership@uni.lu . Alternatively, you can contact LuxProvide , the national HPC center which aims at serving the private sector for HPC needs. How to estimate HPC costs for projects? \u00b6 You can use the following excel document to estimate the cost of your HPC usage: UL HPC Cost Estimates for Project Proposals [xlsx] Note that there are two sheets offering two ways to estimate based on your specific situation. Please read the red sections to ensure that you are using the correct estimation sheet. Note that even if you plan for large-scale experiments on PRACE/EuroHPC supercomputers through computing credits granted by Call for Proposals for Project Access , you should plan for ULHPC costs since you will have to demonstrate the scalability of your code -- the University's facility is ideal for that. You can contact hpc-partnership@uni.lu for more details about this. HPC price list - 2022-10-01 \u00b6 Note that ULHPC price list has been updated, see below. Compute \u00b6 Compute type Description \u20ac (excl. VAT) / node-hour CPU - small 28 cores, 128 GB RAM 0.25\u20ac CPU - regular 128 cores, 256 GB RAM 1.25\u20ac CPU - big mem 112 cores, 3 TB RAM 6.00\u20ac GPU 4 V100, 28 cores, 768 GB RAM 5.00\u20ac Storage \u00b6 Storage type \u20ac (excl. VAT) / GB / Month Additional information Home Free 500 GB Project 0.02\u20ac 1 TB free Scratch Free 10 TB HPC Resource allocation for UL internal R&D and training \u00b6 ULHPC resources are free of charge for UL staff for their internal work and training activities . Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back. Any other activities will be reviewed with the rectorate and are a priori subjected to be billed. Submit project related jobs \u00b6 To allow the ULHPC team to keep track of the jobs related to a project, use the -A <projectname> flag in Slurm, either in the Slurm directives preamble of your script, e.g., #SBATCH -A myproject or on the command line when you submit your job, e.g. , sbatch -A myproject /path/to/launcher.sh","title":"Usage Charging Policy"},{"location":"policies/usage-charging/#ulhpc-usage-charging-policy","text":"The advertised prices are for internal partners only The price list and all other information of this page are meant for internal partners, i.e., not for external companies. If you are not an internal partner, please contact us at hpc-partnership@uni.lu . Alternatively, you can contact LuxProvide , the national HPC center which aims at serving the private sector for HPC needs.","title":"ULHPC Usage Charging Policy"},{"location":"policies/usage-charging/#how-to-estimate-hpc-costs-for-projects","text":"You can use the following excel document to estimate the cost of your HPC usage: UL HPC Cost Estimates for Project Proposals [xlsx] Note that there are two sheets offering two ways to estimate based on your specific situation. Please read the red sections to ensure that you are using the correct estimation sheet. Note that even if you plan for large-scale experiments on PRACE/EuroHPC supercomputers through computing credits granted by Call for Proposals for Project Access , you should plan for ULHPC costs since you will have to demonstrate the scalability of your code -- the University's facility is ideal for that. You can contact hpc-partnership@uni.lu for more details about this.","title":"How to estimate HPC costs for projects?"},{"location":"policies/usage-charging/#hpc-price-list-2022-10-01","text":"Note that ULHPC price list has been updated, see below.","title":"HPC price list - 2022-10-01"},{"location":"policies/usage-charging/#compute","text":"Compute type Description \u20ac (excl. VAT) / node-hour CPU - small 28 cores, 128 GB RAM 0.25\u20ac CPU - regular 128 cores, 256 GB RAM 1.25\u20ac CPU - big mem 112 cores, 3 TB RAM 6.00\u20ac GPU 4 V100, 28 cores, 768 GB RAM 5.00\u20ac","title":"Compute"},{"location":"policies/usage-charging/#storage","text":"Storage type \u20ac (excl. VAT) / GB / Month Additional information Home Free 500 GB Project 0.02\u20ac 1 TB free Scratch Free 10 TB","title":"Storage"},{"location":"policies/usage-charging/#hpc-resource-allocation-for-ul-internal-rd-and-training","text":"ULHPC resources are free of charge for UL staff for their internal work and training activities . Principal Investigators (PI) will nevertheless receive on a regular basis a usage report of their team activities on the UL HPC platform. The corresponding accumulated price will be provided even if this amount is purely indicative and won't be charged back. Any other activities will be reviewed with the rectorate and are a priori subjected to be billed.","title":"HPC Resource allocation for UL internal R&amp;D and training"},{"location":"policies/usage-charging/#submit-project-related-jobs","text":"To allow the ULHPC team to keep track of the jobs related to a project, use the -A <projectname> flag in Slurm, either in the Slurm directives preamble of your script, e.g., #SBATCH -A myproject or on the command line when you submit your job, e.g. , sbatch -A myproject /path/to/launcher.sh","title":"Submit project related jobs"},{"location":"services/","text":"Services \u00b6 The ULHPC Team is committed to excellence and support of the University research community through several side services: ULHPC Gitlab , a comprehensive version control and collaboration (VC&C) solution to deliver better software faster. Etherpad - a web-based collaborative real-time editor Privatebin - secured textual data sharing Gitlab @ Uni.lu \u00b6 Gitlab is an open source software to collaborate on code, very similar to Github . You can manage git repositories with fine grained access controls that keep your code secure and perform code reviews and enhance collaboration with merge requests. Each project can also have an issue tracker and a wiki. The GitLab service is available for UL HPC platform users with their ULHPC account and to their external collaborators that have a GitHub account. [Github] External accounts access are BLOCKED by default By default, external (github) accounts are denied and blocked on the Gitlab service. Access can be granted on-demand after careful review of the ULHPC team and attached to the project indicated by the UL[HPC] PI in charge of the external. Note : externals cannot create groups nor projects. EtherPad \u00b6 Etherpad is a web-based collaborative real-time editor, allowing authors to simultaneously edit a text document, and see all of the participants' edits in real-time, with the ability to display each author's text in their own color. PrivateBin \u00b6 PrivateBin is a minimalist, open source online pastebin where the server has zero knowledge of pasted data. Data is encrypted and decrypted in the browser using 256bit AES in Galois Counter mode. Gforge @ Uni.lu (DEPRECATED) \u00b6 Outside Gitlab , the seminal web-based project management and collaboration system deployed at the university was the Gforge @ Uni.lu service. Inspired by the InriaGforge , the general principle is to offer easy access through projects to subversion repositories, mailing lists, bug trackers, message boards/forums, task management, site hosting, permanent file archival, full backups, and total web-based administration. The major features are listed in helpdesk website . Access to GForge is through dedicated accounts, (requested by mail: admin@gforge.uni.lu ), for both UL members and external partners. Service feature comparison between GForge ( gforge.uni.lu ) and GitLab ( gitlab.uni.lu ) Service Git SVN/Subversion ULHPC users External Users Static websites hosting GForge GitLab (restricted use) Gforge service is decommissioned since 2020 Situation : the Gforge @ Uni.lu service has been in production since 2008 and kept up-to-date until now. Nevertheless, the underground product line (Gforge, moved to FusionForge after the a break-up of the original open source project in february 2009) proved to be hard to maintained, security fixes takes time to be integrated and the few unique features of the service (SVN support, static website hosting) no longer justify the maintenance effort as more recent and sustainable alternatives emerged. For this reason, the Gforge service will be no longer available as of Dec 31, 2020 Migration Plan for project repositories For SVN-based projects: Migrate to Git Favor public hosting services for your project as github.com Github offers both private and public projects management free of charge Github also supports SVN clients consider self-hosted solutions on servers you can maintain as gitolite or gitea As a last resort, consider using one of the Gitlab instances running in the different department of the university, or the ULHPC gitlab You can then update the Git remote url to transition to the new host for your git project as follows: git remote set-url origin [ ... ] This will allow you to keep all commit history . Nevertheless, consider that all other information part of your Gforge project (tickets, project and forum messages etc.) will be lost once Gforge is declared out-of-service. Alternative to Gforge features Git repository hosting : see above Project Management/Bug Tracking : see above as all proposed alternatives ( Github etc.) supports this feature SVN repository hosting : if you really need to stick to SVN, consider RhodeCode or Assembla Mailing list : The IT service of the university can grant you administration rights on a custom mailing-list upon demand of the service portal Static website hosting : if you really need to maintain a static website, consider: Github pages Read the docs Ask the University IT service for a website host","title":"Services"},{"location":"services/#services","text":"The ULHPC Team is committed to excellence and support of the University research community through several side services: ULHPC Gitlab , a comprehensive version control and collaboration (VC&C) solution to deliver better software faster. Etherpad - a web-based collaborative real-time editor Privatebin - secured textual data sharing","title":"Services"},{"location":"services/#gitlab-unilu","text":"Gitlab is an open source software to collaborate on code, very similar to Github . You can manage git repositories with fine grained access controls that keep your code secure and perform code reviews and enhance collaboration with merge requests. Each project can also have an issue tracker and a wiki. The GitLab service is available for UL HPC platform users with their ULHPC account and to their external collaborators that have a GitHub account. [Github] External accounts access are BLOCKED by default By default, external (github) accounts are denied and blocked on the Gitlab service. Access can be granted on-demand after careful review of the ULHPC team and attached to the project indicated by the UL[HPC] PI in charge of the external. Note : externals cannot create groups nor projects.","title":"Gitlab @ Uni.lu"},{"location":"services/#etherpad","text":"Etherpad is a web-based collaborative real-time editor, allowing authors to simultaneously edit a text document, and see all of the participants' edits in real-time, with the ability to display each author's text in their own color.","title":"EtherPad"},{"location":"services/#privatebin","text":"PrivateBin is a minimalist, open source online pastebin where the server has zero knowledge of pasted data. Data is encrypted and decrypted in the browser using 256bit AES in Galois Counter mode.","title":"PrivateBin"},{"location":"services/#gforge-unilu-deprecated","text":"Outside Gitlab , the seminal web-based project management and collaboration system deployed at the university was the Gforge @ Uni.lu service. Inspired by the InriaGforge , the general principle is to offer easy access through projects to subversion repositories, mailing lists, bug trackers, message boards/forums, task management, site hosting, permanent file archival, full backups, and total web-based administration. The major features are listed in helpdesk website . Access to GForge is through dedicated accounts, (requested by mail: admin@gforge.uni.lu ), for both UL members and external partners. Service feature comparison between GForge ( gforge.uni.lu ) and GitLab ( gitlab.uni.lu ) Service Git SVN/Subversion ULHPC users External Users Static websites hosting GForge GitLab (restricted use) Gforge service is decommissioned since 2020 Situation : the Gforge @ Uni.lu service has been in production since 2008 and kept up-to-date until now. Nevertheless, the underground product line (Gforge, moved to FusionForge after the a break-up of the original open source project in february 2009) proved to be hard to maintained, security fixes takes time to be integrated and the few unique features of the service (SVN support, static website hosting) no longer justify the maintenance effort as more recent and sustainable alternatives emerged. For this reason, the Gforge service will be no longer available as of Dec 31, 2020 Migration Plan for project repositories For SVN-based projects: Migrate to Git Favor public hosting services for your project as github.com Github offers both private and public projects management free of charge Github also supports SVN clients consider self-hosted solutions on servers you can maintain as gitolite or gitea As a last resort, consider using one of the Gitlab instances running in the different department of the university, or the ULHPC gitlab You can then update the Git remote url to transition to the new host for your git project as follows: git remote set-url origin [ ... ] This will allow you to keep all commit history . Nevertheless, consider that all other information part of your Gforge project (tickets, project and forum messages etc.) will be lost once Gforge is declared out-of-service. Alternative to Gforge features Git repository hosting : see above Project Management/Bug Tracking : see above as all proposed alternatives ( Github etc.) supports this feature SVN repository hosting : if you really need to stick to SVN, consider RhodeCode or Assembla Mailing list : The IT service of the university can grant you administration rights on a custom mailing-list upon demand of the service portal Static website hosting : if you really need to maintain a static website, consider: Github pages Read the docs Ask the University IT service for a website host","title":"Gforge @ Uni.lu (DEPRECATED)"},{"location":"services/jupyter/","text":"Jupyter Notebook \u00b6 Jupyter is a flexible, popular literate-computing web application for creating notebooks containing code, equations, visualization, and text. Notebooks are documents that contain both computer code and rich text elements (paragraphs, equations, figures, widgets, links). They are human-readable documents containing analysis descriptions and results but are also executable data analytics artifacts. Notebooks are associated with kernels, processes that actually execute code. Notebooks can be shared or converted into static HTML documents. They are a powerful tool for reproducible research and teaching. Install Jupyter \u00b6 While Jupyter runs code in many programming languages, Python is a requirement (Python 3.3 or greater, or Python 2.7) for installing the Jupyter Notebook. New users may wish to use Anaconda or conda to install Jupyter. Hereafter, the pip package manager will be used to install Jupyter. We strongly recommend to use the Python module provided by the ULHPC and installing jupyter inside a virtualenv after upgrading pip . $ si $ module load lang/Python #Loading default Python $ python -m venv jupyter_env $ source jupyter_env/bin/activate $ python -m pip install --upgrade pip $ python -m pip install jupyter ipykernel Warning Modules are not allowed on the access servers. To test interactively Singularity, remember to ask for an interactive job first using for instance the si tool. Once Jupyter is installed along with IPython , you can start to configure your installation setting the environment variables corresponding to your needs: JUPYTER_CONFIG_DIR : Set this environment variable to use a particular directory, other than the default, for Jupyter config files JUPYTER_PATH : Set this environment variable to provide extra directories for the data search path. JUPYTER_PATH should contain a series of directories, separated by os.pathsep(; on Windows, : on Unix). Directories given in JUPYTER_PATH are searched before other locations. This is used in addition to other entries, rather than replacing any JUPYTER_DATA_DIR : Set this environment variable to use a particular directory, other than the default, as the user data directory JUPYTER_RUNTIME_DIR : Set this to override where Jupyter stores runtime files IPYTHONDIR : If set, this environment variable should be the path to a directory, which IPython will use for user data. IPython will create it if it does not exist. Jupyter Notebook makes sure that the IPython kernel is available, but you have to manually add a kernel with a different version of Python or a virtual environment. Register the kernel using the following command: python -m ipykernel install --sys-prefix --name jupyter_env Jupyter and your virtualenv are now installed and ready. Starting a Jupyter Notebook \u00b6 Jupyter notebooks can be started as a slurm job . The following script is an example how to proceed: Slurm Launcher script for Jupyter Notebook #!/bin/bash -l #SBATCH -J Jupyter #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 2 # Cores assigned to each tasks #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" # Python 3.X by default (also on system) module load lang/Python source jupyter_env/bin/activate jupyter notebook --ip $( facter ipaddress ) --no-browser & pid = $! sleep 5s jupyter notebook list jupyter --paths jupyter kernelspec list echo \"Enter this command on your laptop: ssh -p 8022 -NL 8888: $( facter ipaddress ) :8888 ${ USER } @access-iris.uni.lu \" > notebook.log wait $pid Once your job is running (see Joining/monitoring running jobs , you can use ssh forwarding to connect to the notebook from your laptop. Open a terminal on your laptop and copy-paste the ssh command in the file notebook.log . You should be now able to reach your notebook. Then open your browser and go to the url: http://127.0.0.1:8888/ . Jupyter should ask you for a password (see screenshot below). This password can be set before running the jupyter notebook and his part of the initial configuartion detailed at Jupyter official documentation . if by mistake, you forgot to setup this password, have a look in the slurm-****.out file in which the output of the command jupyter notebook list has been recorded. >$ cat slurm-2152135.out Currently running servers: config: /mnt/irisgpfs/users/ekieffer/.jupyter /mnt/irisgpfs/users/ekieffer/jupyter_env/etc/jupyter /usr/local/etc/jupyter /etc/jupyter data: /home/users/ekieffer/.local/share/jupyter /mnt/irisgpfs/users/ekieffer/jupyter_env/share/jupyter /usr/local/share/jupyter /usr/share/jupyter runtime: /home/users/ekieffer/.local/share/jupyter/runtime Available kernels: jupyter_env /home/users/ekieffer/.local/share/jupyter/kernels/jupyter_env python3 /home/users/ekieffer/.local/share/jupyter/kernels/python3 venv /home/users/ekieffer/.local/share/jupyter/kernels/venv [ I 15 :15:42.682 NotebookApp ] Serving notebooks from local directory: /mnt/irisgpfs/users/ekieffer [ I 15 :15:42.682 NotebookApp ] Jupyter Notebook 6 .1.5 is running at: [ I 15 :15:42.682 NotebookApp ] http://172.17.6.75:8888/?token = 5e976373fcb84e9be7796c35a17d232eb594a7b0bb6647a1 [ I 15 :15:42.682 NotebookApp ] or http://127.0.0.1:8888/?token = 5e976373fcb84e9be7796c35a17d232eb594a7b0bb6647a1 [ I 15 :15:42.682 NotebookApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 15 :15:42.697 NotebookApp ] To access the notebook, open this file in a browser: file:///home/users/ekieffer/.local/share/jupyter/runtime/nbserver-21681-open.html Or copy and paste one of these URLs: http://172.17.6.75:8888/?token = 5e976373fcb84e9be7796c35a17d232eb594a7b0bb6647a1 or http://127.0.0.1:8888/?token = 5e976373fcb84e9be7796c35a17d232eb594a7b0bb6647a1 [ I 15 :36:47.512 NotebookApp ] 302 GET / ( 172 .17.2.11 ) 0 .88ms [ I 15 :36:47.572 NotebookApp ] 302 GET /tree? ( 172 .17.2.11 ) 1 .21ms Jupyter provides you a token to connect to the notebook. You can also notice the available kernels and more specifically the jupyter_env. Warning Do not forget to click on the quit button when finished to stop the jupyter server and release the ressources.","title":"Jupyter Notebook"},{"location":"services/jupyter/#jupyter-notebook","text":"Jupyter is a flexible, popular literate-computing web application for creating notebooks containing code, equations, visualization, and text. Notebooks are documents that contain both computer code and rich text elements (paragraphs, equations, figures, widgets, links). They are human-readable documents containing analysis descriptions and results but are also executable data analytics artifacts. Notebooks are associated with kernels, processes that actually execute code. Notebooks can be shared or converted into static HTML documents. They are a powerful tool for reproducible research and teaching.","title":"Jupyter Notebook"},{"location":"services/jupyter/#install-jupyter","text":"While Jupyter runs code in many programming languages, Python is a requirement (Python 3.3 or greater, or Python 2.7) for installing the Jupyter Notebook. New users may wish to use Anaconda or conda to install Jupyter. Hereafter, the pip package manager will be used to install Jupyter. We strongly recommend to use the Python module provided by the ULHPC and installing jupyter inside a virtualenv after upgrading pip . $ si $ module load lang/Python #Loading default Python $ python -m venv jupyter_env $ source jupyter_env/bin/activate $ python -m pip install --upgrade pip $ python -m pip install jupyter ipykernel Warning Modules are not allowed on the access servers. To test interactively Singularity, remember to ask for an interactive job first using for instance the si tool. Once Jupyter is installed along with IPython , you can start to configure your installation setting the environment variables corresponding to your needs: JUPYTER_CONFIG_DIR : Set this environment variable to use a particular directory, other than the default, for Jupyter config files JUPYTER_PATH : Set this environment variable to provide extra directories for the data search path. JUPYTER_PATH should contain a series of directories, separated by os.pathsep(; on Windows, : on Unix). Directories given in JUPYTER_PATH are searched before other locations. This is used in addition to other entries, rather than replacing any JUPYTER_DATA_DIR : Set this environment variable to use a particular directory, other than the default, as the user data directory JUPYTER_RUNTIME_DIR : Set this to override where Jupyter stores runtime files IPYTHONDIR : If set, this environment variable should be the path to a directory, which IPython will use for user data. IPython will create it if it does not exist. Jupyter Notebook makes sure that the IPython kernel is available, but you have to manually add a kernel with a different version of Python or a virtual environment. Register the kernel using the following command: python -m ipykernel install --sys-prefix --name jupyter_env Jupyter and your virtualenv are now installed and ready.","title":"Install Jupyter"},{"location":"services/jupyter/#starting-a-jupyter-notebook","text":"Jupyter notebooks can be started as a slurm job . The following script is an example how to proceed: Slurm Launcher script for Jupyter Notebook #!/bin/bash -l #SBATCH -J Jupyter #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 2 # Cores assigned to each tasks #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" # Python 3.X by default (also on system) module load lang/Python source jupyter_env/bin/activate jupyter notebook --ip $( facter ipaddress ) --no-browser & pid = $! sleep 5s jupyter notebook list jupyter --paths jupyter kernelspec list echo \"Enter this command on your laptop: ssh -p 8022 -NL 8888: $( facter ipaddress ) :8888 ${ USER } @access-iris.uni.lu \" > notebook.log wait $pid Once your job is running (see Joining/monitoring running jobs , you can use ssh forwarding to connect to the notebook from your laptop. Open a terminal on your laptop and copy-paste the ssh command in the file notebook.log . You should be now able to reach your notebook. Then open your browser and go to the url: http://127.0.0.1:8888/ . Jupyter should ask you for a password (see screenshot below). This password can be set before running the jupyter notebook and his part of the initial configuartion detailed at Jupyter official documentation . if by mistake, you forgot to setup this password, have a look in the slurm-****.out file in which the output of the command jupyter notebook list has been recorded. >$ cat slurm-2152135.out Currently running servers: config: /mnt/irisgpfs/users/ekieffer/.jupyter /mnt/irisgpfs/users/ekieffer/jupyter_env/etc/jupyter /usr/local/etc/jupyter /etc/jupyter data: /home/users/ekieffer/.local/share/jupyter /mnt/irisgpfs/users/ekieffer/jupyter_env/share/jupyter /usr/local/share/jupyter /usr/share/jupyter runtime: /home/users/ekieffer/.local/share/jupyter/runtime Available kernels: jupyter_env /home/users/ekieffer/.local/share/jupyter/kernels/jupyter_env python3 /home/users/ekieffer/.local/share/jupyter/kernels/python3 venv /home/users/ekieffer/.local/share/jupyter/kernels/venv [ I 15 :15:42.682 NotebookApp ] Serving notebooks from local directory: /mnt/irisgpfs/users/ekieffer [ I 15 :15:42.682 NotebookApp ] Jupyter Notebook 6 .1.5 is running at: [ I 15 :15:42.682 NotebookApp ] http://172.17.6.75:8888/?token = 5e976373fcb84e9be7796c35a17d232eb594a7b0bb6647a1 [ I 15 :15:42.682 NotebookApp ] or http://127.0.0.1:8888/?token = 5e976373fcb84e9be7796c35a17d232eb594a7b0bb6647a1 [ I 15 :15:42.682 NotebookApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 15 :15:42.697 NotebookApp ] To access the notebook, open this file in a browser: file:///home/users/ekieffer/.local/share/jupyter/runtime/nbserver-21681-open.html Or copy and paste one of these URLs: http://172.17.6.75:8888/?token = 5e976373fcb84e9be7796c35a17d232eb594a7b0bb6647a1 or http://127.0.0.1:8888/?token = 5e976373fcb84e9be7796c35a17d232eb594a7b0bb6647a1 [ I 15 :36:47.512 NotebookApp ] 302 GET / ( 172 .17.2.11 ) 0 .88ms [ I 15 :36:47.572 NotebookApp ] 302 GET /tree? ( 172 .17.2.11 ) 1 .21ms Jupyter provides you a token to connect to the notebook. You can also notice the available kernels and more specifically the jupyter_env. Warning Do not forget to click on the quit button when finished to stop the jupyter server and release the ressources.","title":"Starting a Jupyter Notebook"},{"location":"slurm/","text":"Slurm Resource and Job Management System \u00b6 ULHPC uses Slurm ( Simple Linux Utility for Resource Management ) for cluster/resource management and job scheduling. This middleware is responsible for allocating resources to users, providing a framework for starting, executing and monitoring work on allocated resources and scheduling work for future execution. Official docs Official FAQ ULHPC Tutorial/Getting Started IEEE ISPDC22: ULHPC Slurm 2.0 If you want more details on the RJMS optimizations performed upon Aion acquisition, check out our IEEE ISPDC22 conference paper (21 st IEEE Int. Symp. on Parallel and Distributed Computing) presented in Basel (Switzerland) on July 13, 2022. IEEE Reference Format | ORBilu entry | ULHPC blog post | slides Sebastien Varrette, Emmanuel Kieffer, and Frederic Pinel, \"Optimizing the Resource and Job Management System of an Academic HPC and Research Computing Facility\". In 21 st IEEE Intl. Symp. on Parallel and Distributed Computing (ISPDC\u201922) , Basel, Switzerland, 2022. TL;DR Slurm on ULHPC clusters \u00b6 In its concise form, the Slurm configuration in place on ULHPC supercomputers features the following attributes you should be aware of when interacting with it: Predefined Queues/Partitions depending on node type batch (Default Dual-CPU nodes) Max : 64 nodes, 2 days walltime gpu (GPU nodes nodes) Max : 4 nodes, 2 days walltime bigmem (Large-Memory nodes) Max : 1 node, 2 days walltime In addition: interactive (for quicks tests) Max : 2 nodes, 2h walltime for code development, testing, and debugging Queue Policy: cross-partition QOS , mainly tied to priority level ( low \\rightarrow \\rightarrow urgent ) long QOS with extended Max walltime ( MaxWall ) set to 14 days special preemptible QOS for best-effort jobs: besteffort . Accounts hierarchy associated to supervisors (multiple associations possible), projects or trainings you MUST use the proper account as a detailed usage tracking is performed and reported. Slurm Federation configuration between iris and aion ensures global policy (coherent job ID, global scheduling, etc.) within ULHPC systems easily submit jobs from one cluster to another using -M, --cluster aion|iris For more details, see the appropriate pages in the left menu (or the above conference paper ). Jobs \u00b6 A job is an allocation of resources such as compute nodes assigned to a user for an certain amount of time. Jobs can be interactive or passive (e.g., a batch script) scheduled for later execution. What characterize a job? A user jobs have the following key characteristics: set of requested resources: number of computing resources: nodes (including all their CPUs and cores) or CPUs (including all their cores) or cores amount of memory : either per node or per CPU (wall)time needed for the users tasks to complete their work a requested node partition (job queue) a requested quality of service (QoS) level which grants users specific accesses a requested account for accounting purposes Once a job is assigned a set of nodes, the user is able to initiate parallel work in the form of job steps (sets of tasks) in any configuration within the allocation. When you login to a ULHPC system you land on a access/login node . Login nodes are only for editing and preparing jobs: They are not meant for actually running jobs. From the login node you can interact with Slurm to submit job scripts or start interactive jobs, which will be further run on the compute nodes. Submit Jobs \u00b6 There are three ways of submitting jobs with slurm, using either sbatch , srun or salloc : sbatch (passive job) ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly sbatch -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] <path/to/launcher.sh> srun (interactive job) ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly srun -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] ---pty bash srun is also to be using within your launcher script to initiate a job step . salloc (request allocation/interactive job) # Request interactive jobs/allocations ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly salloc -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] <command> sbatch \u00b6 sbatch is used to submit a batch launcher script for later execution, corresponding to batch/passive submission mode . The script will typically contain one or more srun commands to launch parallel tasks. Upon submission with sbatch , Slurm will: allocate resources (nodes, tasks, partition, constraints, etc.) runs a single copy of the batch script on the first allocated node in particular, if you depend on other scripts, ensure you have refer to them with the complete path toward them. When you submit the job, Slurm responds with the job's ID, which will be used to identify this job in reports from Slurm. # /!\\ ADAPT path to launcher accordingly $ sbatch <path/to/launcher>.sh Submitted batch job 864933 srun \u00b6 srun is used to initiate parallel job steps within a job OR to start an interactive job Upon submission with srun , Slurm will: ( eventually ) allocate resources (nodes, tasks, partition, constraints, etc.) when run for interactive submission launch a job step that will execute on the allocated resources. A job can contain multiple job steps executing sequentially or in parallel on independent or shared resources within the job's node allocation. salloc \u00b6 salloc is used to allocate resources for a job in real time. Typically this is used to allocate resources (nodes, tasks, partition, etc.) and spawn a shell. The shell is then used to execute srun commands to launch parallel tasks. Specific Resource Allocation \u00b6 Within a job, you aim at running a certain number of tasks , and Slurm allow for a fine-grain control of the resource allocation that must be satisfied for each task. Beware of Slurm terminology in Multicore Architecture ! Slurm Node = Physical node , specified with -N <#nodes> Advice : always explicit number of expected number of tasks per node using --ntasks-per-node <n> . This way you control the node footprint of your job. Slurm Socket = Physical Socket/CPU/Processor Advice : if possible, explicit also the number of expected number of tasks per socket (processor) using --ntasks-per-socket <s> . relations between <s> and <n> must be aligned with the physical NUMA characteristics of the node. For instance on aion nodes, <n> = 8*<s> For instance on iris regular nodes, <n>=2*<s> when on iris bigmem nodes, <n>=4*<s> . ( the most confusing ): Slurm CPU = Physical CORE use -c <#threads> to specify the number of cores reserved per task. Hyper-Threading (HT) Technology is disabled on all ULHPC compute nodes. In particular: assume #cores = #threads , thus when using -c <threads> , you can safely set OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } # Default to 1 if SLURM_CPUS_PER_TASK not set to automatically abstract from the job context you have interest to match the physical NUMA characteristics of the compute node you're running at (Ex: target 16 threads per socket on Aion nodes (as there are 8 virtual sockets per nodes, 14 threads per socket on Iris regular nodes). The total number of tasks defined in a given job is stored in the $SLURM_NTASKS environment variable. This is very convenient to abstract from the job context to run MPI tasks/processes in parallel using for instance: srun -n ${ SLURM_NTASKS } [ ... ] We encourage you to always explicitly specify upon resource allocation the number of tasks you want per node/socket ( --ntasks-per-node <n> --ntasks-per-socket <s> ), to easily scale on multiple nodes with -N <N> . Adapt the number of threads and the settings to match the physical NUMA characteristics of the nodes Aion 16 cores per socket and 8 (virtual) sockets (CPUs) per aion node. {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <8n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 8\\times \\times 8\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 16 Ex: -N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4 ( Total : 64 tasks) Iris (default Dual-CPU) 14 cores per socket and 2 sockets (physical CPUs) per regular iris . {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <2n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 2\\times \\times 2\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 14 Ex: -N 2 --ntasks-per-node 4 --ntasks-per-socket 2 -c 7 ( Total : 8 tasks) Iris (Bigmem) 28 cores per socket and 4 sockets (physical CPUs) per bigmem iris {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <4n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 4\\times \\times 4\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 28 Ex: -N 2 --ntasks-per-node 8 --ntasks-per-socket 2 -c 14 ( Total : 16 tasks) Job submission options \u00b6 There are several useful environment variables set be Slurm within an allocated job. The most important ones are detailed in the below table which summarizes the main job submission options offered with {sbatch | srun | salloc} [...] : Command-line option Description Example -N <N> <N> Nodes request -N 2 --ntasks-per-node=<n> <n> Tasks-per-node request --ntasks-per-node=28 --ntasks-per-socket=<s> <s> Tasks-per-socket request --ntasks-per-socket=14 -c <c> <c> Cores-per-task request (multithreading) -c 1 --mem=<m>GB <m> GB memory per node request --mem 0 -t [DD-]HH[:MM:SS]> Walltime request -t 4:00:00 -G <gpu> <gpu> GPU(s) request -G 4 -C <feature> Feature request ( broadwell,skylake... ) -C skylake -p <partition> Specify job partition/queue --qos <qos> Specify job qos -A <account> Specify account -J <name> Job name -J MyApp -d <specification> Job dependency -d singleton --mail-user=<email> Specify email address --mail-type=<type> Notify user by email when certain event types occur. --mail-type=END,FAIL At a minimum a job submission script must include number of nodes, time, type of partition and nodes (resource allocation constraint and features), and quality of service (QOS). If a script does not specify any of these options then a default may be applied. The full list of directives is documented in the man pages for the sbatch command (see. man sbatch ). #SBATCH directives vs. CLI options \u00b6 Each option can be specified either as an #SBATCH [...] directive in the job submission script: #!/bin/bash -l # <--- DO NOT FORGET '-l' ### Request a single task using one core on one node for 5 minutes in the batch queue #SBATCH -N 2 #SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --time=0-00:05:00 #SBATCH -p batch # [...] Or as a command line option when submitting the script: $ sbatch -p batch -N 2 --ntasks-per-node = 1 -c 1 --time = 0 -00:05:00 ./first-job.sh The command line and directive versions of an option are equivalent and interchangeable : if the same option is present both on the command line and as a directive, the command line will be honored. If the same option or directive is specified twice, the last value supplied will be used. Also, many options have both a long form, eg --nodes=2 and a short form, eg -N 2 . These are equivalent and interchangable. Common options to sbatch and srun Many options are common to both sbatch and srun , for example sbatch -N 4 ./first-job.sh allocates 4 nodes to first-job.sh , and srun -N 4 uname -n inside the job runs a copy of uname -n on each of 4 nodes. If you don't specify an option in the srun command line, srun will inherit the value of that option from sbatch . In these cases the default behavior of srun is to assume the same options as were passed to sbatch . This is achieved via environment variables: sbatch sets a number of environment variables with names like SLURM_NNODES and srun checks the values of those variables. This has two important consequences: Your job script can see the settings it was submitted with by checking these environment variables You should NOT override these environment variables. Also be aware that if your job script tries to do certain tricky things, such as using ssh to launch a command on another node, the environment might not be propagated and your job may not behave correctly HW characteristics and Slurm features of ULHPC nodes \u00b6 When selecting specific resources allocations, it is crucial to match the hardware characteristics of the computing nodes. Details are provided below: Node (type) #Nodes #Socket / #Cores RAM [GB] Features aion-[0001-0318] 318 8 / 128 256 batch,epyc iris-[001-108] 108 2 / 28 128 batch,broadwell iris-[109-168] 60 2 / 28 128 batch,skylake iris-[169-186] (GPU) 18 2 / 28 768 gpu,skylake,volta iris-[191-196] (GPU) 6 2 / 28 768 gpu,skylake,volta32 iris-[187-190] (Large-Memory) 4 4 / 112 3072 bigmem,skylake As can be seen, Slurm [features] are associated to ULHPC compute nodes and permits to easily filter with the -C <feature> option the list of nodes. To list available features, use sfeatures : sfeatures # sinfo -o '%20N %.6D %.6c %15F %12P %f' # NODELIST NODES CPUS NODES(A/I/O/T) PARTITION AVAIL_FEATURES # [...] Always try to align resource specifications for your jobs with physical characteristics The typical format of your Slurm submission should thus probably be: sbatch|srun|... [-N <N>] --ntasks-per-node <n> -c <thread> [...] sbatch|srun|... [-N <N>] --ntasks-per-node <#sockets * s> --ntasks-per-socket <s> -c <thread> [...] This would define a total of <N> \\times \\times <n> TASKS (first form) or <N> \\times \\#sockets \\times \\times \\#sockets \\times <s> TASKS (second form), each on <thread> threads . You MUST ensure that either: <n> \\times \\times <thread> matches the number of cores avaiable on the target computing node (first form), or <n> = \\#sockets \\times \\#sockets \\times <s> , and <s> \\times \\times <thread> matches the number of cores per socket available on the target computing node (second form). Aion (default Dual-CPU) 16 cores per socket and 8 virtual sockets (CPUs) per aion node. Depending on the selected form, you MUST ensure that either <n> \\times \\times <thread> =128, or that <n> =8 <s> and <s> \\times \\times <thread> =16. ### Example 1 - use all cores available { sbatch | srun | salloc } -N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4 [ ... ] # Total: 64 tasks (spread across 2 nodes), each on 4 cores/threads ### Example 2 - use all cores available { sbatch | srun | salloc } --ntasks-per-node 128 -c 1 [ ... ] # Total; 128 (single-core) tasks ### Example 3 - use all cores available { sbatch | srun | salloc } -N 1 --ntasks-per-node 8 --ntasks-per-socket 1 -c 16 [ ... ] # Total: 8 tasks, each on 16 cores/threads ### Example 4 - use all cores available { sbatch | srun | salloc } -N 1 --ntasks-per-node 2 -c 64 [ ... ] # Total: 2 tasks, each on 64 cores/threads Iris (default Dual-CPU) 14 cores per socket and 2 sockets (physical CPUs) per regular iris node. Depending on the selected form, you MUST ensure that either <n> \\times \\times <thread> =28, or that <n> =2 <s> and <s> \\times \\times <thread> =14. ### Example 1 - use all cores available { sbatch | srun | salloc } -N 3 --ntasks-per-node 14 --ntasks-per-socket 7 -c 2 [ ... ] # Total: 42 tasks (spread across 3 nodes), each on 2 cores/threads ### Example 2 - use all cores available { sbatch | srun | salloc } -N 2 --ntasks-per-node 28 -c 1 [ ... ] # Total; 56 (single-core) tasks ### Example 3 - use all cores available { sbatch | srun | salloc } -N 2 --ntasks-per-node 2 --ntasks-per-socket 1 -c 14 [ ... ] # Total: 4 tasks (spread across 2 nodes), each on 14 cores/threads Iris (Large-Memory) 28 cores per socket and 4 sockets (physical CPUs) per bigmem iris node. Depending on the selected form, you MUST ensure that either <n> \\times \\times <thread> =112, or that <n> =4 <s> and <s> \\times \\times <thread> =28. ### Example 1 - use all cores available { sbatch | srun | salloc } -N 1 --ntasks-per-node 56 --ntasks-per-socket 14 -c 2 [ ... ] # Total: 56 tasks on a single bigmem node, each on 2 cores/threads ### Example 2 - use all cores available { sbatch | srun | salloc } --ntasks-per-node 112 -c 1 [ ... ] # Total; 112 (single-core) tasks ### Example 3 - use all cores available { sbatch | srun | salloc } -N 1 --ntasks-per-node 4 --ntasks-per-socket 1 -c 28 [ ... ] # Total: 4 tasks, each on 28 cores/threads Using Slurm Environment variables \u00b6 Recall that the Slurm controller will set several SLURM_* variables in the environment of the batch script. The most important are listed in the table below - use them wisely to make your launcher script as flexible as possible to abstract and adapt from the allocation context, \" independently \" of the way the job script has been submitted. Submission option Environment variable Typical usage -N <N> SLURM_JOB_NUM_NODES or SLURM_NNODES --ntasks-per-node=<n> SLURM_NTASKS_PER_NODE --ntasks-per-socket=<s> SLURM_NTASKS_PER_SOCKET -c <c> SLURM_CPUS_PER_TASK OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} SLURM_NTASKS Total number of tasks srun -n $SLURM_NTASKS [...]","title":"Slurm Overview"},{"location":"slurm/#slurm-resource-and-job-management-system","text":"ULHPC uses Slurm ( Simple Linux Utility for Resource Management ) for cluster/resource management and job scheduling. This middleware is responsible for allocating resources to users, providing a framework for starting, executing and monitoring work on allocated resources and scheduling work for future execution. Official docs Official FAQ ULHPC Tutorial/Getting Started IEEE ISPDC22: ULHPC Slurm 2.0 If you want more details on the RJMS optimizations performed upon Aion acquisition, check out our IEEE ISPDC22 conference paper (21 st IEEE Int. Symp. on Parallel and Distributed Computing) presented in Basel (Switzerland) on July 13, 2022. IEEE Reference Format | ORBilu entry | ULHPC blog post | slides Sebastien Varrette, Emmanuel Kieffer, and Frederic Pinel, \"Optimizing the Resource and Job Management System of an Academic HPC and Research Computing Facility\". In 21 st IEEE Intl. Symp. on Parallel and Distributed Computing (ISPDC\u201922) , Basel, Switzerland, 2022.","title":"Slurm Resource and Job Management System"},{"location":"slurm/#tldr-slurm-on-ulhpc-clusters","text":"In its concise form, the Slurm configuration in place on ULHPC supercomputers features the following attributes you should be aware of when interacting with it: Predefined Queues/Partitions depending on node type batch (Default Dual-CPU nodes) Max : 64 nodes, 2 days walltime gpu (GPU nodes nodes) Max : 4 nodes, 2 days walltime bigmem (Large-Memory nodes) Max : 1 node, 2 days walltime In addition: interactive (for quicks tests) Max : 2 nodes, 2h walltime for code development, testing, and debugging Queue Policy: cross-partition QOS , mainly tied to priority level ( low \\rightarrow \\rightarrow urgent ) long QOS with extended Max walltime ( MaxWall ) set to 14 days special preemptible QOS for best-effort jobs: besteffort . Accounts hierarchy associated to supervisors (multiple associations possible), projects or trainings you MUST use the proper account as a detailed usage tracking is performed and reported. Slurm Federation configuration between iris and aion ensures global policy (coherent job ID, global scheduling, etc.) within ULHPC systems easily submit jobs from one cluster to another using -M, --cluster aion|iris For more details, see the appropriate pages in the left menu (or the above conference paper ).","title":"TL;DR Slurm on ULHPC clusters"},{"location":"slurm/#jobs","text":"A job is an allocation of resources such as compute nodes assigned to a user for an certain amount of time. Jobs can be interactive or passive (e.g., a batch script) scheduled for later execution. What characterize a job? A user jobs have the following key characteristics: set of requested resources: number of computing resources: nodes (including all their CPUs and cores) or CPUs (including all their cores) or cores amount of memory : either per node or per CPU (wall)time needed for the users tasks to complete their work a requested node partition (job queue) a requested quality of service (QoS) level which grants users specific accesses a requested account for accounting purposes Once a job is assigned a set of nodes, the user is able to initiate parallel work in the form of job steps (sets of tasks) in any configuration within the allocation. When you login to a ULHPC system you land on a access/login node . Login nodes are only for editing and preparing jobs: They are not meant for actually running jobs. From the login node you can interact with Slurm to submit job scripts or start interactive jobs, which will be further run on the compute nodes.","title":"Jobs"},{"location":"slurm/#submit-jobs","text":"There are three ways of submitting jobs with slurm, using either sbatch , srun or salloc : sbatch (passive job) ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly sbatch -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] <path/to/launcher.sh> srun (interactive job) ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly srun -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] ---pty bash srun is also to be using within your launcher script to initiate a job step . salloc (request allocation/interactive job) # Request interactive jobs/allocations ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly salloc -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] <command>","title":"Submit Jobs"},{"location":"slurm/#sbatch","text":"sbatch is used to submit a batch launcher script for later execution, corresponding to batch/passive submission mode . The script will typically contain one or more srun commands to launch parallel tasks. Upon submission with sbatch , Slurm will: allocate resources (nodes, tasks, partition, constraints, etc.) runs a single copy of the batch script on the first allocated node in particular, if you depend on other scripts, ensure you have refer to them with the complete path toward them. When you submit the job, Slurm responds with the job's ID, which will be used to identify this job in reports from Slurm. # /!\\ ADAPT path to launcher accordingly $ sbatch <path/to/launcher>.sh Submitted batch job 864933","title":"sbatch"},{"location":"slurm/#srun","text":"srun is used to initiate parallel job steps within a job OR to start an interactive job Upon submission with srun , Slurm will: ( eventually ) allocate resources (nodes, tasks, partition, constraints, etc.) when run for interactive submission launch a job step that will execute on the allocated resources. A job can contain multiple job steps executing sequentially or in parallel on independent or shared resources within the job's node allocation.","title":"srun"},{"location":"slurm/#salloc","text":"salloc is used to allocate resources for a job in real time. Typically this is used to allocate resources (nodes, tasks, partition, etc.) and spawn a shell. The shell is then used to execute srun commands to launch parallel tasks.","title":"salloc"},{"location":"slurm/#specific-resource-allocation","text":"Within a job, you aim at running a certain number of tasks , and Slurm allow for a fine-grain control of the resource allocation that must be satisfied for each task. Beware of Slurm terminology in Multicore Architecture ! Slurm Node = Physical node , specified with -N <#nodes> Advice : always explicit number of expected number of tasks per node using --ntasks-per-node <n> . This way you control the node footprint of your job. Slurm Socket = Physical Socket/CPU/Processor Advice : if possible, explicit also the number of expected number of tasks per socket (processor) using --ntasks-per-socket <s> . relations between <s> and <n> must be aligned with the physical NUMA characteristics of the node. For instance on aion nodes, <n> = 8*<s> For instance on iris regular nodes, <n>=2*<s> when on iris bigmem nodes, <n>=4*<s> . ( the most confusing ): Slurm CPU = Physical CORE use -c <#threads> to specify the number of cores reserved per task. Hyper-Threading (HT) Technology is disabled on all ULHPC compute nodes. In particular: assume #cores = #threads , thus when using -c <threads> , you can safely set OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } # Default to 1 if SLURM_CPUS_PER_TASK not set to automatically abstract from the job context you have interest to match the physical NUMA characteristics of the compute node you're running at (Ex: target 16 threads per socket on Aion nodes (as there are 8 virtual sockets per nodes, 14 threads per socket on Iris regular nodes). The total number of tasks defined in a given job is stored in the $SLURM_NTASKS environment variable. This is very convenient to abstract from the job context to run MPI tasks/processes in parallel using for instance: srun -n ${ SLURM_NTASKS } [ ... ] We encourage you to always explicitly specify upon resource allocation the number of tasks you want per node/socket ( --ntasks-per-node <n> --ntasks-per-socket <s> ), to easily scale on multiple nodes with -N <N> . Adapt the number of threads and the settings to match the physical NUMA characteristics of the nodes Aion 16 cores per socket and 8 (virtual) sockets (CPUs) per aion node. {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <8n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 8\\times \\times 8\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 16 Ex: -N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4 ( Total : 64 tasks) Iris (default Dual-CPU) 14 cores per socket and 2 sockets (physical CPUs) per regular iris . {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <2n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 2\\times \\times 2\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 14 Ex: -N 2 --ntasks-per-node 4 --ntasks-per-socket 2 -c 7 ( Total : 8 tasks) Iris (Bigmem) 28 cores per socket and 4 sockets (physical CPUs) per bigmem iris {sbatch|srun|salloc|si} [-N <N>] --ntasks-per-node <4n> --ntasks-per-socket <n> -c <thread> Total : <N> \\times 4\\times \\times 4\\times <n> tasks, each on <thread> threads Ensure <n> \\times \\times <thread> = 28 Ex: -N 2 --ntasks-per-node 8 --ntasks-per-socket 2 -c 14 ( Total : 16 tasks)","title":"Specific Resource Allocation"},{"location":"slurm/#job-submission-options","text":"There are several useful environment variables set be Slurm within an allocated job. The most important ones are detailed in the below table which summarizes the main job submission options offered with {sbatch | srun | salloc} [...] : Command-line option Description Example -N <N> <N> Nodes request -N 2 --ntasks-per-node=<n> <n> Tasks-per-node request --ntasks-per-node=28 --ntasks-per-socket=<s> <s> Tasks-per-socket request --ntasks-per-socket=14 -c <c> <c> Cores-per-task request (multithreading) -c 1 --mem=<m>GB <m> GB memory per node request --mem 0 -t [DD-]HH[:MM:SS]> Walltime request -t 4:00:00 -G <gpu> <gpu> GPU(s) request -G 4 -C <feature> Feature request ( broadwell,skylake... ) -C skylake -p <partition> Specify job partition/queue --qos <qos> Specify job qos -A <account> Specify account -J <name> Job name -J MyApp -d <specification> Job dependency -d singleton --mail-user=<email> Specify email address --mail-type=<type> Notify user by email when certain event types occur. --mail-type=END,FAIL At a minimum a job submission script must include number of nodes, time, type of partition and nodes (resource allocation constraint and features), and quality of service (QOS). If a script does not specify any of these options then a default may be applied. The full list of directives is documented in the man pages for the sbatch command (see. man sbatch ).","title":"Job submission options"},{"location":"slurm/#sbatch-directives-vs-cli-options","text":"Each option can be specified either as an #SBATCH [...] directive in the job submission script: #!/bin/bash -l # <--- DO NOT FORGET '-l' ### Request a single task using one core on one node for 5 minutes in the batch queue #SBATCH -N 2 #SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --time=0-00:05:00 #SBATCH -p batch # [...] Or as a command line option when submitting the script: $ sbatch -p batch -N 2 --ntasks-per-node = 1 -c 1 --time = 0 -00:05:00 ./first-job.sh The command line and directive versions of an option are equivalent and interchangeable : if the same option is present both on the command line and as a directive, the command line will be honored. If the same option or directive is specified twice, the last value supplied will be used. Also, many options have both a long form, eg --nodes=2 and a short form, eg -N 2 . These are equivalent and interchangable. Common options to sbatch and srun Many options are common to both sbatch and srun , for example sbatch -N 4 ./first-job.sh allocates 4 nodes to first-job.sh , and srun -N 4 uname -n inside the job runs a copy of uname -n on each of 4 nodes. If you don't specify an option in the srun command line, srun will inherit the value of that option from sbatch . In these cases the default behavior of srun is to assume the same options as were passed to sbatch . This is achieved via environment variables: sbatch sets a number of environment variables with names like SLURM_NNODES and srun checks the values of those variables. This has two important consequences: Your job script can see the settings it was submitted with by checking these environment variables You should NOT override these environment variables. Also be aware that if your job script tries to do certain tricky things, such as using ssh to launch a command on another node, the environment might not be propagated and your job may not behave correctly","title":"#SBATCH directives vs. CLI options"},{"location":"slurm/#hw-characteristics-and-slurm-features-of-ulhpc-nodes","text":"When selecting specific resources allocations, it is crucial to match the hardware characteristics of the computing nodes. Details are provided below: Node (type) #Nodes #Socket / #Cores RAM [GB] Features aion-[0001-0318] 318 8 / 128 256 batch,epyc iris-[001-108] 108 2 / 28 128 batch,broadwell iris-[109-168] 60 2 / 28 128 batch,skylake iris-[169-186] (GPU) 18 2 / 28 768 gpu,skylake,volta iris-[191-196] (GPU) 6 2 / 28 768 gpu,skylake,volta32 iris-[187-190] (Large-Memory) 4 4 / 112 3072 bigmem,skylake As can be seen, Slurm [features] are associated to ULHPC compute nodes and permits to easily filter with the -C <feature> option the list of nodes. To list available features, use sfeatures : sfeatures # sinfo -o '%20N %.6D %.6c %15F %12P %f' # NODELIST NODES CPUS NODES(A/I/O/T) PARTITION AVAIL_FEATURES # [...] Always try to align resource specifications for your jobs with physical characteristics The typical format of your Slurm submission should thus probably be: sbatch|srun|... [-N <N>] --ntasks-per-node <n> -c <thread> [...] sbatch|srun|... [-N <N>] --ntasks-per-node <#sockets * s> --ntasks-per-socket <s> -c <thread> [...] This would define a total of <N> \\times \\times <n> TASKS (first form) or <N> \\times \\#sockets \\times \\times \\#sockets \\times <s> TASKS (second form), each on <thread> threads . You MUST ensure that either: <n> \\times \\times <thread> matches the number of cores avaiable on the target computing node (first form), or <n> = \\#sockets \\times \\#sockets \\times <s> , and <s> \\times \\times <thread> matches the number of cores per socket available on the target computing node (second form). Aion (default Dual-CPU) 16 cores per socket and 8 virtual sockets (CPUs) per aion node. Depending on the selected form, you MUST ensure that either <n> \\times \\times <thread> =128, or that <n> =8 <s> and <s> \\times \\times <thread> =16. ### Example 1 - use all cores available { sbatch | srun | salloc } -N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4 [ ... ] # Total: 64 tasks (spread across 2 nodes), each on 4 cores/threads ### Example 2 - use all cores available { sbatch | srun | salloc } --ntasks-per-node 128 -c 1 [ ... ] # Total; 128 (single-core) tasks ### Example 3 - use all cores available { sbatch | srun | salloc } -N 1 --ntasks-per-node 8 --ntasks-per-socket 1 -c 16 [ ... ] # Total: 8 tasks, each on 16 cores/threads ### Example 4 - use all cores available { sbatch | srun | salloc } -N 1 --ntasks-per-node 2 -c 64 [ ... ] # Total: 2 tasks, each on 64 cores/threads Iris (default Dual-CPU) 14 cores per socket and 2 sockets (physical CPUs) per regular iris node. Depending on the selected form, you MUST ensure that either <n> \\times \\times <thread> =28, or that <n> =2 <s> and <s> \\times \\times <thread> =14. ### Example 1 - use all cores available { sbatch | srun | salloc } -N 3 --ntasks-per-node 14 --ntasks-per-socket 7 -c 2 [ ... ] # Total: 42 tasks (spread across 3 nodes), each on 2 cores/threads ### Example 2 - use all cores available { sbatch | srun | salloc } -N 2 --ntasks-per-node 28 -c 1 [ ... ] # Total; 56 (single-core) tasks ### Example 3 - use all cores available { sbatch | srun | salloc } -N 2 --ntasks-per-node 2 --ntasks-per-socket 1 -c 14 [ ... ] # Total: 4 tasks (spread across 2 nodes), each on 14 cores/threads Iris (Large-Memory) 28 cores per socket and 4 sockets (physical CPUs) per bigmem iris node. Depending on the selected form, you MUST ensure that either <n> \\times \\times <thread> =112, or that <n> =4 <s> and <s> \\times \\times <thread> =28. ### Example 1 - use all cores available { sbatch | srun | salloc } -N 1 --ntasks-per-node 56 --ntasks-per-socket 14 -c 2 [ ... ] # Total: 56 tasks on a single bigmem node, each on 2 cores/threads ### Example 2 - use all cores available { sbatch | srun | salloc } --ntasks-per-node 112 -c 1 [ ... ] # Total; 112 (single-core) tasks ### Example 3 - use all cores available { sbatch | srun | salloc } -N 1 --ntasks-per-node 4 --ntasks-per-socket 1 -c 28 [ ... ] # Total: 4 tasks, each on 28 cores/threads","title":"HW characteristics and Slurm features of ULHPC nodes"},{"location":"slurm/#using-slurm-environment-variables","text":"Recall that the Slurm controller will set several SLURM_* variables in the environment of the batch script. The most important are listed in the table below - use them wisely to make your launcher script as flexible as possible to abstract and adapt from the allocation context, \" independently \" of the way the job script has been submitted. Submission option Environment variable Typical usage -N <N> SLURM_JOB_NUM_NODES or SLURM_NNODES --ntasks-per-node=<n> SLURM_NTASKS_PER_NODE --ntasks-per-socket=<s> SLURM_NTASKS_PER_SOCKET -c <c> SLURM_CPUS_PER_TASK OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} SLURM_NTASKS Total number of tasks srun -n $SLURM_NTASKS [...]","title":"Using Slurm Environment variables"},{"location":"slurm/accounts/","text":"Slurm Account Hierarchy \u00b6 The ULHPC resources can be reserved and allocated for the execution of jobs scheduled on the platform thanks to a Resource and Job Management Systems (RJMS) - Slurm in practice. This tool is configured to collect accounting information for every job and job step executed -- see SchedMD accounting documentation . ULHPC account (login) vs. Slurm [meta-]account Your ULHPC account defines the UNIX user you can use to connect to the facility and make you known to our systems. They are managed by IPA and define your login . Slurm accounts , refered to as meta-account in the sequel, are more loosely defined in Slurm , and should be seen as something similar to a UNIX group: it may contain other (set of) slurm account(s), multiple users, or just a single user. A user may belong to multiple slurm accounts, but MUST have a DefaultAccount , which is set to your line manager or principal investigator meta-account. ULHPC Account Tree Hierarchy \u00b6 Every user job runs under a group account, granting access to specific QOS levels. Such an account is unique within the account hierarchy. Accounting records are organized as a hierarchical tree according to 3 layers (slurm accounts) as depicted in the below figure ( click to enlarge ). At the leaf hierarchy stands the End user <login> from the IPA IdM database, bringing a total of 4 levels. Level Account Type Description Example L1 meta-account Top-level structure / organizations UL, CRP, Externals, Projects, Trainings L2 meta-account Organizational Unit (Faculty, ICs, External partner, Funding program...) FSTM, LCSB, LIST... L3 meta-account Principal investigators (PIs), project, courses/lectures <firstname>.<lastname> , <acronym> , <course> L4 login End-users (staff, student): your ULHPC/IPA login yourlogin Extracting your association tree By default, you will be able to see only the account hierarchy you belongs too through the association(s) set with your login. You can extract it with: $ sacctmgr show association where parent = root format = \"account,user%20,Share,QOS%50\" withsubaccounts Account User Share QOS ---------------------- -------- ----------- -------------------------------------------------- <top> <L1share> besteffort,debug,long,low,normal <orgunit> <L2share> besteffort,debug,long,low,normal <firstname>.<lastname> <L3share> besteffort,debug,long,low,normal <firstname>.<lastname> <login> <L4share> besteffort,debug,long,low,normal ( Admins ) Extract the full hierarchy The below commands assumes you have supervision rights on the root account. To list available L1 accounts (Top-level structure / organizations), use sacctmgr show association where parent = root format = \"cluster,account,Share,QOS%50\" To list L2 accounts: Under Uni.lu (UL) sacctmgr show association where parent = UL format = \"cluster,account,Share,QOS%50\" Under CRP sacctmgr show association where parent = CRP format = \"cluster,account,Share,QOS%50\" Under Externals sacctmgr show association where parent = externals format = \"cluster,account,Share,QOS%50\" Under Projects sacctmgr show association where parent = projects format = \"cluster,account,Share,QOS%50\" Under Trainings sacctmgr show association where parent = trainings format = \"cluster,account,Share,QOS%50\" To quickly list L3 accounts and its subaccounts: sassoc <account> , or sacctmgr show association where accounts=<L3account> format=\"account%20,user%20,Share,QOS%50\" To quickly list End User (L4) associations, use sassoc <login> , or sacctmgr show association where users=<login> format=\"account%20,user%20,Share,QOS%50\" Default account vs. multiple associations A given user <login> can be associated to multiple accounts , but have a single DefaultAccount (a meta-account at L3 level reflecting your line manager (Format: <firstname>.<lastname> ). To get information about your account information in the hierarchy, use the custom acct helper function , typically as acct $USER . Get ULHPC account information with acct <login> # /!\\ ADAPT <login> accordingly $ acct <login> # sacctmgr show user where name=\"<login>\" format=user,account%20,DefaultAccount%20,share,qos%50 withassoc User Account Def Acct Share QOS ------- ----------------------- ---------------------- ------- --------------------------------------- <login> project_<name1> <firstname>.<lastname> 1 besteffort,debug,long,low,normal <login> project_<name2> <firstname>.<lastname> 1 besteffort,debug,high,long,low,normal <login> <firstname>.<lastname> <firstname>.<lastname> 1 besteffort,debug,long,low,normal # ==> <login> Default account: <firstname>.<lastname> In the above example, the user <login> is associated to 3 meta-accounts at the L3 level of the hierarchy (his PI <firstname>.<lastname> and two projects account), each granting access to potentially different QOS . The account used upon job submission can be set with the -A <account> option. With the above example: $ sbatch | srun | ... [ ... ] # Use default account: <firstname>.<lastname> $ sbatch | srun | ... -A project_<name1> [ ... ] # Use account project_<name1> $ sbatch | srun | ... -A project_<name2> --qos high [ ... ] # Use account project_<name2>, granting access to high QOS $ sbatch | srun | ... -A anotheraccount [ ... ] # Error: non-existing association between <login> and anotheraccount To list all associations for a given user or meta-account, use the sassoc helper function : # /!\\ ADAPT <login> accordingly $ sassoc <login> You may use more classically the sacctmgr show [...] command: User information: sacctmgr show user where name=<login> [withassoc] (use the withassoc attribute to list all associations). Default account: sacctmgr show user where name=\"<login>\" format=DefaultAccount -P -n Get the parent account: sacctmgr show account where name=ulhpc format=Org -n -P To get the current association tree : add withsubaccounts to see ALL sub accounts # L1,L2 or L3 account /!\\ ADAPT <name> accordingly sacctmgr show association tree where accounts = <name> format = account,share # End user (L4) sacctmgr show association where users = $USER format = account,User,share,Partition,QOS No association, no job! It is mandatory to have your login registered within at least one association toward a meta-account (PI, project name) to be able to schedule jobs on the Impact on FairSharing and Job Accounting \u00b6 Every node in the above-mentioned tree hierarchy is associated with a weight defining its Raw Share in the FairSharing mechanism in place. Different rules are applied to define these weights/shares depending on the level in the hierarchy: L1 (Organizational Unit): arbitrary shares to dedicate at least 85% of the platform to serve UL needs and projects L2 : function of the out-degree of the tree nodes, reflecting also the past year funding L3 : a function reflecting the budget contribution of the PI/project (normalized on a per-month basis) for the year in exercise. L4 (ULHPC/IPA login): efficiency score, giving incentives for a more efficient usage of the platform. More details are given on this page . Default vs. Project accounts \u00b6 Default account associations are defined as follows: For UL staff or external partners: your direct Line Manager firstname.lastname within the institution (Faculty, IC, Company) you belong too. For students: the lecture/course they are registered too Guest student/training accounts are associated to the Students meta-account. In addition, your user account (ULHPC login) may be associated to other meta-accounts such as projects or specific training events. To establish job accounting against these extra specific accounts, use: {sbatch|srun} -A project_<name> [...] For more details, see Project accounts . restrictions applies and do not permit to reveal all information for other accounts than yours. \u21a9","title":"Account Hierarchy"},{"location":"slurm/accounts/#slurm-account-hierarchy","text":"The ULHPC resources can be reserved and allocated for the execution of jobs scheduled on the platform thanks to a Resource and Job Management Systems (RJMS) - Slurm in practice. This tool is configured to collect accounting information for every job and job step executed -- see SchedMD accounting documentation . ULHPC account (login) vs. Slurm [meta-]account Your ULHPC account defines the UNIX user you can use to connect to the facility and make you known to our systems. They are managed by IPA and define your login . Slurm accounts , refered to as meta-account in the sequel, are more loosely defined in Slurm , and should be seen as something similar to a UNIX group: it may contain other (set of) slurm account(s), multiple users, or just a single user. A user may belong to multiple slurm accounts, but MUST have a DefaultAccount , which is set to your line manager or principal investigator meta-account.","title":"Slurm Account Hierarchy"},{"location":"slurm/accounts/#ulhpc-account-tree-hierarchy","text":"Every user job runs under a group account, granting access to specific QOS levels. Such an account is unique within the account hierarchy. Accounting records are organized as a hierarchical tree according to 3 layers (slurm accounts) as depicted in the below figure ( click to enlarge ). At the leaf hierarchy stands the End user <login> from the IPA IdM database, bringing a total of 4 levels. Level Account Type Description Example L1 meta-account Top-level structure / organizations UL, CRP, Externals, Projects, Trainings L2 meta-account Organizational Unit (Faculty, ICs, External partner, Funding program...) FSTM, LCSB, LIST... L3 meta-account Principal investigators (PIs), project, courses/lectures <firstname>.<lastname> , <acronym> , <course> L4 login End-users (staff, student): your ULHPC/IPA login yourlogin Extracting your association tree By default, you will be able to see only the account hierarchy you belongs too through the association(s) set with your login. You can extract it with: $ sacctmgr show association where parent = root format = \"account,user%20,Share,QOS%50\" withsubaccounts Account User Share QOS ---------------------- -------- ----------- -------------------------------------------------- <top> <L1share> besteffort,debug,long,low,normal <orgunit> <L2share> besteffort,debug,long,low,normal <firstname>.<lastname> <L3share> besteffort,debug,long,low,normal <firstname>.<lastname> <login> <L4share> besteffort,debug,long,low,normal ( Admins ) Extract the full hierarchy The below commands assumes you have supervision rights on the root account. To list available L1 accounts (Top-level structure / organizations), use sacctmgr show association where parent = root format = \"cluster,account,Share,QOS%50\" To list L2 accounts: Under Uni.lu (UL) sacctmgr show association where parent = UL format = \"cluster,account,Share,QOS%50\" Under CRP sacctmgr show association where parent = CRP format = \"cluster,account,Share,QOS%50\" Under Externals sacctmgr show association where parent = externals format = \"cluster,account,Share,QOS%50\" Under Projects sacctmgr show association where parent = projects format = \"cluster,account,Share,QOS%50\" Under Trainings sacctmgr show association where parent = trainings format = \"cluster,account,Share,QOS%50\" To quickly list L3 accounts and its subaccounts: sassoc <account> , or sacctmgr show association where accounts=<L3account> format=\"account%20,user%20,Share,QOS%50\" To quickly list End User (L4) associations, use sassoc <login> , or sacctmgr show association where users=<login> format=\"account%20,user%20,Share,QOS%50\" Default account vs. multiple associations A given user <login> can be associated to multiple accounts , but have a single DefaultAccount (a meta-account at L3 level reflecting your line manager (Format: <firstname>.<lastname> ). To get information about your account information in the hierarchy, use the custom acct helper function , typically as acct $USER . Get ULHPC account information with acct <login> # /!\\ ADAPT <login> accordingly $ acct <login> # sacctmgr show user where name=\"<login>\" format=user,account%20,DefaultAccount%20,share,qos%50 withassoc User Account Def Acct Share QOS ------- ----------------------- ---------------------- ------- --------------------------------------- <login> project_<name1> <firstname>.<lastname> 1 besteffort,debug,long,low,normal <login> project_<name2> <firstname>.<lastname> 1 besteffort,debug,high,long,low,normal <login> <firstname>.<lastname> <firstname>.<lastname> 1 besteffort,debug,long,low,normal # ==> <login> Default account: <firstname>.<lastname> In the above example, the user <login> is associated to 3 meta-accounts at the L3 level of the hierarchy (his PI <firstname>.<lastname> and two projects account), each granting access to potentially different QOS . The account used upon job submission can be set with the -A <account> option. With the above example: $ sbatch | srun | ... [ ... ] # Use default account: <firstname>.<lastname> $ sbatch | srun | ... -A project_<name1> [ ... ] # Use account project_<name1> $ sbatch | srun | ... -A project_<name2> --qos high [ ... ] # Use account project_<name2>, granting access to high QOS $ sbatch | srun | ... -A anotheraccount [ ... ] # Error: non-existing association between <login> and anotheraccount To list all associations for a given user or meta-account, use the sassoc helper function : # /!\\ ADAPT <login> accordingly $ sassoc <login> You may use more classically the sacctmgr show [...] command: User information: sacctmgr show user where name=<login> [withassoc] (use the withassoc attribute to list all associations). Default account: sacctmgr show user where name=\"<login>\" format=DefaultAccount -P -n Get the parent account: sacctmgr show account where name=ulhpc format=Org -n -P To get the current association tree : add withsubaccounts to see ALL sub accounts # L1,L2 or L3 account /!\\ ADAPT <name> accordingly sacctmgr show association tree where accounts = <name> format = account,share # End user (L4) sacctmgr show association where users = $USER format = account,User,share,Partition,QOS No association, no job! It is mandatory to have your login registered within at least one association toward a meta-account (PI, project name) to be able to schedule jobs on the","title":"ULHPC Account Tree Hierarchy"},{"location":"slurm/accounts/#impact-on-fairsharing-and-job-accounting","text":"Every node in the above-mentioned tree hierarchy is associated with a weight defining its Raw Share in the FairSharing mechanism in place. Different rules are applied to define these weights/shares depending on the level in the hierarchy: L1 (Organizational Unit): arbitrary shares to dedicate at least 85% of the platform to serve UL needs and projects L2 : function of the out-degree of the tree nodes, reflecting also the past year funding L3 : a function reflecting the budget contribution of the PI/project (normalized on a per-month basis) for the year in exercise. L4 (ULHPC/IPA login): efficiency score, giving incentives for a more efficient usage of the platform. More details are given on this page .","title":"Impact on FairSharing and Job Accounting"},{"location":"slurm/accounts/#default-vs-project-accounts","text":"Default account associations are defined as follows: For UL staff or external partners: your direct Line Manager firstname.lastname within the institution (Faculty, IC, Company) you belong too. For students: the lecture/course they are registered too Guest student/training accounts are associated to the Students meta-account. In addition, your user account (ULHPC login) may be associated to other meta-accounts such as projects or specific training events. To establish job accounting against these extra specific accounts, use: {sbatch|srun} -A project_<name> [...] For more details, see Project accounts . restrictions applies and do not permit to reveal all information for other accounts than yours. \u21a9","title":"Default vs. Project accounts"},{"location":"slurm/commands/","text":"Main Slurm Commands \u00b6 Submit Jobs \u00b6 There are three ways of submitting jobs with slurm, using either sbatch , srun or salloc : sbatch (passive job) ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly sbatch -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] <path/to/launcher.sh> srun (interactive job) ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly srun -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] ---pty bash srun is also to be using within your launcher script to initiate a job step . salloc (request allocation/interactive job) # Request interactive jobs/allocations ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly salloc -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] <command> sbatch \u00b6 sbatch is used to submit a batch launcher script for later execution, corresponding to batch/passive submission mode . The script will typically contain one or more srun commands to launch parallel tasks. Upon submission with sbatch , Slurm will: allocate resources (nodes, tasks, partition, constraints, etc.) runs a single copy of the batch script on the first allocated node in particular, if you depend on other scripts, ensure you have refer to them with the complete path toward them. When you submit the job, Slurm responds with the job's ID, which will be used to identify this job in reports from Slurm. # /!\\ ADAPT path to launcher accordingly $ sbatch <path/to/launcher>.sh Submitted batch job 864933 srun \u00b6 srun is used to initiate parallel job steps within a job OR to start an interactive job Upon submission with srun , Slurm will: ( eventually ) allocate resources (nodes, tasks, partition, constraints, etc.) when run for interactive submission launch a job step that will execute on the allocated resources. A job can contain multiple job steps executing sequentially or in parallel on independent or shared resources within the job's node allocation. salloc \u00b6 salloc is used to allocate resources for a job in real time. Typically this is used to allocate resources (nodes, tasks, partition, etc.) and spawn a shell. The shell is then used to execute srun commands to launch parallel tasks. Interactive jobs: si* \u00b6 You should use the helper functions si , si-gpu , si-bigmem to submit an interactive job. For more details, see interactive jobs . Collect Job Information \u00b6 Command Description sacct [-X] -j <jobid> [...] display accounting information on jobs. scontrol show [...] view and/or update system, nodes, job, step, partition or reservation status seff <jobid> get efficiency metrics of past job smap graphically show information on jobs, nodes, partitions sprio show factors that comprise a jobs scheduling priority squeue [-u $(whoami)] display jobs[steps] and their state sstat show status of running jobs. squeue \u00b6 You can view information about jobs located in the Slurm scheduling queue (partition/qos), eventually filter on specific job state ( R :running / PD :pending / F :failed / PR :preempted) with squeue : $ squeue [ -u <user> ] [ -p <partition> ] [ ---qos <qos> ] [ --reservation <name> ] [ -t R | PD | F | PR ] To quickly access your jobs, you can simply use sq Live job statistics \u00b6 You can use the scurrent (for current interactive job) or (more generally) scontrol show job <jobid> to collect detailed information for a running job. scontrol show job <jobid> $ scontrol show job 2166371 JobId=2166371 JobName=bash UserId=<login>(<uid>) GroupId=clusterusers(666) MCS_label=N/A Priority=12741 Nice=0 Account=ulhpc QOS=debug JobState=RUNNING Reason=None [...] SubmitTime=2020-12-07T22:08:25 EligibleTime=2020-12-07T22:08:25 StartTime=2020-12-07T22:08:25 EndTime=2020-12-07T22:38:25 [...] WorkDir=/mnt/irisgpfs/users/<login> Past job statistics: slist , sreport \u00b6 Use the slist helper for a given job: # /!\\ ADAPT <jobid> accordingly $ slist <jobid> # sacct -j <JOBID> --format User,JobID,Jobname%30,partition,state,time,elapsed,\\ # MaxRss,MaxVMSize,nnodes,ncpus,nodelist,AveCPU,ConsumedEnergyRaw # seff <jobid> You can also use sreport o generate reports of job usage and cluster utilization for Slurm jobs. For instance, to list your usage in CPU-hours since the beginning of the year: $ sreport -t hours cluster UserUtilizationByAccount Users = $USER Start = $( date +%Y ) -01-01 -------------------------------------------------------------------------------- Cluster/User/Account Utilization 2021-01-01T00:00:00 - 2021-02-13T23:59:59 (3801600 secs) Usage reported in CPU Hours ---------------------------------------------------------------------------- Cluster Login Proper Name Account Used Energy --------- --------- --------------- ---------------------- -------- -------- iris <login> <name> <firstname>.<lastname> [...] iris <login> <name> project_<acronym> [...] Job efficiency \u00b6 seff \u00b6 Use seff to double check a past job CPU/Memory efficiency. Below examples should be self-speaking: Good CPU Eff. $ seff 2171749 Job ID: 2171749 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 28 CPU Utilized: 41-01:38:14 CPU Efficiency: 99.64% of 41-05:09:44 core-walltime Job Wall-clock time: 1-11:19:38 Memory Utilized: 2.73 GB Memory Efficiency: 2.43% of 112.00 GB Good Memory Eff. $ seff 2117620 Job ID: 2117620 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 16 CPU Utilized: 14:24:49 CPU Efficiency: 23.72% of 2-12:46:24 core-walltime Job Wall-clock time: 03:47:54 Memory Utilized: 193.04 GB Memory Efficiency: 80.43% of 240.00 GB Good CPU and Memory Eff. $ seff 2138087 Job ID: 2138087 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 64 CPU Utilized: 87-16:58:22 CPU Efficiency: 86.58% of 101-07:16:16 core-walltime Job Wall-clock time: 1-13:59:19 Memory Utilized: 1.64 TB Memory Efficiency: 99.29% of 1.65 TB [Very] Bad efficiency This illustrates a very bad job in terms of CPU/memory efficiency (below 4%), which illustrate a case where basically the user wasted 4 hours of computation while mobilizing a full node and its 28 cores. $ seff 2199497 Job ID: 2199497 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 28 CPU Utilized: 00:08:33 CPU Efficiency: 3.55% of 04:00:48 core-walltime Job Wall-clock time: 00:08:36 Memory Utilized: 55.84 MB Memory Efficiency: 0.05% of 112.00 GB This is typical of a single-core task can could be drastically improved via GNU Parallel . Note however that demonstrating a CPU good efficiency with seff may not be enough! You may still induce an abnormal load on the reserved nodes if you spawn more processes than allowed by the Slurm reservation. To avoid that, always try to prefix your executions with srun within your launchers. See also Specific Resource Allocations . susage \u00b6 Use susage to check your past jobs walltime accuracy ( Timelimit vs. Elapsed ) $ susage -h Usage: susage [-m] [-Y] [-S YYYY-MM-DD] [-E YYYT-MM-DD] For a specific user (if accounting rights granted): susage [...] -u <user> For a specific account (if accounting rights granted): susage [...] -A <account> Display past job usage summary Official sacct command \u00b6 Alternatively, you can use sacct (use sacct --helpformat to get the list of) for COMPLETED or TIMEOUT jobs (see Job State Codes ). using sacct -X -S <start> [...] --format [...],time,elapsed,[...] ADAPT -S <start> and -E <end> dates accordingly - Format: YYYY-MM-DD . hint : $(date +%F) will return today's date in that format, $(date +%Y) return the current year, so the below command will list your completed (or timeout jobs) since the beginning of the month: $ sacct -X -S $( date +%Y ) -01-01 -E $( date +%F ) --partition batch,gpu,bigmem --state CD,TO --format User,JobID,partition%12,qos,state,time,elapsed,nnodes,ncpus,allocGRES User JobID Partition QOS State Timelimit Elapsed NNodes NCPUS AllocGRES --------- ------------ ------------ ---------- ---------- ---------- ---------- -------- ---------- ------------ <login> 2243517 batch normal TIMEOUT 2-00:00:00 2-00:00:05 4 112 <login> 2243518 batch normal TIMEOUT 2-00:00:00 2-00:00:05 4 112 <login> 2244056 gpu normal TIMEOUT 2-00:00:00 2-00:00:12 1 16 gpu:2 <login> 2246094 gpu high TIMEOUT 2-00:00:00 2-00:00:29 1 16 gpu:2 <login> 2246120 gpu high COMPLETED 2-00:00:00 1-02:18:00 1 16 gpu:2 <login> 2247278 bigmem normal COMPLETED 2-00:00:00 1-05:59:21 1 56 <login> 2250178 batch normal COMPLETED 2-00:00:00 10:04:32 1 1 <login> 2251232 gpu normal COMPLETED 1-00:00:00 12:05:46 1 6 gpu:1 Platform Status \u00b6 sinfo \u00b6 sinfo allow to view information about partition status ( -p <partition> ), problematic nodes ( -R ), reservations ( -T ), eventually in a summarized form ( -s ), sinfo [-p <partition>] {-s | -R | -T |...} We are providing a certain number of helper functions based on sinfo : Command Description nodelist List available nodes allocnodes List currently allocated nodes idlenodes List currently idle nodes deadnodes List dead nodes per partition (hopefully none ;)) sissues List nodes with issues/problems, with reasons sfeatures List available node features Cluster, partition and QOS usage stats \u00b6 We have defined several custom ULHPC Slurm helpers defined in /etc/profile.d/slurm.sh to facilitate access to account/parition/qos/usage information. They are listed below. Command Description acct <name> Get information on user/account holder <name> in Slurm accounting DB irisstat , aionstat report cluster status (utilization, partition and QOS live stats) listpartitionjobs <part> List jobs (and current load) of the slurm partition <part> pload [-a] i/b/g/m Overview of the Slurm partition load qload [-a] <qos> Show current load of the slurm QOS <qos> sbill <jobid> Display job charging / billing summary sjoin [-w <node>] join a running job sassoc <name> Show Slurm association information for <name> (user or account) slist <jobid> [-X] List statistics of a past job sqos Show QOS information and limits susage [-m] [-Y] [...] Display past job usage summary Updating jobs \u00b6 Command Description scancel <jobid> cancel a job or set of jobs. scontrol update jobid=<jobid> [...] update pending job definition scontrol hold <jobid> Hold job scontrol resume <jobid> Resume held job The scontrol command allows certain charactistics of a job to be updated while it is still queued ( i.e. not running ), with the syntax scontrol update jobid=<jobid> [...] Important Once the job is running, most changes requested with scontrol update jobid=[...] will NOT be applied. Change timelimit \u00b6 # /!\\ ADAPT <jobid> and new time limit accordingly scontrol update jobid = <jobid> timelimit = < [ DD- ] HH:MM::SS> Change QOS or Reservation \u00b6 # /!\\ ADAPT <jobid>, <qos>, <resname> accordingly scontrol update jobid = <jobid> qos = <qos> scontrol update jobid = <jobid> reservationname = <resname> Change account \u00b6 If you forgot to specify the expected project account: # /!\\ ADAPT <jobid>, <account> accordingly scontrol update jobid = <jobid> account = <account> The new account must be eligible to run the job. See Account Hierarchy for more details. Hold and Resume jobs \u00b6 Prevent a pending job from being started: # /!\\ ADAPT <jobid> accordingly scontrol hold <jobid> Allow a held job to accrue priority and run: # /!\\ ADAPT <jobid> accordingly scontrol release <jobid> Cancel jobs \u00b6 Cancel a specific job: # /!\\ ADAPT <jobid> accordingly scancel <jobid> Cancel all jobs owned by a user (you) scancel -u $USER This only applies to jobs which are associated with your accounts.","title":"Convenient Slurm Commands"},{"location":"slurm/commands/#main-slurm-commands","text":"","title":"Main Slurm Commands"},{"location":"slurm/commands/#submit-jobs","text":"There are three ways of submitting jobs with slurm, using either sbatch , srun or salloc : sbatch (passive job) ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly sbatch -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] <path/to/launcher.sh> srun (interactive job) ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly srun -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] ---pty bash srun is also to be using within your launcher script to initiate a job step . salloc (request allocation/interactive job) # Request interactive jobs/allocations ### /!\\ Adapt <partition>, <qos>, <account> and <command> accordingly salloc -p <partition> [ --qos <qos> ] [ -A <account> ] [ ... ] <command>","title":"Submit Jobs"},{"location":"slurm/commands/#sbatch","text":"sbatch is used to submit a batch launcher script for later execution, corresponding to batch/passive submission mode . The script will typically contain one or more srun commands to launch parallel tasks. Upon submission with sbatch , Slurm will: allocate resources (nodes, tasks, partition, constraints, etc.) runs a single copy of the batch script on the first allocated node in particular, if you depend on other scripts, ensure you have refer to them with the complete path toward them. When you submit the job, Slurm responds with the job's ID, which will be used to identify this job in reports from Slurm. # /!\\ ADAPT path to launcher accordingly $ sbatch <path/to/launcher>.sh Submitted batch job 864933","title":"sbatch"},{"location":"slurm/commands/#srun","text":"srun is used to initiate parallel job steps within a job OR to start an interactive job Upon submission with srun , Slurm will: ( eventually ) allocate resources (nodes, tasks, partition, constraints, etc.) when run for interactive submission launch a job step that will execute on the allocated resources. A job can contain multiple job steps executing sequentially or in parallel on independent or shared resources within the job's node allocation.","title":"srun"},{"location":"slurm/commands/#salloc","text":"salloc is used to allocate resources for a job in real time. Typically this is used to allocate resources (nodes, tasks, partition, etc.) and spawn a shell. The shell is then used to execute srun commands to launch parallel tasks.","title":"salloc"},{"location":"slurm/commands/#interactive-jobs-si","text":"You should use the helper functions si , si-gpu , si-bigmem to submit an interactive job. For more details, see interactive jobs .","title":"Interactive jobs: si*"},{"location":"slurm/commands/#collect-job-information","text":"Command Description sacct [-X] -j <jobid> [...] display accounting information on jobs. scontrol show [...] view and/or update system, nodes, job, step, partition or reservation status seff <jobid> get efficiency metrics of past job smap graphically show information on jobs, nodes, partitions sprio show factors that comprise a jobs scheduling priority squeue [-u $(whoami)] display jobs[steps] and their state sstat show status of running jobs.","title":"Collect Job Information"},{"location":"slurm/commands/#squeue","text":"You can view information about jobs located in the Slurm scheduling queue (partition/qos), eventually filter on specific job state ( R :running / PD :pending / F :failed / PR :preempted) with squeue : $ squeue [ -u <user> ] [ -p <partition> ] [ ---qos <qos> ] [ --reservation <name> ] [ -t R | PD | F | PR ] To quickly access your jobs, you can simply use sq","title":"squeue"},{"location":"slurm/commands/#live-job-statistics","text":"You can use the scurrent (for current interactive job) or (more generally) scontrol show job <jobid> to collect detailed information for a running job. scontrol show job <jobid> $ scontrol show job 2166371 JobId=2166371 JobName=bash UserId=<login>(<uid>) GroupId=clusterusers(666) MCS_label=N/A Priority=12741 Nice=0 Account=ulhpc QOS=debug JobState=RUNNING Reason=None [...] SubmitTime=2020-12-07T22:08:25 EligibleTime=2020-12-07T22:08:25 StartTime=2020-12-07T22:08:25 EndTime=2020-12-07T22:38:25 [...] WorkDir=/mnt/irisgpfs/users/<login>","title":"Live job statistics"},{"location":"slurm/commands/#past-job-statistics-slist-sreport","text":"Use the slist helper for a given job: # /!\\ ADAPT <jobid> accordingly $ slist <jobid> # sacct -j <JOBID> --format User,JobID,Jobname%30,partition,state,time,elapsed,\\ # MaxRss,MaxVMSize,nnodes,ncpus,nodelist,AveCPU,ConsumedEnergyRaw # seff <jobid> You can also use sreport o generate reports of job usage and cluster utilization for Slurm jobs. For instance, to list your usage in CPU-hours since the beginning of the year: $ sreport -t hours cluster UserUtilizationByAccount Users = $USER Start = $( date +%Y ) -01-01 -------------------------------------------------------------------------------- Cluster/User/Account Utilization 2021-01-01T00:00:00 - 2021-02-13T23:59:59 (3801600 secs) Usage reported in CPU Hours ---------------------------------------------------------------------------- Cluster Login Proper Name Account Used Energy --------- --------- --------------- ---------------------- -------- -------- iris <login> <name> <firstname>.<lastname> [...] iris <login> <name> project_<acronym> [...]","title":"Past job statistics: slist, sreport"},{"location":"slurm/commands/#job-efficiency","text":"","title":"Job efficiency"},{"location":"slurm/commands/#seff","text":"Use seff to double check a past job CPU/Memory efficiency. Below examples should be self-speaking: Good CPU Eff. $ seff 2171749 Job ID: 2171749 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 28 CPU Utilized: 41-01:38:14 CPU Efficiency: 99.64% of 41-05:09:44 core-walltime Job Wall-clock time: 1-11:19:38 Memory Utilized: 2.73 GB Memory Efficiency: 2.43% of 112.00 GB Good Memory Eff. $ seff 2117620 Job ID: 2117620 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 16 CPU Utilized: 14:24:49 CPU Efficiency: 23.72% of 2-12:46:24 core-walltime Job Wall-clock time: 03:47:54 Memory Utilized: 193.04 GB Memory Efficiency: 80.43% of 240.00 GB Good CPU and Memory Eff. $ seff 2138087 Job ID: 2138087 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 64 CPU Utilized: 87-16:58:22 CPU Efficiency: 86.58% of 101-07:16:16 core-walltime Job Wall-clock time: 1-13:59:19 Memory Utilized: 1.64 TB Memory Efficiency: 99.29% of 1.65 TB [Very] Bad efficiency This illustrates a very bad job in terms of CPU/memory efficiency (below 4%), which illustrate a case where basically the user wasted 4 hours of computation while mobilizing a full node and its 28 cores. $ seff 2199497 Job ID: 2199497 Cluster: iris User/Group: <login>/clusterusers State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 28 CPU Utilized: 00:08:33 CPU Efficiency: 3.55% of 04:00:48 core-walltime Job Wall-clock time: 00:08:36 Memory Utilized: 55.84 MB Memory Efficiency: 0.05% of 112.00 GB This is typical of a single-core task can could be drastically improved via GNU Parallel . Note however that demonstrating a CPU good efficiency with seff may not be enough! You may still induce an abnormal load on the reserved nodes if you spawn more processes than allowed by the Slurm reservation. To avoid that, always try to prefix your executions with srun within your launchers. See also Specific Resource Allocations .","title":"seff"},{"location":"slurm/commands/#susage","text":"Use susage to check your past jobs walltime accuracy ( Timelimit vs. Elapsed ) $ susage -h Usage: susage [-m] [-Y] [-S YYYY-MM-DD] [-E YYYT-MM-DD] For a specific user (if accounting rights granted): susage [...] -u <user> For a specific account (if accounting rights granted): susage [...] -A <account> Display past job usage summary","title":"susage"},{"location":"slurm/commands/#official-sacct-command","text":"Alternatively, you can use sacct (use sacct --helpformat to get the list of) for COMPLETED or TIMEOUT jobs (see Job State Codes ). using sacct -X -S <start> [...] --format [...],time,elapsed,[...] ADAPT -S <start> and -E <end> dates accordingly - Format: YYYY-MM-DD . hint : $(date +%F) will return today's date in that format, $(date +%Y) return the current year, so the below command will list your completed (or timeout jobs) since the beginning of the month: $ sacct -X -S $( date +%Y ) -01-01 -E $( date +%F ) --partition batch,gpu,bigmem --state CD,TO --format User,JobID,partition%12,qos,state,time,elapsed,nnodes,ncpus,allocGRES User JobID Partition QOS State Timelimit Elapsed NNodes NCPUS AllocGRES --------- ------------ ------------ ---------- ---------- ---------- ---------- -------- ---------- ------------ <login> 2243517 batch normal TIMEOUT 2-00:00:00 2-00:00:05 4 112 <login> 2243518 batch normal TIMEOUT 2-00:00:00 2-00:00:05 4 112 <login> 2244056 gpu normal TIMEOUT 2-00:00:00 2-00:00:12 1 16 gpu:2 <login> 2246094 gpu high TIMEOUT 2-00:00:00 2-00:00:29 1 16 gpu:2 <login> 2246120 gpu high COMPLETED 2-00:00:00 1-02:18:00 1 16 gpu:2 <login> 2247278 bigmem normal COMPLETED 2-00:00:00 1-05:59:21 1 56 <login> 2250178 batch normal COMPLETED 2-00:00:00 10:04:32 1 1 <login> 2251232 gpu normal COMPLETED 1-00:00:00 12:05:46 1 6 gpu:1","title":"Official sacct command"},{"location":"slurm/commands/#platform-status","text":"","title":"Platform Status"},{"location":"slurm/commands/#sinfo","text":"sinfo allow to view information about partition status ( -p <partition> ), problematic nodes ( -R ), reservations ( -T ), eventually in a summarized form ( -s ), sinfo [-p <partition>] {-s | -R | -T |...} We are providing a certain number of helper functions based on sinfo : Command Description nodelist List available nodes allocnodes List currently allocated nodes idlenodes List currently idle nodes deadnodes List dead nodes per partition (hopefully none ;)) sissues List nodes with issues/problems, with reasons sfeatures List available node features","title":"sinfo"},{"location":"slurm/commands/#cluster-partition-and-qos-usage-stats","text":"We have defined several custom ULHPC Slurm helpers defined in /etc/profile.d/slurm.sh to facilitate access to account/parition/qos/usage information. They are listed below. Command Description acct <name> Get information on user/account holder <name> in Slurm accounting DB irisstat , aionstat report cluster status (utilization, partition and QOS live stats) listpartitionjobs <part> List jobs (and current load) of the slurm partition <part> pload [-a] i/b/g/m Overview of the Slurm partition load qload [-a] <qos> Show current load of the slurm QOS <qos> sbill <jobid> Display job charging / billing summary sjoin [-w <node>] join a running job sassoc <name> Show Slurm association information for <name> (user or account) slist <jobid> [-X] List statistics of a past job sqos Show QOS information and limits susage [-m] [-Y] [...] Display past job usage summary","title":"Cluster, partition and QOS usage stats"},{"location":"slurm/commands/#updating-jobs","text":"Command Description scancel <jobid> cancel a job or set of jobs. scontrol update jobid=<jobid> [...] update pending job definition scontrol hold <jobid> Hold job scontrol resume <jobid> Resume held job The scontrol command allows certain charactistics of a job to be updated while it is still queued ( i.e. not running ), with the syntax scontrol update jobid=<jobid> [...] Important Once the job is running, most changes requested with scontrol update jobid=[...] will NOT be applied.","title":"Updating jobs"},{"location":"slurm/commands/#change-timelimit","text":"# /!\\ ADAPT <jobid> and new time limit accordingly scontrol update jobid = <jobid> timelimit = < [ DD- ] HH:MM::SS>","title":"Change timelimit"},{"location":"slurm/commands/#change-qos-or-reservation","text":"# /!\\ ADAPT <jobid>, <qos>, <resname> accordingly scontrol update jobid = <jobid> qos = <qos> scontrol update jobid = <jobid> reservationname = <resname>","title":"Change QOS or Reservation"},{"location":"slurm/commands/#change-account","text":"If you forgot to specify the expected project account: # /!\\ ADAPT <jobid>, <account> accordingly scontrol update jobid = <jobid> account = <account> The new account must be eligible to run the job. See Account Hierarchy for more details.","title":"Change account"},{"location":"slurm/commands/#hold-and-resume-jobs","text":"Prevent a pending job from being started: # /!\\ ADAPT <jobid> accordingly scontrol hold <jobid> Allow a held job to accrue priority and run: # /!\\ ADAPT <jobid> accordingly scontrol release <jobid>","title":"Hold and Resume jobs"},{"location":"slurm/commands/#cancel-jobs","text":"Cancel a specific job: # /!\\ ADAPT <jobid> accordingly scancel <jobid> Cancel all jobs owned by a user (you) scancel -u $USER This only applies to jobs which are associated with your accounts.","title":"Cancel jobs"},{"location":"slurm/fairsharing/","text":"Fairsharing and Job Accounting \u00b6 Resources : Slurm Priority, Fairshare and Fair Tree (PDF) SchedMD Slurm documentation: Multifactor Priority Plugin Fair tree algorithm, FAS RC docs , Official sshare documentation Fairshare allows past resource utilization information to be taken into account into job feasibility and priority decisions to ensure a fair allocation of the computational resources between the all ULHPC users. A difference with a equal scheduling is illustrated in the side picture ( source ). Essentially fairshare is a way of ensuring that users get their appropriate portion of a system. Sadly this term is also used confusingly for different parts of fairshare listed below, so for the sake of clarity, the following terms will be used: [Raw] Share : portion of the system users have been granted [Raw] Usage : amount of the system users have actually used so far The fairshare score is the value the system calculates based on the usage and the share (see below) Priority : the priority that users are assigned based off of their fairshare score. Demystifying Fairshare While fairshare may seem complex and confusing, it is actually quite logical once you think about it. The scheduler needs some way to adjudicate who gets what resources when different groups on the cluster have been granted different resources and shares for various reasons (see Account Hierarchy ). In order to serve the great variety of groups and needs on the cluster, a method of fairly adjudicating job priority is required. This is the goal of Fairshare . Fairshare allows those users who have not fully used their resource grant to get higher priority for their jobs on the cluster, while making sure that those groups that have used more than their resource grant do not overuse the cluster. The ULHPC supercomputers are a limited shared resource, and Fairshare ensures everyone gets a fair opportunity to use it regardless of how big or small the group is . FairTree Algorithm \u00b6 There exists several fairsharing algorithms implemented in Slurm: Classic Fairshare Depth-Oblivious Fair-share Fair Tree (now implemented on ULHPC since Oct 2020) What is Fair Tree? The Fair Tree algorithm prioritizes users such that if accounts A and B are siblings and A has a higher fairshare factor than B, then all children of A will have higher fairshare factors than all children of B. This is done through a rooted plane tree (PDF) , also known as a rooted ordered tree, which is logically created then sorted by fairshare with the highest fairshare values on the left. The tree is then visited in a depth-first traversal way. Users are ranked in pre-order as they are found. The ranking is used to create the final fairshare factor for the user. Fair Tree Traversal Illustrated - initial post Some of the benefits include: All users from a higher priority account receive a higher fair share factor than all users from a lower priority account. Users are sorted and ranked to prevent errors due to precision loss. Ties are allowed. Account coordinators cannot accidentally harm the priority of their users relative to users in other accounts. Users are extremely unlikely to have exactly the same fairshare factor as another user due to loss of precision in calculations. New jobs are immediately assigned a priority. Overview of Fair Tree for End Users Level Fairshare Calculation Shares \u00b6 On ULHPC facilities, each user is associated by default to a meta-account reflecting its direct Line Manager within the institution (Faculty, IC, Company) you belong too -- see ULHPC Account Hierarchy . You may have other account associations (typically toward projects accounts, granting access to different QOS for instance), and each accounts have Shares granted to them. These Shares determine how much of the cluster that group/account has been granted . Users when they run are charged back for their runs against the account used upon job submission -- you can use sbatch|srun|... -A <account> [...] to change that account. ULHPC Usage Charging Policy Different rules are applied to define these weights/shares depending on the level in the hierarchy: L1 (Organizational Unit): arbitrary shares to dedicate at least 85% of the platform to serve UL needs and projects L2 : function of the out-degree of the tree nodes, reflecting also the past year funding L3 : a function reflecting the budget contribution of the PI/project (normalized on a per-month basis) for the year in exercise. L4 (ULHPC/IPA login): efficiency score, giving incentives for a more efficient usage of the platform. Fair Share Factor \u00b6 The Fairshare score is the value Slurm calculates based off of user's usage reflecting the difference between the portion of the computing resource that has been promised (share) and the amount of resources that has been consumed. It thus influences the order in which a user's queued jobs are scheduled to run based on the portion of the computing resources they have been allocated and the resources their jobs have already consumed. In practice, Slurm's fair-share factor is a floating point number between 0.0 and 1.0 that reflects the shares of a computing resource that a user has been allocated and the amount of computing resources the user's jobs have consumed. The higher the value, the higher is the placement in the queue of jobs waiting to be scheduled. Reciprocally, the more resources the users is consuming, the lower the fair share factor will be which will result in lower priorities. ulhpcshare helper \u00b6 Listing the ULHPC shares: ulhpcshare helper sshare can be used to view the fair share factors and corresponding promised and actual usage for all users. However , you are encouraged to use the ulhpcshare helper function: # your current shares and fair-share factors among your associations ulhpcshare # as above, but for user '<login>' ulhpcshare -u <login> # as above, but for account '<account>' ulhpcshare -A <account> The column that contains the actual factor is called \"FairShare\". Official sshare utility \u00b6 ulhpcshare is a wrapper around the official sshare utility. You can quickly see your score with $ sshare [ -A <account> ] [ -l ] [ --format = Account,User,RawShares,NormShares,EffectvUsage,LevelFS,FairShare ] It will show the Level Fairshare value as Level FS . The field shows the value for each association, thus allowing users to see the results of the fairshare calculation at each level. Note : Unlike the Effective Usage, the Norm Usage is not used by Fair Tree but is still displayed in this case. Slurm Parameter Definitions \u00b6 In this part some of the set slurm parameters are explained which are used to set up the Fair Tree Fairshare Algorithm. For a more detailed explanation please consult the official documentation PriorityCalcPeriod=HH:MM::SS : frequency in minutes that job half-life decay and Fair Tree calculations are performed. PriorityDecayHalfLife=[number of days]-[number of hours] : the time, of which the resource consumption is taken into account for the Fairshare Algorithm, can be set by this. PriorityMaxAge=[number of days]-[number of hours] : the maximal queueing time which counts for the priority calculation. Note that queueing times above are possible but do not contribute to the priority factor. A quick way to check the currently running configuration is: scontrol show config | grep -i priority Trackable RESources (TRES) Billing Weights \u00b6 Slurm saves accounting data for every job or job step that the user submits. On ULHPC facilities, Slurm Trackable RESources (TRES) is enabled to allow for the scheduler to charge back users for how much they have used of different features (i.e. not only CPU) on the cluster -- see Job Accounting and Billing . This is important as the usage of the cluster factors into the Fairshare calculation. As explained in the ULHPC Usage Charging Policy , we set TRES for CPU, GPU, and Memory usage according to weights defined as follows: Weight Description \\alpha_{cpu} \\alpha_{cpu} Normalized relative performance of CPU processor core (ref.: skylake 73.6 GFlops/core) \\alpha_{mem} \\alpha_{mem} Inverse of the average available memory size per core \\alpha_{GPU} \\alpha_{GPU} Weight per GPU accelerator Each partition has its own weights (combined into TRESBillingWeight ) you can check with # /!\\ ADAPT <partition> accordingly scontrol show partition <partition> FAQ \u00b6 Q: My user fairshare is low, what can I do? \u00b6 We have introduced an efficiency score evaluated on a regular basis (by default, every year) to measure how efficient you use the computational resources of the University according to several measures for completed jobs: How efficient you were to estimate the walltime of your jobs (Average Walltime Accuracy) How CPU/Memory efficient were your completed jobs (see seff ) Without entering into the details, we combine these metrics to compute an unique score value S_\\text{efficiency} S_\\text{efficiency} and you obtain a grade: A (very good), B , C , or D (very bad) which can increase your user share. Q: My account fairshare is low, what can I do? \u00b6 There are several things that can be done when your fairshare is low: Do not run jobs : Fairshare recovers via two routes. The first is via your group not running any jobs and letting others use the resource. That allows your fractional usage to decrease which in turn increases your fairshare score. The second is via the half-life we apply to fairshare which ages out old usage over time. Both of these method require not action but inaction on the part of your group. Thus to recover your fairshare simply stop running jobs until your fairshare reaches the level you desire. Be warned this could take several weeks to accomplish depending on your current usage. Be patient , as a corollary to the previous point. Even if your fairshare is low, your job gains priority by sitting the queue (see Job Priority ) The longer it sits the higher priority it gains. So even if you have very low fairshare your jobs will eventually run, it just may take several days to accomplish. Leverage Backfill : Slurm runs in two scheduling loops. The first loop is the main loop which simply looks at the top of the priority chain for the partition and tries to schedule that job. It will schedule jobs until it hits a job it cannot schedule and then it restarts the loop. The second loop is the backfill loop. This loop looks through jobs further down in the queue and asks can I schedule this job now and not interfere with the start time of the top priority job. Think of it as the scheduler playing giant game of three dimensional tetris, where the dimensions are number of cores, amount of memory, and amount of time. If your job will fit in the gaps that the scheduler has it will put your job in that spot even if it is low priority. This requires you to be very accurate in specifying the core, memory, and time usage ( typically below ) of your job. The better constrained your job is the more likely the scheduler is to fit you in to these gaps**. The seff utility is a great way of figuring out your job performance. Plan : Better planning and knowledge of your historic usage can help you better budget your time on the cluster. Our clusters are not infinite resources . You have been allocated a slice of the cluster, thus it is best to budget your usage so that you can run high priority jobs when you need to. HPC Budget contribution : If your group has persistent high demand that cannot be met with your current allocation, serious consideration should be given to contributing to the ULHPC budget line. This should be done for funded research projects - see HPC Resource Allocations for Research Project This can be done by each individual PI, Dean or IC director In all cases, any contribution on year Y grants additional shares for the group starting year Y+1 . We apply a consistent (complex) function taking into account depreciation of the investment. Contact us (by mail or by a ticket for more details.","title":"Fairsharing"},{"location":"slurm/fairsharing/#fairsharing-and-job-accounting","text":"Resources : Slurm Priority, Fairshare and Fair Tree (PDF) SchedMD Slurm documentation: Multifactor Priority Plugin Fair tree algorithm, FAS RC docs , Official sshare documentation Fairshare allows past resource utilization information to be taken into account into job feasibility and priority decisions to ensure a fair allocation of the computational resources between the all ULHPC users. A difference with a equal scheduling is illustrated in the side picture ( source ). Essentially fairshare is a way of ensuring that users get their appropriate portion of a system. Sadly this term is also used confusingly for different parts of fairshare listed below, so for the sake of clarity, the following terms will be used: [Raw] Share : portion of the system users have been granted [Raw] Usage : amount of the system users have actually used so far The fairshare score is the value the system calculates based on the usage and the share (see below) Priority : the priority that users are assigned based off of their fairshare score. Demystifying Fairshare While fairshare may seem complex and confusing, it is actually quite logical once you think about it. The scheduler needs some way to adjudicate who gets what resources when different groups on the cluster have been granted different resources and shares for various reasons (see Account Hierarchy ). In order to serve the great variety of groups and needs on the cluster, a method of fairly adjudicating job priority is required. This is the goal of Fairshare . Fairshare allows those users who have not fully used their resource grant to get higher priority for their jobs on the cluster, while making sure that those groups that have used more than their resource grant do not overuse the cluster. The ULHPC supercomputers are a limited shared resource, and Fairshare ensures everyone gets a fair opportunity to use it regardless of how big or small the group is .","title":"Fairsharing and Job Accounting"},{"location":"slurm/fairsharing/#fairtree-algorithm","text":"There exists several fairsharing algorithms implemented in Slurm: Classic Fairshare Depth-Oblivious Fair-share Fair Tree (now implemented on ULHPC since Oct 2020) What is Fair Tree? The Fair Tree algorithm prioritizes users such that if accounts A and B are siblings and A has a higher fairshare factor than B, then all children of A will have higher fairshare factors than all children of B. This is done through a rooted plane tree (PDF) , also known as a rooted ordered tree, which is logically created then sorted by fairshare with the highest fairshare values on the left. The tree is then visited in a depth-first traversal way. Users are ranked in pre-order as they are found. The ranking is used to create the final fairshare factor for the user. Fair Tree Traversal Illustrated - initial post Some of the benefits include: All users from a higher priority account receive a higher fair share factor than all users from a lower priority account. Users are sorted and ranked to prevent errors due to precision loss. Ties are allowed. Account coordinators cannot accidentally harm the priority of their users relative to users in other accounts. Users are extremely unlikely to have exactly the same fairshare factor as another user due to loss of precision in calculations. New jobs are immediately assigned a priority. Overview of Fair Tree for End Users Level Fairshare Calculation","title":"FairTree Algorithm"},{"location":"slurm/fairsharing/#shares","text":"On ULHPC facilities, each user is associated by default to a meta-account reflecting its direct Line Manager within the institution (Faculty, IC, Company) you belong too -- see ULHPC Account Hierarchy . You may have other account associations (typically toward projects accounts, granting access to different QOS for instance), and each accounts have Shares granted to them. These Shares determine how much of the cluster that group/account has been granted . Users when they run are charged back for their runs against the account used upon job submission -- you can use sbatch|srun|... -A <account> [...] to change that account. ULHPC Usage Charging Policy Different rules are applied to define these weights/shares depending on the level in the hierarchy: L1 (Organizational Unit): arbitrary shares to dedicate at least 85% of the platform to serve UL needs and projects L2 : function of the out-degree of the tree nodes, reflecting also the past year funding L3 : a function reflecting the budget contribution of the PI/project (normalized on a per-month basis) for the year in exercise. L4 (ULHPC/IPA login): efficiency score, giving incentives for a more efficient usage of the platform.","title":"Shares"},{"location":"slurm/fairsharing/#fair-share-factor","text":"The Fairshare score is the value Slurm calculates based off of user's usage reflecting the difference between the portion of the computing resource that has been promised (share) and the amount of resources that has been consumed. It thus influences the order in which a user's queued jobs are scheduled to run based on the portion of the computing resources they have been allocated and the resources their jobs have already consumed. In practice, Slurm's fair-share factor is a floating point number between 0.0 and 1.0 that reflects the shares of a computing resource that a user has been allocated and the amount of computing resources the user's jobs have consumed. The higher the value, the higher is the placement in the queue of jobs waiting to be scheduled. Reciprocally, the more resources the users is consuming, the lower the fair share factor will be which will result in lower priorities.","title":"Fair Share Factor"},{"location":"slurm/fairsharing/#ulhpcshare-helper","text":"Listing the ULHPC shares: ulhpcshare helper sshare can be used to view the fair share factors and corresponding promised and actual usage for all users. However , you are encouraged to use the ulhpcshare helper function: # your current shares and fair-share factors among your associations ulhpcshare # as above, but for user '<login>' ulhpcshare -u <login> # as above, but for account '<account>' ulhpcshare -A <account> The column that contains the actual factor is called \"FairShare\".","title":"ulhpcshare helper"},{"location":"slurm/fairsharing/#official-sshare-utility","text":"ulhpcshare is a wrapper around the official sshare utility. You can quickly see your score with $ sshare [ -A <account> ] [ -l ] [ --format = Account,User,RawShares,NormShares,EffectvUsage,LevelFS,FairShare ] It will show the Level Fairshare value as Level FS . The field shows the value for each association, thus allowing users to see the results of the fairshare calculation at each level. Note : Unlike the Effective Usage, the Norm Usage is not used by Fair Tree but is still displayed in this case.","title":"Official sshare utility"},{"location":"slurm/fairsharing/#slurm-parameter-definitions","text":"In this part some of the set slurm parameters are explained which are used to set up the Fair Tree Fairshare Algorithm. For a more detailed explanation please consult the official documentation PriorityCalcPeriod=HH:MM::SS : frequency in minutes that job half-life decay and Fair Tree calculations are performed. PriorityDecayHalfLife=[number of days]-[number of hours] : the time, of which the resource consumption is taken into account for the Fairshare Algorithm, can be set by this. PriorityMaxAge=[number of days]-[number of hours] : the maximal queueing time which counts for the priority calculation. Note that queueing times above are possible but do not contribute to the priority factor. A quick way to check the currently running configuration is: scontrol show config | grep -i priority","title":"Slurm Parameter Definitions"},{"location":"slurm/fairsharing/#trackable-resources-tres-billing-weights","text":"Slurm saves accounting data for every job or job step that the user submits. On ULHPC facilities, Slurm Trackable RESources (TRES) is enabled to allow for the scheduler to charge back users for how much they have used of different features (i.e. not only CPU) on the cluster -- see Job Accounting and Billing . This is important as the usage of the cluster factors into the Fairshare calculation. As explained in the ULHPC Usage Charging Policy , we set TRES for CPU, GPU, and Memory usage according to weights defined as follows: Weight Description \\alpha_{cpu} \\alpha_{cpu} Normalized relative performance of CPU processor core (ref.: skylake 73.6 GFlops/core) \\alpha_{mem} \\alpha_{mem} Inverse of the average available memory size per core \\alpha_{GPU} \\alpha_{GPU} Weight per GPU accelerator Each partition has its own weights (combined into TRESBillingWeight ) you can check with # /!\\ ADAPT <partition> accordingly scontrol show partition <partition>","title":"Trackable RESources (TRES) Billing Weights"},{"location":"slurm/fairsharing/#faq","text":"","title":"FAQ"},{"location":"slurm/fairsharing/#q-my-user-fairshare-is-low-what-can-i-do","text":"We have introduced an efficiency score evaluated on a regular basis (by default, every year) to measure how efficient you use the computational resources of the University according to several measures for completed jobs: How efficient you were to estimate the walltime of your jobs (Average Walltime Accuracy) How CPU/Memory efficient were your completed jobs (see seff ) Without entering into the details, we combine these metrics to compute an unique score value S_\\text{efficiency} S_\\text{efficiency} and you obtain a grade: A (very good), B , C , or D (very bad) which can increase your user share.","title":"Q: My user fairshare is low, what can I do?"},{"location":"slurm/fairsharing/#q-my-account-fairshare-is-low-what-can-i-do","text":"There are several things that can be done when your fairshare is low: Do not run jobs : Fairshare recovers via two routes. The first is via your group not running any jobs and letting others use the resource. That allows your fractional usage to decrease which in turn increases your fairshare score. The second is via the half-life we apply to fairshare which ages out old usage over time. Both of these method require not action but inaction on the part of your group. Thus to recover your fairshare simply stop running jobs until your fairshare reaches the level you desire. Be warned this could take several weeks to accomplish depending on your current usage. Be patient , as a corollary to the previous point. Even if your fairshare is low, your job gains priority by sitting the queue (see Job Priority ) The longer it sits the higher priority it gains. So even if you have very low fairshare your jobs will eventually run, it just may take several days to accomplish. Leverage Backfill : Slurm runs in two scheduling loops. The first loop is the main loop which simply looks at the top of the priority chain for the partition and tries to schedule that job. It will schedule jobs until it hits a job it cannot schedule and then it restarts the loop. The second loop is the backfill loop. This loop looks through jobs further down in the queue and asks can I schedule this job now and not interfere with the start time of the top priority job. Think of it as the scheduler playing giant game of three dimensional tetris, where the dimensions are number of cores, amount of memory, and amount of time. If your job will fit in the gaps that the scheduler has it will put your job in that spot even if it is low priority. This requires you to be very accurate in specifying the core, memory, and time usage ( typically below ) of your job. The better constrained your job is the more likely the scheduler is to fit you in to these gaps**. The seff utility is a great way of figuring out your job performance. Plan : Better planning and knowledge of your historic usage can help you better budget your time on the cluster. Our clusters are not infinite resources . You have been allocated a slice of the cluster, thus it is best to budget your usage so that you can run high priority jobs when you need to. HPC Budget contribution : If your group has persistent high demand that cannot be met with your current allocation, serious consideration should be given to contributing to the ULHPC budget line. This should be done for funded research projects - see HPC Resource Allocations for Research Project This can be done by each individual PI, Dean or IC director In all cases, any contribution on year Y grants additional shares for the group starting year Y+1 . We apply a consistent (complex) function taking into account depreciation of the investment. Contact us (by mail or by a ticket for more details.","title":"Q: My account fairshare is low, what can I do?"},{"location":"slurm/launchers/","text":"Slurm Launcher Examples \u00b6 ULHPC Tutorial / Getting Started ULHPC Tutorial / OpenMP/MPI When setting your default #SBATCH directive, always keep in mind your expected default resource allocation that would permit to submit your launchers without options sbatch <launcher> (you will be glad in a couple of month not to have to remember the options you need to pass) and try to stick to a single node (to avoid to accidentally induce a huge submission). Resource allocation Guidelines \u00b6 General guidelines Always try to align resource specifications for your jobs with physical characteristics. Always prefer the use of --ntasks-per-{node,socket} over -n when defining your tasks allocation request to automatically scale appropriately upon multi-nodes submission with for instance sbatch -N 2 <launcher> . Launcher template: #!/bin/bash -l # <--- DO NOT FORGET '-l' to facilitate further access to ULHPC modules #SBATCH -p <partition> #SBATCH -p <partition> #SBATCH -N 1 #SBATCH -N 1 #SBATCH --ntasks-per-node=<n> #SBATCH --ntasks-per-node <#sockets * s> #SBATCH -c <thread> #SBATCH --ntasks-per-socket <s> #SBATCH -c <thread> This would define by default a total of <n> (left) or \\#sockets \\times \\#sockets \\times <s> (right) tasks per node , each on <thread> threads . You MUST ensure that either: <n> \\times \\times <thread> matches the number of cores avaiable on the target computing node (left), or <n> = \\#sockets \\times \\#sockets \\times <s> , and <s> \\times \\times <thread> matches the number of cores per socket available on the target computing node (right). See Specific Resource Allocation Node (type) #Nodes #Socket / #Cores RAM [GB] Features aion-[0001-0318] 318 8 / 128 256 batch,epyc iris-[001-108] 108 2 / 28 128 batch,broadwell iris-[109-168] 60 2 / 28 128 batch,skylake iris-[169-186] (GPU) 18 2 / 28 768 gpu,skylake,volta iris-[191-196] (GPU) 6 2 / 28 768 gpu,skylake,volta32 iris-[187-190] (Large-Memory) 4 4 / 112 3072 bigmem,skylake Aion (default Dual-CPU) 16 cores per socket and 8 (virtual) sockets (CPUs) per aion node. Examples: #SBATCH -p batch #SBATCH -p batch #SBATCH -p batch #SBATCH -N 1 #SBATCH -N 1 #SBATCH -N 1 #SBATCH --ntasks-per-node=128 #SBATCH --ntasks-per-node 16 #SBATCH --ntasks-per-node 8 #SBATCH --ntasks-per-socket 16 #SBATCH --ntasks-per-socket 2 #SBATCH --ntasks-per-socket 1 #SBATCH -c 1 #SBATCH -c 8 #SBATCH -c 16 Iris (default Dual-CPU) 14 cores per socket and 2 sockets (physical CPUs) per regular iris . Examples: #SBATCH -p batch #SBATCH -p batch #SBATCH -p batch #SBATCH -N 1 #SBATCH -N 1 #SBATCH -N 1 #SBATCH --ntasks-per-node=28 #SBATCH --ntasks-per-node 14 #SBATCH --ntasks-per-node 4 #SBATCH --ntasks-per-socket=14 #SBATCH --ntasks-per-socket 7 #SBATCH --ntasks-per-socket 2 #SBATCH -c 1 #SBATCH -c 2 #SBATCH -c 7 Iris (GPU) 14 cores per socket and 2 sockets (physical CPUs) per gpu iris , 4 GPU accelerator cards per node. You probably want to dedicate 1 task and \\frac{1}{4} \\frac{1}{4} of the available cores to the management of each GPU accelerator. Examples: #SBATCH -p gpu #SBATCH -p gpu #SBATCH -p gpu #SBATCH -N 1 #SBATCH -N 1 #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH --ntasks-per-node 2 #SBATCH --ntasks-per-node 4 #SBATCH -c 7 #SBATCH --ntasks-per-socket 1 #SBATCH --ntasks-per-socket 2 #SBATCH -G 1 #SBATCH -c 7 #SBATCH -c 7 #SBATCH -G 2 #SBATCH -G 4 Iris (Large-Memory) 28 cores per socket and 4 sockets (physical CPUs) per bigmem iris node. Examples: #SBATCH -p bigmem #SBATCH -p bigmem #SBATCH -p bigmem #SBATCH -N 1 #SBATCH -N 1 #SBATCH -N 1 #SBATCH --ntasks-per-node=4 #SBATCH --ntasks-per-node 8 #SBATCH --ntasks-per-node 16 #SBATCH --ntasks-per-socket=1 #SBATCH --ntasks-per-socket 2 #SBATCH --ntasks-per-socket 4 #SBATCH -c 28 #SBATCH -c 14 #SBATCH -c 7 You probably want to play with a single task but define the expected memory allocation with --mem=<size[units]> (Default units are megabytes - Different units can be specified using the suffix [K|M|G|T] ) Basic Slurm Launcher Examples \u00b6 Single core task 1 task per job (Note: prefer GNU Parallel in that case - see below) #!/bin/bash -l # <--- DO NOT FORGET '-l' ### Request a single task using one core on one node for 5 minutes in the batch queue #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --time=0-00:05:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } # Safeguard for NOT running this launcher on access/login nodes module purge || print_error_and_exit \"No 'module' command\" # List modules required for execution of the task module load <...> # [...] Multiple Single core tasks 28 single-core tasks per job #!/bin/bash -l ### Request as many tasks as cores available on a single node for 3 hours #SBATCH -N 1 #SBATCH --ntasks-per-node=28 # On iris; for aion, use --ntasks-per-node=128 #SBATCH -c 1 #SBATCH --time=0-03:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load <...> # [...] Multithreaded parallel tasks 7 multithreaded tasks per job (4 threads each) #!/bin/bash -l ### Request as many tasks as cores available on a single node for 3 hours #SBATCH -N 1 #SBATCH --ntasks-per-node=7 # On iris; for aion, use --ntasks-per-node=32 #SBATCH -c 4 #SBATCH --time=0-03:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load <...> # [...] Embarrassingly Parallel Tasks \u00b6 For many users, the reason to consider (or being encouraged) to offload their computing executions on a (remote) HPC or Cloud facility is tied to the limits reached by their computing devices (laptop or workstation). It is generally motivated by time constraints \"My computations take several hours/days to complete. On an HPC, it will last a few minutes, no?\" or search-space explorations: \"I need to check my application against a huge number of input pieces (files) - it worked on a few of them locally but takes ages for a single check. How to proceed on HPC?\" In most of the cases, your favorite Java application or R/python (custom) development scripts, iterated again over multiple input conditions, are inherently SERIAL : they are able to use only one core when executed. You thus deal with what is often call a Bag of (independent) tasks , also referred to as embarrassingly parallel tasks . In this case, you MUST NOT overload the job scheduler with a large number of small (single-core) jobs. Instead, you should use GNU Parallel which permits the effective management of such tasks in a way that optimize both the resource allocation and the completion time. More specifically, GNU Parallel is a tool for executing tasks in parallel, typically on a single machine. When coupled with the Slurm command srun, parallel becomes a powerful way of distributing a set of tasks amongst a number of workers. This is particularly useful when the number of tasks is significantly larger than the number of available workers (i.e. $SLURM_NTASKS ), and each tasks is independent of the others. ULHPC Tutorial: GNU Parallel launcher for Embarrassingly Parallel Jobs Luckily, we have prepared a generic GNU Parallel launcher that should be straight forward to adapt to your own workflow following our tutorial : Create a dedicated script run_<task> responsible to run your java/R/Python tasks while taking as argument the parameter of each run. You can inspire from run_stressme for instance. test it in interactive rename the generic launcher launcher.parallel.sh to launcher_<task>.sh , enable #SBATCH --dependency singleton set the jobname change TASK to point to the absolute path to run_<task> script set TASKLISTFILE to point to a files with the parameters to pass to your script for each task adapt eventually the #SBATCH --ntasks-per-node [...] and #SBATCH -c [...] to match your needs AND the hardware configs of a single node (28 cores on iris, 128 cores on Aion) -- see guidelines test a batch run -- stick to a single node to take the best out of one full node. Serial Task script Launcher \u00b6 Serial Killer (Generic template) #!/bin/bash -l # <--- DO NOT FORGET '-l' #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" # C/C++: module load toolchain/intel # OR: module load toolchain/foss # Java: module load lang/Java/1.8 # Ruby/Perl/Rust...: module load lang/{Ruby,Perl,Rust...} # /!\\ ADAPT TASK variable accordingly - absolute path to the (serial) task to be executed TASK = ${ TASK := ${ HOME } /bin/app.exe } OPTS = $* srun ${ TASK } ${ OPTS } Serial Python #!/bin/bash -l #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" # Python 3.X by default (also on system) module load lang/Python # module load lang/SciPy-bundle # and/or: activate the virtualenv <name> you previously generated with # python -m venv <name> source ./<name>/bin/activate OPTS = $* srun python [ ... ] ${ OPTS } R #!/bin/bash -l #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 28 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load lang/R export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun Rscript <script>.R ${ OPTS } |& tee job_ ${ SLURM_JOB_NAME } .out Matlab ... but why? just use Python or R. #!/bin/bash -l #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 28 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load math/MATLAB matlab -nodisplay -nosplash < INPUTFILE.m > OUTPUTFILE.out Specialized BigData/GPU launchers \u00b6 BigData/[Large-]memory single-core tasks #!/bin/bash -l ### Request one sequential task requiring half the memory of a regular iris node for 1 day #SBATCH -J MyLargeMemorySequentialJob # Job name #SBATCH --mail-user=Your.Email@Address.lu # mail me ... #SBATCH --mail-type=end,fail # ... upon end or failure #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --mem=64GB # if above 112GB: consider bigmem partition (USE WITH CAUTION) #SBATCH --time=1-00:00:00 #SBATCH -p batch # if above 112GB: consider bigmem partition (USE WITH CAUTION) print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load <...> # [...] AI/DL task tasks #!/bin/bash -l ### Request one GPU tasks for 4 hours - dedicate 1/4 of available cores for its management #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 7 #SBATCH -G 1 #SBATCH --time=04:00:00 #SBATCH -p gpu print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load <...> # USE apps compiled against the {foss,intel}cuda toolchain ! # Ex: # module load numlib/cuDNN # This should report a single GPU (over 4 available per gpu node) nvidia-smi # [...] srun [ ... ] pthreads/OpenMP Launcher \u00b6 Always set OMP_NUM_THREADS to match ${SLURM_CPUS_PER_TASK:-1} You MUST enforce the use of -c <threads> in your launcher to ensure the variable $SLURM_CPUS_PER_TASK exists within your launcher scripts. This is the appropriate value to set for OMP_NUM_THREAD , with default to 1 as extra safely which can be obtained with the following affectation: export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } Aion (default Dual-CPU) Single node, threaded (pthreads/OpenMP) application launcher #!/bin/bash -l # Single node, threaded (pthreads/OpenMP) application launcher, using all 128 cores of an aion cluster node #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 128 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun /path/to/your/threaded.app ${ OPTS } Iris (default Dual-CPU) Single node, threaded (pthreads/OpenMP) application launcher #!/bin/bash -l # Single node, threaded (pthreads/OpenMP) application launcher, using all 28 cores of an iris cluster node: #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 28 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun /path/to/your/threaded.app ${ OPTS } MPI \u00b6 Intel MPI Launchers \u00b6 Official Slurm guide for Intel MPI Aion (default Dual-CPU) Multi-node parallel application IntelMPI launcher #!/bin/bash -l # Multi-node parallel application IntelMPI launcher, using 256 MPI processes #SBATCH -N 2 #SBATCH --ntasks-per-node 128 # MPI processes per node #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/intel OPTS = $* srun -n $SLURM_NTASKS /path/to/your/intel-toolchain-compiled-application ${ OPTS } Recall to use si-bigmem to request an interactive job when testing your script. Iris (default Dual-CPU) Multi-node parallel application IntelMPI launcher #!/bin/bash -l # Multi-node parallel application IntelMPI launcher, using 56 MPI processes #SBATCH -N 2 #SBATCH --ntasks-per-node 28 # MPI processes per node #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/intel OPTS = $* srun -n $SLURM_NTASKS /path/to/your/intel-toolchain-compiled-application ${ OPTS } Recall to use si-gpu to request an interactive job when testing your script on a GPU node. You may want to use PMIx as MPI initiator -- use srun --mpi=list to list the available implementations (default: pmi2), and srun --mpi=pmix[_v3] [...] to use PMIx. OpenMPI Slurm Launchers \u00b6 Official Slurm guide for Open MPI Aion (default Dual-CPU) Multi-node parallel application OpenMPI launcher #!/bin/bash -l # Multi-node parallel application OpenMPI launcher, using 256 MPI processes #SBATCH -N 2 #SBATCH --ntasks-per-node 128 # MPI processes per node #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss module load mpi/OpenMPI OPTS = $* srun -n $SLURM_NTASKS /path/to/your/foss-toolchain-openMPIcompiled-application ${ OPTS } Iris (default Dual-CPU) Multi-node parallel application OpenMPI launcher #!/bin/bash -l # Multi-node parallel application OpenMPI launcher, using 56 MPI processes #SBATCH -N 2 #SBATCH --ntasks-per-node 28 # MPI processes per node #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss module load mpi/OpenMPI OPTS = $* srun -n $SLURM_NTASKS /path/to/your/foss-toolchain-openMPIcompiled-application ${ OPTS } Hybrid Intel MPI+OpenMP Launcher \u00b6 Aion (default Dual-CPU) Multi-node hybrid parallel application IntelMPI/OpenMP launcher #!/bin/bash -l # Multi-node hybrid application IntelMPI+OpenMP launcher, using 16 threads per socket(CPU) on 2 nodes (256 cores): #SBATCH -N 2 #SBATCH --ntasks-per-node 8 # MPI processes per node #SBATCH --ntasks-per-socket 1 # MPI processes per (virtual) processor #SBATCH -c 16 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/intel export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${ OPTS } Iris (default Dual-CPU) Multi-node hybrid parallel application IntelMPI/OpenMP launcher #!/bin/bash -l # Multi-node hybrid application IntelMPI+OpenMP launcher, using 14 threads per socket(CPU) on 2 nodes (56 cores): #SBATCH -N 2 #SBATCH --ntasks-per-node 2 # MPI processes per node #SBATCH --ntasks-per-socket 1 # MPI processes per processor #SBATCH -c 14 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/intel export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${ OPTS } Hybrid OpenMPI+OpenMP Launcher \u00b6 Aion (default Dual-CPU) Multi-node hybrid parallel application OpenMPI/OpenMP launcher #!/bin/bash -l # Multi-node hybrid application OpenMPI+OpenMP launcher, using 16 threads per socket(CPU) on 2 nodes (256 cores): #SBATCH -N 2 #SBATCH --ntasks-per-node 8 # MPI processes per node #SBATCH --ntasks-per-socket 1 # MPI processes per processor #SBATCH -c 16 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss module load mpi/OpenMPI export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${ OPTS } Iris (default Dual-CPU) Multi-node hybrid parallel application OpenMPI/OpenMP launcher #!/bin/bash -l # Multi-node hybrid application OpenMPI+OpenMP launcher, using 14 threads per socket(CPU) on 2 nodes (56 cores): #SBATCH -N 2 #SBATCH --ntasks-per-node 2 # MPI processes per node #SBATCH --ntasks-per-socket 1 # MPI processes per processor #SBATCH -c 14 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss module load mpi/OpenMPI export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${ OPTS }","title":"Launcher Scripts Examples"},{"location":"slurm/launchers/#slurm-launcher-examples","text":"ULHPC Tutorial / Getting Started ULHPC Tutorial / OpenMP/MPI When setting your default #SBATCH directive, always keep in mind your expected default resource allocation that would permit to submit your launchers without options sbatch <launcher> (you will be glad in a couple of month not to have to remember the options you need to pass) and try to stick to a single node (to avoid to accidentally induce a huge submission).","title":"Slurm Launcher Examples"},{"location":"slurm/launchers/#resource-allocation-guidelines","text":"General guidelines Always try to align resource specifications for your jobs with physical characteristics. Always prefer the use of --ntasks-per-{node,socket} over -n when defining your tasks allocation request to automatically scale appropriately upon multi-nodes submission with for instance sbatch -N 2 <launcher> . Launcher template: #!/bin/bash -l # <--- DO NOT FORGET '-l' to facilitate further access to ULHPC modules #SBATCH -p <partition> #SBATCH -p <partition> #SBATCH -N 1 #SBATCH -N 1 #SBATCH --ntasks-per-node=<n> #SBATCH --ntasks-per-node <#sockets * s> #SBATCH -c <thread> #SBATCH --ntasks-per-socket <s> #SBATCH -c <thread> This would define by default a total of <n> (left) or \\#sockets \\times \\#sockets \\times <s> (right) tasks per node , each on <thread> threads . You MUST ensure that either: <n> \\times \\times <thread> matches the number of cores avaiable on the target computing node (left), or <n> = \\#sockets \\times \\#sockets \\times <s> , and <s> \\times \\times <thread> matches the number of cores per socket available on the target computing node (right). See Specific Resource Allocation Node (type) #Nodes #Socket / #Cores RAM [GB] Features aion-[0001-0318] 318 8 / 128 256 batch,epyc iris-[001-108] 108 2 / 28 128 batch,broadwell iris-[109-168] 60 2 / 28 128 batch,skylake iris-[169-186] (GPU) 18 2 / 28 768 gpu,skylake,volta iris-[191-196] (GPU) 6 2 / 28 768 gpu,skylake,volta32 iris-[187-190] (Large-Memory) 4 4 / 112 3072 bigmem,skylake Aion (default Dual-CPU) 16 cores per socket and 8 (virtual) sockets (CPUs) per aion node. Examples: #SBATCH -p batch #SBATCH -p batch #SBATCH -p batch #SBATCH -N 1 #SBATCH -N 1 #SBATCH -N 1 #SBATCH --ntasks-per-node=128 #SBATCH --ntasks-per-node 16 #SBATCH --ntasks-per-node 8 #SBATCH --ntasks-per-socket 16 #SBATCH --ntasks-per-socket 2 #SBATCH --ntasks-per-socket 1 #SBATCH -c 1 #SBATCH -c 8 #SBATCH -c 16 Iris (default Dual-CPU) 14 cores per socket and 2 sockets (physical CPUs) per regular iris . Examples: #SBATCH -p batch #SBATCH -p batch #SBATCH -p batch #SBATCH -N 1 #SBATCH -N 1 #SBATCH -N 1 #SBATCH --ntasks-per-node=28 #SBATCH --ntasks-per-node 14 #SBATCH --ntasks-per-node 4 #SBATCH --ntasks-per-socket=14 #SBATCH --ntasks-per-socket 7 #SBATCH --ntasks-per-socket 2 #SBATCH -c 1 #SBATCH -c 2 #SBATCH -c 7 Iris (GPU) 14 cores per socket and 2 sockets (physical CPUs) per gpu iris , 4 GPU accelerator cards per node. You probably want to dedicate 1 task and \\frac{1}{4} \\frac{1}{4} of the available cores to the management of each GPU accelerator. Examples: #SBATCH -p gpu #SBATCH -p gpu #SBATCH -p gpu #SBATCH -N 1 #SBATCH -N 1 #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH --ntasks-per-node 2 #SBATCH --ntasks-per-node 4 #SBATCH -c 7 #SBATCH --ntasks-per-socket 1 #SBATCH --ntasks-per-socket 2 #SBATCH -G 1 #SBATCH -c 7 #SBATCH -c 7 #SBATCH -G 2 #SBATCH -G 4 Iris (Large-Memory) 28 cores per socket and 4 sockets (physical CPUs) per bigmem iris node. Examples: #SBATCH -p bigmem #SBATCH -p bigmem #SBATCH -p bigmem #SBATCH -N 1 #SBATCH -N 1 #SBATCH -N 1 #SBATCH --ntasks-per-node=4 #SBATCH --ntasks-per-node 8 #SBATCH --ntasks-per-node 16 #SBATCH --ntasks-per-socket=1 #SBATCH --ntasks-per-socket 2 #SBATCH --ntasks-per-socket 4 #SBATCH -c 28 #SBATCH -c 14 #SBATCH -c 7 You probably want to play with a single task but define the expected memory allocation with --mem=<size[units]> (Default units are megabytes - Different units can be specified using the suffix [K|M|G|T] )","title":"Resource allocation Guidelines"},{"location":"slurm/launchers/#basic-slurm-launcher-examples","text":"Single core task 1 task per job (Note: prefer GNU Parallel in that case - see below) #!/bin/bash -l # <--- DO NOT FORGET '-l' ### Request a single task using one core on one node for 5 minutes in the batch queue #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --time=0-00:05:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } # Safeguard for NOT running this launcher on access/login nodes module purge || print_error_and_exit \"No 'module' command\" # List modules required for execution of the task module load <...> # [...] Multiple Single core tasks 28 single-core tasks per job #!/bin/bash -l ### Request as many tasks as cores available on a single node for 3 hours #SBATCH -N 1 #SBATCH --ntasks-per-node=28 # On iris; for aion, use --ntasks-per-node=128 #SBATCH -c 1 #SBATCH --time=0-03:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load <...> # [...] Multithreaded parallel tasks 7 multithreaded tasks per job (4 threads each) #!/bin/bash -l ### Request as many tasks as cores available on a single node for 3 hours #SBATCH -N 1 #SBATCH --ntasks-per-node=7 # On iris; for aion, use --ntasks-per-node=32 #SBATCH -c 4 #SBATCH --time=0-03:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load <...> # [...]","title":"Basic Slurm Launcher Examples"},{"location":"slurm/launchers/#embarrassingly-parallel-tasks","text":"For many users, the reason to consider (or being encouraged) to offload their computing executions on a (remote) HPC or Cloud facility is tied to the limits reached by their computing devices (laptop or workstation). It is generally motivated by time constraints \"My computations take several hours/days to complete. On an HPC, it will last a few minutes, no?\" or search-space explorations: \"I need to check my application against a huge number of input pieces (files) - it worked on a few of them locally but takes ages for a single check. How to proceed on HPC?\" In most of the cases, your favorite Java application or R/python (custom) development scripts, iterated again over multiple input conditions, are inherently SERIAL : they are able to use only one core when executed. You thus deal with what is often call a Bag of (independent) tasks , also referred to as embarrassingly parallel tasks . In this case, you MUST NOT overload the job scheduler with a large number of small (single-core) jobs. Instead, you should use GNU Parallel which permits the effective management of such tasks in a way that optimize both the resource allocation and the completion time. More specifically, GNU Parallel is a tool for executing tasks in parallel, typically on a single machine. When coupled with the Slurm command srun, parallel becomes a powerful way of distributing a set of tasks amongst a number of workers. This is particularly useful when the number of tasks is significantly larger than the number of available workers (i.e. $SLURM_NTASKS ), and each tasks is independent of the others. ULHPC Tutorial: GNU Parallel launcher for Embarrassingly Parallel Jobs Luckily, we have prepared a generic GNU Parallel launcher that should be straight forward to adapt to your own workflow following our tutorial : Create a dedicated script run_<task> responsible to run your java/R/Python tasks while taking as argument the parameter of each run. You can inspire from run_stressme for instance. test it in interactive rename the generic launcher launcher.parallel.sh to launcher_<task>.sh , enable #SBATCH --dependency singleton set the jobname change TASK to point to the absolute path to run_<task> script set TASKLISTFILE to point to a files with the parameters to pass to your script for each task adapt eventually the #SBATCH --ntasks-per-node [...] and #SBATCH -c [...] to match your needs AND the hardware configs of a single node (28 cores on iris, 128 cores on Aion) -- see guidelines test a batch run -- stick to a single node to take the best out of one full node.","title":"Embarrassingly Parallel Tasks"},{"location":"slurm/launchers/#serial-task-script-launcher","text":"Serial Killer (Generic template) #!/bin/bash -l # <--- DO NOT FORGET '-l' #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" # C/C++: module load toolchain/intel # OR: module load toolchain/foss # Java: module load lang/Java/1.8 # Ruby/Perl/Rust...: module load lang/{Ruby,Perl,Rust...} # /!\\ ADAPT TASK variable accordingly - absolute path to the (serial) task to be executed TASK = ${ TASK := ${ HOME } /bin/app.exe } OPTS = $* srun ${ TASK } ${ OPTS } Serial Python #!/bin/bash -l #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" # Python 3.X by default (also on system) module load lang/Python # module load lang/SciPy-bundle # and/or: activate the virtualenv <name> you previously generated with # python -m venv <name> source ./<name>/bin/activate OPTS = $* srun python [ ... ] ${ OPTS } R #!/bin/bash -l #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 28 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load lang/R export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun Rscript <script>.R ${ OPTS } |& tee job_ ${ SLURM_JOB_NAME } .out Matlab ... but why? just use Python or R. #!/bin/bash -l #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 28 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load math/MATLAB matlab -nodisplay -nosplash < INPUTFILE.m > OUTPUTFILE.out","title":"Serial Task script Launcher"},{"location":"slurm/launchers/#specialized-bigdatagpu-launchers","text":"BigData/[Large-]memory single-core tasks #!/bin/bash -l ### Request one sequential task requiring half the memory of a regular iris node for 1 day #SBATCH -J MyLargeMemorySequentialJob # Job name #SBATCH --mail-user=Your.Email@Address.lu # mail me ... #SBATCH --mail-type=end,fail # ... upon end or failure #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --mem=64GB # if above 112GB: consider bigmem partition (USE WITH CAUTION) #SBATCH --time=1-00:00:00 #SBATCH -p batch # if above 112GB: consider bigmem partition (USE WITH CAUTION) print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load <...> # [...] AI/DL task tasks #!/bin/bash -l ### Request one GPU tasks for 4 hours - dedicate 1/4 of available cores for its management #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 7 #SBATCH -G 1 #SBATCH --time=04:00:00 #SBATCH -p gpu print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load <...> # USE apps compiled against the {foss,intel}cuda toolchain ! # Ex: # module load numlib/cuDNN # This should report a single GPU (over 4 available per gpu node) nvidia-smi # [...] srun [ ... ]","title":"Specialized BigData/GPU launchers"},{"location":"slurm/launchers/#pthreadsopenmp-launcher","text":"Always set OMP_NUM_THREADS to match ${SLURM_CPUS_PER_TASK:-1} You MUST enforce the use of -c <threads> in your launcher to ensure the variable $SLURM_CPUS_PER_TASK exists within your launcher scripts. This is the appropriate value to set for OMP_NUM_THREAD , with default to 1 as extra safely which can be obtained with the following affectation: export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } Aion (default Dual-CPU) Single node, threaded (pthreads/OpenMP) application launcher #!/bin/bash -l # Single node, threaded (pthreads/OpenMP) application launcher, using all 128 cores of an aion cluster node #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 128 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun /path/to/your/threaded.app ${ OPTS } Iris (default Dual-CPU) Single node, threaded (pthreads/OpenMP) application launcher #!/bin/bash -l # Single node, threaded (pthreads/OpenMP) application launcher, using all 28 cores of an iris cluster node: #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 28 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun /path/to/your/threaded.app ${ OPTS }","title":"pthreads/OpenMP Launcher"},{"location":"slurm/launchers/#mpi","text":"","title":"MPI"},{"location":"slurm/launchers/#intel-mpi-launchers","text":"Official Slurm guide for Intel MPI Aion (default Dual-CPU) Multi-node parallel application IntelMPI launcher #!/bin/bash -l # Multi-node parallel application IntelMPI launcher, using 256 MPI processes #SBATCH -N 2 #SBATCH --ntasks-per-node 128 # MPI processes per node #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/intel OPTS = $* srun -n $SLURM_NTASKS /path/to/your/intel-toolchain-compiled-application ${ OPTS } Recall to use si-bigmem to request an interactive job when testing your script. Iris (default Dual-CPU) Multi-node parallel application IntelMPI launcher #!/bin/bash -l # Multi-node parallel application IntelMPI launcher, using 56 MPI processes #SBATCH -N 2 #SBATCH --ntasks-per-node 28 # MPI processes per node #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/intel OPTS = $* srun -n $SLURM_NTASKS /path/to/your/intel-toolchain-compiled-application ${ OPTS } Recall to use si-gpu to request an interactive job when testing your script on a GPU node. You may want to use PMIx as MPI initiator -- use srun --mpi=list to list the available implementations (default: pmi2), and srun --mpi=pmix[_v3] [...] to use PMIx.","title":"Intel MPI Launchers"},{"location":"slurm/launchers/#openmpi-slurm-launchers","text":"Official Slurm guide for Open MPI Aion (default Dual-CPU) Multi-node parallel application OpenMPI launcher #!/bin/bash -l # Multi-node parallel application OpenMPI launcher, using 256 MPI processes #SBATCH -N 2 #SBATCH --ntasks-per-node 128 # MPI processes per node #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss module load mpi/OpenMPI OPTS = $* srun -n $SLURM_NTASKS /path/to/your/foss-toolchain-openMPIcompiled-application ${ OPTS } Iris (default Dual-CPU) Multi-node parallel application OpenMPI launcher #!/bin/bash -l # Multi-node parallel application OpenMPI launcher, using 56 MPI processes #SBATCH -N 2 #SBATCH --ntasks-per-node 28 # MPI processes per node #SBATCH -c 1 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss module load mpi/OpenMPI OPTS = $* srun -n $SLURM_NTASKS /path/to/your/foss-toolchain-openMPIcompiled-application ${ OPTS }","title":"OpenMPI Slurm Launchers"},{"location":"slurm/launchers/#hybrid-intel-mpiopenmp-launcher","text":"Aion (default Dual-CPU) Multi-node hybrid parallel application IntelMPI/OpenMP launcher #!/bin/bash -l # Multi-node hybrid application IntelMPI+OpenMP launcher, using 16 threads per socket(CPU) on 2 nodes (256 cores): #SBATCH -N 2 #SBATCH --ntasks-per-node 8 # MPI processes per node #SBATCH --ntasks-per-socket 1 # MPI processes per (virtual) processor #SBATCH -c 16 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/intel export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${ OPTS } Iris (default Dual-CPU) Multi-node hybrid parallel application IntelMPI/OpenMP launcher #!/bin/bash -l # Multi-node hybrid application IntelMPI+OpenMP launcher, using 14 threads per socket(CPU) on 2 nodes (56 cores): #SBATCH -N 2 #SBATCH --ntasks-per-node 2 # MPI processes per node #SBATCH --ntasks-per-socket 1 # MPI processes per processor #SBATCH -c 14 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/intel export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${ OPTS }","title":"Hybrid Intel MPI+OpenMP Launcher"},{"location":"slurm/launchers/#hybrid-openmpiopenmp-launcher","text":"Aion (default Dual-CPU) Multi-node hybrid parallel application OpenMPI/OpenMP launcher #!/bin/bash -l # Multi-node hybrid application OpenMPI+OpenMP launcher, using 16 threads per socket(CPU) on 2 nodes (256 cores): #SBATCH -N 2 #SBATCH --ntasks-per-node 8 # MPI processes per node #SBATCH --ntasks-per-socket 1 # MPI processes per processor #SBATCH -c 16 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss module load mpi/OpenMPI export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${ OPTS } Iris (default Dual-CPU) Multi-node hybrid parallel application OpenMPI/OpenMP launcher #!/bin/bash -l # Multi-node hybrid application OpenMPI+OpenMP launcher, using 14 threads per socket(CPU) on 2 nodes (56 cores): #SBATCH -N 2 #SBATCH --ntasks-per-node 2 # MPI processes per node #SBATCH --ntasks-per-socket 1 # MPI processes per processor #SBATCH -c 14 #SBATCH --time=0-01:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load toolchain/foss module load mpi/OpenMPI export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK :- 1 } OPTS = $* srun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${ OPTS }","title":"Hybrid OpenMPI+OpenMP Launcher"},{"location":"slurm/partitions/","text":"ULHPC Slurm Partitions 2.0 \u00b6 In Slurm multiple nodes can be grouped into partitions which are sets of nodes aggregated by shared characteristics or objectives, with associated limits for wall-clock time, job size, etc. These limits are hard limits for the jobs and can not be overruled. To select a given partition with a Slurm command , use the -p <partition> option: srun|sbatch|salloc|sinfo|squeue... -p <partition> [...] You will find on ULHPC resources the following partitions (mostly matching the 3 types of computing resources) batch is intended for running parallel scientific applications as passive jobs on \" regular \" nodes (Dual CPU, no accelerators, 128 to 256 GB of RAM) gpu is intended for running GPU-accelerated scientific applications as passive jobs on \" gpu \" nodes (Dual CPU, 4 Nvidia accelerators, 768 GB RAM) bigmem is dedicated for memory intensive data processing jobs on \" bigmem \" nodes (Quad-CPU, no accelerators, 3072 GB RAM) interactive : a floating partition intended for quick interactive jobs , allowing for quick tests and compilation/preparation work. this is the only partition crossing all type of nodes (thus floating ). use si , si-gpu or si-bigmem to submit an interactive job on either a regular, gpu or bigmem node Aion \u00b6 AION (type) #Nodes (cores/node) Default/MaxTime MaxNodes PriorityTier interactive (floating) 318 30min - 2h 2 100 batch (default) 318 (128c) 2h - 48h 64 1 Iris \u00b6 IRIS (type) #Nodes (cores/n) Default/MaxTime MaxNodes PriorityTier interactive (floating) 196 30min - 2h 2 100 batch (default) 168 (28c) 2h - 48h 64 1 gpu 24 (28c) 2h - 48h 4 1 bigmem 4 (112c) 2h - 48h 1 1 Queues/Partitions State Information \u00b6 For detailed information about all available partitions and their definition/limits: scontrol show partitions [name] Partition load status \u00b6 You can of course use squeue -p <partition> to list the jobs currently scheduled on a given, partition <partition> . As part of the custom ULHPC Slurm helpers defined in /etc/profile.d/slurm.sh , the following commands have been made to facilitate the review of the current load usage of the partitions. Command Description irisstat , aionstat report cluster status (utilization, partition and QOS live stats) pload [-a] i/b/g/m Overview of the Slurm partition load listpartitionjobs <part> List jobs (and current load) of the slurm partition <part> Partition load with pload $ pload -h Usage: pload [-a] [--no-header] <partition> => Show current load of the slurm partition <partition>, eventually without header <partition> shortcuts: i=interactive b=batch g=gpu m=bigmem Options: -a: show all partition $ pload -a Partition CPU Max CPU Used CPU Free Usage[%] batch 4704 4223 481 89.8% gpu 672 412 260 61.3% GPU: 61/96 (63.5%) bigmem 448 431 17 96.2% Partition Limits \u00b6 At partition level, only the following limits can be enforced: DefaultTime : Default time limit MaxNodes : Maximum number of nodes per job MinNodes : Minimum number of nodes per job MaxCPUsPerNode : Maximum number of CPUs job can be allocated on any node MaxMemPerCPU/Node : Maximum memory job can be allocated on any CPU or node MaxTime : Maximum length of time user's job can run","title":"Partition/Queues"},{"location":"slurm/partitions/#ulhpc-slurm-partitions-20","text":"In Slurm multiple nodes can be grouped into partitions which are sets of nodes aggregated by shared characteristics or objectives, with associated limits for wall-clock time, job size, etc. These limits are hard limits for the jobs and can not be overruled. To select a given partition with a Slurm command , use the -p <partition> option: srun|sbatch|salloc|sinfo|squeue... -p <partition> [...] You will find on ULHPC resources the following partitions (mostly matching the 3 types of computing resources) batch is intended for running parallel scientific applications as passive jobs on \" regular \" nodes (Dual CPU, no accelerators, 128 to 256 GB of RAM) gpu is intended for running GPU-accelerated scientific applications as passive jobs on \" gpu \" nodes (Dual CPU, 4 Nvidia accelerators, 768 GB RAM) bigmem is dedicated for memory intensive data processing jobs on \" bigmem \" nodes (Quad-CPU, no accelerators, 3072 GB RAM) interactive : a floating partition intended for quick interactive jobs , allowing for quick tests and compilation/preparation work. this is the only partition crossing all type of nodes (thus floating ). use si , si-gpu or si-bigmem to submit an interactive job on either a regular, gpu or bigmem node","title":"ULHPC Slurm Partitions 2.0"},{"location":"slurm/partitions/#aion","text":"AION (type) #Nodes (cores/node) Default/MaxTime MaxNodes PriorityTier interactive (floating) 318 30min - 2h 2 100 batch (default) 318 (128c) 2h - 48h 64 1","title":"Aion"},{"location":"slurm/partitions/#iris","text":"IRIS (type) #Nodes (cores/n) Default/MaxTime MaxNodes PriorityTier interactive (floating) 196 30min - 2h 2 100 batch (default) 168 (28c) 2h - 48h 64 1 gpu 24 (28c) 2h - 48h 4 1 bigmem 4 (112c) 2h - 48h 1 1","title":"Iris"},{"location":"slurm/partitions/#queuespartitions-state-information","text":"For detailed information about all available partitions and their definition/limits: scontrol show partitions [name]","title":"Queues/Partitions State Information"},{"location":"slurm/partitions/#partition-load-status","text":"You can of course use squeue -p <partition> to list the jobs currently scheduled on a given, partition <partition> . As part of the custom ULHPC Slurm helpers defined in /etc/profile.d/slurm.sh , the following commands have been made to facilitate the review of the current load usage of the partitions. Command Description irisstat , aionstat report cluster status (utilization, partition and QOS live stats) pload [-a] i/b/g/m Overview of the Slurm partition load listpartitionjobs <part> List jobs (and current load) of the slurm partition <part> Partition load with pload $ pload -h Usage: pload [-a] [--no-header] <partition> => Show current load of the slurm partition <partition>, eventually without header <partition> shortcuts: i=interactive b=batch g=gpu m=bigmem Options: -a: show all partition $ pload -a Partition CPU Max CPU Used CPU Free Usage[%] batch 4704 4223 481 89.8% gpu 672 412 260 61.3% GPU: 61/96 (63.5%) bigmem 448 431 17 96.2%","title":"Partition load status"},{"location":"slurm/partitions/#partition-limits","text":"At partition level, only the following limits can be enforced: DefaultTime : Default time limit MaxNodes : Maximum number of nodes per job MinNodes : Minimum number of nodes per job MaxCPUsPerNode : Maximum number of CPUs job can be allocated on any node MaxMemPerCPU/Node : Maximum memory job can be allocated on any CPU or node MaxTime : Maximum length of time user's job can run","title":"Partition Limits"},{"location":"slurm/qos/","text":"ULHPC Slurm QOS 2.0 \u00b6 Quality of Service or QOS is used to constrain or modify the characteristics that a job can have. This could come in the form of specifying a QoS to request for a longer run time or a high priority queue for a given job. To select a given QOS with a Slurm command , use the --qos <qos> option: srun|sbatch|salloc|sinfo|squeue... [-p <partition>] --qos <qos> [...] The default QoS of your jobs depends on your account and affiliation. Normally, the --qos <qos> directive does not need to be set for most jobs We favor in general cross-partition QOS , mainly tied to priority level ( low \\rightarrow \\rightarrow urgent ). A special preemptible QOS exists for best-effort jobs and is named besteffort . Available QOS \u00b6 QOS (partition) Prio GrpTRES MaxTresPJ MaxJobPU MaxWall besteffort (*) 1 50 low (*) 10 2 normal (*) 100 50 long (*) 100 node=6 node=2 4 14-00:00:00 debug ( interactive ) 150 node=8 10 high (*) 200 50 urgent (*) 1000 100 List QOS Limits \u00b6 Use the sqos utility function to list the existing QOS limits. List current ULHPC QOS limits with sqos $ sqos \\# sacctmgr show qos format=\"name%20,preempt,priority,GrpTRES,MaxTresPerJob,MaxJobsPerUser,MaxWall,flags\" Name Preempt Priority GrpTRES MaxTRES MaxJobsPU MaxWall Flags -------------------- ---------- ---------- ------------- ------------- --------- ----------- -------------------- normal besteffort 100 50 DenyOnLimit besteffort 1 100 NoReserve low besteffort 10 2 DenyOnLimit high besteffort 200 50 DenyOnLimit urgent besteffort 1000 100 DenyOnLimit debug besteffort 150 node=8 10 DenyOnLimit long besteffort 100 node=6 node=2 4 14-00:00:00 DenyOnLimit,Partiti+ What are the possible limits set on ULHPC QOS? At the QOS level, the following elements are composed to define the resource limits for our QOS: Limits on Trackable RESources ( TRES - a resource (cpu,node,etc.) tracked for usage or used to enforce limits against), in particular: GrpTRES : The total count of TRES able to be used at any given time from jobs running from the QOS. If this limit is reached new jobs will be queued but only allowed to run after resources have been relinquished from this group. MaxTresPerJob : the maximum size in TRES (cpu,nodes,...) any given job can have from the QOS MaxJobsPerUser : The maximum number of jobs a user can have running at a given time MaxWall[DurationPerJob] = The maximum wall clock time any individual job can run for in the given QOS. As explained in the Limits section, there are basically three layers of Slurm limits, from least to most priority: None partitions account associations: Root/Cluster -> Account (ascending the hierarchy) -> User Job/Partition QOS","title":"Quality of Service (QOS)"},{"location":"slurm/qos/#ulhpc-slurm-qos-20","text":"Quality of Service or QOS is used to constrain or modify the characteristics that a job can have. This could come in the form of specifying a QoS to request for a longer run time or a high priority queue for a given job. To select a given QOS with a Slurm command , use the --qos <qos> option: srun|sbatch|salloc|sinfo|squeue... [-p <partition>] --qos <qos> [...] The default QoS of your jobs depends on your account and affiliation. Normally, the --qos <qos> directive does not need to be set for most jobs We favor in general cross-partition QOS , mainly tied to priority level ( low \\rightarrow \\rightarrow urgent ). A special preemptible QOS exists for best-effort jobs and is named besteffort .","title":"ULHPC Slurm QOS 2.0"},{"location":"slurm/qos/#available-qos","text":"QOS (partition) Prio GrpTRES MaxTresPJ MaxJobPU MaxWall besteffort (*) 1 50 low (*) 10 2 normal (*) 100 50 long (*) 100 node=6 node=2 4 14-00:00:00 debug ( interactive ) 150 node=8 10 high (*) 200 50 urgent (*) 1000 100","title":"Available QOS"},{"location":"slurm/qos/#list-qos-limits","text":"Use the sqos utility function to list the existing QOS limits. List current ULHPC QOS limits with sqos $ sqos \\# sacctmgr show qos format=\"name%20,preempt,priority,GrpTRES,MaxTresPerJob,MaxJobsPerUser,MaxWall,flags\" Name Preempt Priority GrpTRES MaxTRES MaxJobsPU MaxWall Flags -------------------- ---------- ---------- ------------- ------------- --------- ----------- -------------------- normal besteffort 100 50 DenyOnLimit besteffort 1 100 NoReserve low besteffort 10 2 DenyOnLimit high besteffort 200 50 DenyOnLimit urgent besteffort 1000 100 DenyOnLimit debug besteffort 150 node=8 10 DenyOnLimit long besteffort 100 node=6 node=2 4 14-00:00:00 DenyOnLimit,Partiti+ What are the possible limits set on ULHPC QOS? At the QOS level, the following elements are composed to define the resource limits for our QOS: Limits on Trackable RESources ( TRES - a resource (cpu,node,etc.) tracked for usage or used to enforce limits against), in particular: GrpTRES : The total count of TRES able to be used at any given time from jobs running from the QOS. If this limit is reached new jobs will be queued but only allowed to run after resources have been relinquished from this group. MaxTresPerJob : the maximum size in TRES (cpu,nodes,...) any given job can have from the QOS MaxJobsPerUser : The maximum number of jobs a user can have running at a given time MaxWall[DurationPerJob] = The maximum wall clock time any individual job can run for in the given QOS. As explained in the Limits section, there are basically three layers of Slurm limits, from least to most priority: None partitions account associations: Root/Cluster -> Account (ascending the hierarchy) -> User Job/Partition QOS","title":"List QOS Limits"},{"location":"software/","text":"List of all softwares \u00b6 Software Versions Swsets Architectures Clusters Category Description ABAQUS 2018, 2021 2019b, 2020b broadwell, skylake, epyc iris, aion CFD/Finite element modelling Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ABINIT 9.4.1 2020b epyc aion Chemistry ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ABySS 2.2.5 2020b broadwell, epyc, skylake aion, iris Biology Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler ACTC 1.1 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries ACTC converts independent triangles into triangle strips or fans. ANSYS 19.4, 21.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. AOCC 3.1.0 2020b epyc aion Compilers AMD Optimized C/C++ & Fortran compilers (AOCC) based on LLVM 12.0 ASE 3.19.0, 3.20.1, 3.21.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE. ASE uses it automatically when installed. ATK 2.34.1, 2.36.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. Advisor 2019_update5 2019b broadwell, skylake iris Performance measurements Vectorization Optimization and Thread Prototyping - Vectorize & thread code or performance \u201cdies\u201d - Easy workflow + data + tips = faster code faster - Prioritize, Prototype & Predict performance gain Anaconda3 2020.02, 2020.11 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. ArmForge 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Armadillo 10.5.3, 9.900.1 2020b, 2019b broadwell, epyc, skylake aion, iris Numerical libraries Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Arrow 0.16.0 2019b broadwell, skylake iris Data processing Apache Arrow (incl. PyArrow Python bindings)), a cross-language development platform for in-memory data. Aspera-CLI 3.9.1, 3.9.6 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. Autoconf 2.69 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.1, 1.16.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Automake: GNU Standards-compliant Makefile generator Autotools 20180311, 20200321 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development This bundle collect the standard GNU build tools: Autoconf, Automake and libtool BEDTools 2.29.2, 2.30.0 2019b, 2020b broadwell, skylake, epyc iris, aion Biology BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.11.0, 2.9.0 2020b, 2019b broadwell, epyc, skylake aion, iris Biology Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion Biology BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Bazel 0.26.1, 0.29.1, 3.7.2 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. BioPerl 1.7.2, 1.7.8 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bison 3.3.2, 3.5.3, 3.7.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Boost.Python 1.74.0 2020b broadwell, epyc, skylake aion, iris Libraries Boost.Python is a C++ library which enables seamless interoperability between C++ and the Python programming language. Boost 1.71.0, 1.74.0 2019b, 2020b broadwell, skylake, epyc iris, aion Development Boost provides free peer-reviewed portable C++ source libraries. Bowtie2 2.3.5.1, 2.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. CGAL 4.14.1, 5.2 2019b, 2020b broadwell, skylake, epyc iris, aion Numerical libraries The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. CMake 3.15.3, 3.18.4, 3.20.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development CMake, the cross-platform, open-source build system. CMake is a family of tools designed to build, test and package software. CPLEX 12.10 2019b broadwell, skylake iris Mathematics IBM ILOG CPLEX Optimizer's mathematical programming technology enables analytical decision support for improving efficiency, reducing costs, and increasing profitability. CRYSTAL 17 2019b broadwell, skylake iris Chemistry The CRYSTAL package performs ab initio calculations of the ground state energy, energy gradient, electronic wave function and properties of periodic systems. Hartree-Fock or Kohn- Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used. CUDA 10.1.243, 11.1.1 2019b, 2020b gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. CUDAcore 11.1.1 2020b gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. Check 0.15.2 2020b gpu iris Libraries Check is a unit testing framework for C. It features a simple interface for defining unit tests, putting little in the way of the developer. Tests are run in a separate address space, so both assertion failures and code errors that cause segmentation faults or other signals can be caught. Test results are reportable in the following: Subunit, TAP, XML, and a generic logging format. Clang 11.0.1, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers C, C++, Objective-C compiler, based on LLVM. Does not include C++ standard library -- use libstdc++ from GCC. CubeGUI 4.4.4 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube graphical report explorer. CubeLib 4.4.4 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube general purpose C++ library component and command-line tools. CubeWriter 4.4.3 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube high-performance C writer library component. DB 18.1.32, 18.1.40 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DB_File 1.855 2020b broadwell, epyc, skylake aion, iris Data processing Perl5 access to Berkeley DB version 1.x. DBus 1.13.12, 1.13.18 2019b, 2020b broadwell, skylake, epyc iris, aion Development D-Bus is a message bus system, a simple way for applications to talk to one another. In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. DMTCP 2.5.2 2019b broadwell, skylake iris Utilities DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. Dakota 6.11.0, 6.15.0 2019b, 2020b broadwell, skylake iris Mathematics The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" Doxygen 1.8.16, 1.8.20 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. ELPA 2019.11.001, 2020.11.001 2019b, 2020b broadwell, epyc, skylake iris, aion Mathematics Eigenvalue SoLvers for Petaflop-Applications . EasyBuild 4.3.0, 4.3.3, 4.4.1, 4.4.2, 4.5.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. Eigen 3.3.7, 3.3.8, 3.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Elk 6.3.2, 7.0.12 2019b, 2020b broadwell, skylake, epyc iris, aion Physics An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.1, 6.7.6 2019b, 2020b broadwell, skylake, epyc iris, aion Physics Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. FFTW 3.3.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. FFmpeg 4.2.1, 4.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation A complete, cross-platform solution to record, convert and stream audio and video. FLAC 1.3.3 2020b broadwell, epyc, skylake, gpu aion, iris Libraries FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. FLTK 1.3.5 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FastQC 0.11.9 2019b, 2020b broadwell, skylake, epyc iris, aion Biology FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. Flask 1.1.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. This module includes the Flask extensions: Flask-Cors Flink 1.11.2 2020b broadwell, epyc, skylake aion, iris Development Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. FreeImage 3.18.0 2020b broadwell, epyc, skylake aion, iris Visualisation FreeImage is an Open Source library project for developers who would like to support popular graphics image formats like PNG, BMP, JPEG, TIFF and others as needed by today's multimedia applications. FreeImage is easy to use, fast, multithreading safe. FriBidi 1.0.10, 1.0.5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Programming Languages The Free Implementation of the Unicode Bidirectional Algorithm. GCC 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GDAL 3.0.2, 3.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. GDB 10.1, 9.1 2020b, 2019b broadwell, epyc, skylake aion, iris Debugging The GNU Project Debugger GDRCopy 2.1 2020b gpu iris Libraries A low-latency GPU memory copy library based on NVIDIA GPUDirect RDMA technology. GEOS 3.8.0, 3.9.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GLPK 4.65 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. GLib 2.62.0, 2.66.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation GLib is one of the base libraries of the GTK+ project GMP 6.1.2, 6.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. GObject-Introspection 1.63.1, 1.66.1 2019b, 2020b broadwell, skylake, epyc iris, aion Development GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. GPAW-setups 0.9.20000 2019b broadwell, skylake iris Chemistry PAW setup for the GPAW Density Functional Theory package. Users can install setups manually using 'gpaw install-data' or use setups from this package. The versions of GPAW and GPAW-setups can be intermixed. GPAW 20.1.0 2019b broadwell, skylake iris Chemistry GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). It uses real-space uniform grids and multigrid methods or atom-centered basis-functions. GROMACS 2019.4, 2019.6, 2020, 2021, 2021.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GSL 2.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. GTK+ 3.24.13, 3.24.23 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.38.2, 2.40.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. Ghostscript 9.50, 9.53.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Go 1.14.1, 1.16.6 2019b, 2020b broadwell, skylake, epyc iris, aion Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Guile 1.8.8, 2.2.4 2019b broadwell, skylake iris Programming Languages Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Gurobi 9.0.0, 9.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. HDF5 1.10.5, 1.10.7 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF 4.2.15 2020b broadwell, epyc, skylake, gpu aion, iris Data processing HDF (also known as HDF4) is a library and multi-object file format for storing and managing data between machines. HTSlib 1.10.2, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Biology A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix Hadoop 2.10.0 2020b broadwell, epyc, skylake aion, iris Utilities Hadoop MapReduce by Cloudera HarfBuzz 2.6.4, 2.6.7 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation HarfBuzz is an OpenType text shaping engine. Harminv 1.4.1 2019b broadwell, skylake iris Mathematics Harminv is a free program (and accompanying library) to solve the problem of harmonic inversion - given a discrete-time, finite-length signal that consists of a sum of finitely-many sinusoids (possibly exponentially decaying) in a given bandwidth, it determines the frequencies, decay constants, amplitudes, and phases of those sinusoids. Horovod 0.19.1, 0.22.0 2019b, 2020b broadwell, skylake, gpu iris Utilities Horovod is a distributed training framework for TensorFlow. Hypre 2.20.0 2020b broadwell, epyc, skylake aion, iris Numerical libraries Hypre is a library for solving large, sparse linear systems of equations on massively parallel computers. The problems of interest arise in the simulation codes being developed at LLNL and elsewhere to study physical phenomena in the defense, environmental, energy, and biological sciences. ICU 64.2, 67.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. ISL 0.23 2020b broadwell, epyc, skylake aion, iris Mathematics isl is a library for manipulating sets and relations of integer points bounded by linear constraints. ImageMagick 7.0.10-35, 7.0.9-5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation ImageMagick is a software suite to create, edit, compose, or convert bitmap images Inspector 2019_update5 2019b broadwell, skylake iris Utilities Intel Inspector XE is an easy to use memory error checker and thread checker for serial and parallel applications JasPer 2.0.14, 2.0.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. Java 1.8.0_241, 11.0.2, 13.0.2, 16.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Jellyfish 2.3.0 2019b broadwell, skylake iris Biology Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA. JsonCpp 1.9.3, 1.9.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. Julia 1.4.1, 1.6.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Julia is a high-level, high-performance dynamic programming language for numerical computing Keras 2.3.1, 2.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Mathematics Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. LAME 3.100 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. LLVM 10.0.1, 11.0.0, 9.0.0, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LMDB 0.9.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.0.10, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries tiff: Library and tools for reading and writing TIFF data files LittleCMS 2.11, 2.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Lua 5.1.5, 5.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. M4 1.4.18 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. MATLAB 2019b, 2020a, 2021a 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPC 1.2.1 2020b broadwell, epyc, skylake aion, iris Mathematics Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. It extends the principles of the IEEE-754 standard for fixed precision real floating point numbers to complex numbers, providing well-defined semantics for every operation. At the same time, speed of operation at high precision is a major design goal. MPFR 4.0.2, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. MUMPS 5.3.5 2020b broadwell, epyc, skylake aion, iris Mathematics A parallel sparse direct solver Mako 1.1.0, 1.1.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development A super-fast templating language that borrows the best ideas from the existing templating languages Mathematica 12.0.0, 12.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Maven 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Development Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. Meep 1.4.3 2019b broadwell, skylake iris Physics Meep (or MEEP) is a free finite-difference time-domain (FDTD) simulation software package developed at MIT to model electromagnetic systems. Mesa 19.1.7, 19.2.1, 20.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Meson 0.51.2, 0.55.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. Mesquite 2.3.0 2019b broadwell, skylake iris Mathematics Mesh-Quality Improvement Library NAMD 2.13 2019b broadwell, skylake iris Chemistry NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NASM 2.14.02, 2.15.05 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages NASM: General-purpose x86 assembler NCCL 2.4.8, 2.8.3 2019b, 2020b gpu iris Libraries The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NLopt 2.6.1, 2.6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. NSPR 4.21, 4.29 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.45, 3.57 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. Ninja 1.10.1, 1.9.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Utilities Ninja is a small build system with a focus on speed. OPARI2 2.0.5 2019b broadwell, skylake iris Performance measurements OPARI2, the successor of Forschungszentrum Juelich's OPARI, is a source-to-source instrumentation tool for OpenMP and hybrid codes. It surrounds OpenMP directives and runtime library calls with calls to the POMP2 measurement interface. OTF2 2.2 2019b broadwell, skylake iris Performance measurements The Open Trace Format 2 is a highly scalable, memory efficient event trace data format plus support library. It is the new standard trace format for Scalasca, Vampir, and TAU and is open for other tools. OpenBLAS 0.3.12, 0.3.7 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Numerical libraries OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. OpenCV 4.2.0, 4.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenEXR 2.5.5 2020b broadwell, epyc, skylake aion, iris Visualisation OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light & Magic for use in computer imaging applications OpenFOAM-Extend 4.1-20200408 2019b broadwell, skylake iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenFOAM 8, v1912 2020b, 2019b epyc, broadwell, skylake aion, iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenMPI 3.1.4, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion MPI The Open MPI Project is an open source MPI-3 implementation. PAPI 6.0.0 2019b, 2020b broadwell, skylake, epyc iris, aion Performance measurements PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PCRE2 10.33, 10.35 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.43, 8.44 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PDT 3.25 2019b broadwell, skylake iris Performance measurements Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program knowledge accessible to developers of static and dynamic analysis tools. PDT implements a standard program representation, the program database (PDB), that can be accessed in a uniform way through a class library supporting common PDB operations. PETSc 3.14.4 2020b broadwell, epyc, skylake aion, iris Numerical libraries PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. PGI 19.10 2019b broadwell, skylake iris Compilers C, C++ and Fortran compilers from The Portland Group - PGI PLUMED 2.5.3, 2.7.0 2019b, 2020b broadwell, skylake, epyc iris, aion Chemistry PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. POV-Ray 3.7.0.8 2020b broadwell, epyc, skylake aion, iris Visualisation The Persistence of Vision Raytracer, or POV-Ray, is a ray tracing program which generates images from a text-based scene description, and is available for a variety of computer platforms. POV-Ray is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports. PROJ 6.2.1, 7.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates Pango 1.44.7, 1.47.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParMETIS 4.0.3 2019b broadwell, skylake iris Mathematics ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes. ParMGridGen 1.0 2019b broadwell, skylake iris Mathematics ParMGridGen is an MPI-based parallel library that is based on the serial package MGridGen, that implements (serial) algorithms for obtaining a sequence of successive coarse grids that are well-suited for geometric multigrid methods. ParaView 5.6.2, 5.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation ParaView is a scientific parallel visualizer. Perl 5.30.0, 5.32.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Pillow 6.2.1, 8.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyOpenGL 3.1.5 2020b broadwell, epyc, skylake aion, iris Visualisation PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs. PyQt5 5.15.1 2020b broadwell, epyc, skylake aion, iris Visualisation PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company. This bundle includes PyQtWebEngine, a set of Python bindings for The Qt Company\u2019s Qt WebEngine framework. PyQtGraph 0.11.1 2020b broadwell, epyc, skylake aion, iris Visualisation PyQtGraph is a pure-python graphics and GUI library built on PyQt5/PySide2 and numpy. PyTorch-Geometric 1.6.3 2020b broadwell, epyc, skylake, gpu aion, iris Libraries PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch. PyTorch 1.4.0, 1.7.1, 1.8.1, 1.9.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyYAML 5.1.2, 5.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries PyYAML is a YAML parser and emitter for the Python programming language. Python 2.7.16, 2.7.18, 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Qt5 5.13.1, 5.14.2 2019b, 2020b broadwell, skylake, epyc iris, aion Development Qt is a comprehensive cross-platform C++ application framework. QuantumESPRESSO 6.7 2019b, 2020b broadwell, epyc, skylake iris, aion Chemistry Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). RDFlib 5.0.0 2020b broadwell, epyc, skylake, gpu aion, iris Libraries RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information. R 3.6.2, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages R is a free software environment for statistical computing and graphics. ReFrame 2.21, 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Development ReFrame is a framework for writing regression tests for HPC systems. Ruby 2.7.1, 2.7.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. Rust 1.37.0 2019b broadwell, skylake iris Programming Languages Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. SAMtools 1.10, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Biology SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. SCOTCH 6.0.9, 6.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. SDL2 2.0.14 2020b broadwell, epyc, skylake aion, iris Libraries SDL: Simple DirectMedia Layer, a cross-platform multimedia library SIONlib 1.7.6 2019b broadwell, skylake iris Libraries SIONlib is a scalable I/O library for parallel access to task-local files. The library not only supports writing and reading binary data to or from several thousands of processors into a single or a small number of physical files, but also provides global open and close functions to access SIONlib files in parallel. This package provides a stripped-down installation of SIONlib for use with performance tools (e.g., Score-P), with renamed symbols to avoid conflicts when an application using SIONlib itself is linked against a tool requiring a different SIONlib version. SLEPc 3.14.2 2020b broadwell, epyc, skylake aion, iris Numerical libraries SLEPc (Scalable Library for Eigenvalue Problem Computations) is a software library for the solution of large scale sparse eigenvalue problems on parallel computers. It is an extension of PETSc and can be used for either standard or generalized eigenproblems, with real or complex arithmetic. It can also be used for computing a partial SVD of a large, sparse, rectangular matrix, and to solve quadratic eigenvalue problems. SQLite 3.29.0, 3.33.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development SQLite: SQL Database Engine in a C Library SWIG 4.0.1, 4.0.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Salmon 1.1.0 2019b broadwell, skylake iris Biology Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data. Salome 8.5.0, 9.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion CFD/Finite element modelling The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. ScaLAPACK 2.0.2, 2.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. Scalasca 2.5 2019b broadwell, skylake iris Performance measurements Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks -- in particular those concerning communication and synchronization -- and offers guidance in exploring their causes. SciPy-bundle 2019.10, 2020.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Bundle of Python packages for scientific software Score-P 6.0 2019b broadwell, skylake iris Performance measurements The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Singularity 3.6.0, 3.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities SingularityCE is an open source container platform designed to be simple, fast, and secure. Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Spack 0.12.1 2019b, 2020b broadwell, skylake, epyc iris, aion Development Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Spark 2.4.3 2019b broadwell, skylake iris Development Spark is Hadoop MapReduce done in memory Stata 17 2020b broadwell, epyc, skylake aion, iris Mathematics Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics. SuiteSparse 5.8.1 2020b broadwell, epyc, skylake aion, iris Numerical libraries SuiteSparse is a collection of libraries manipulate sparse matrices. Sumo 1.3.1 2019b broadwell, skylake iris Utilities Sumo is an open source, highly portable, microscopic and continuous traffic simulation package designed to handle large road networks. Szip 2.1.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Szip compression software, providing lossless compression of scientific data Tcl 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Programming Languages Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. TensorFlow 1.15.5, 2.1.0, 2.4.1, 2.5.0 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Libraries An open-source software library for Machine Intelligence Theano 1.0.4, 1.1.2 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Mathematics Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Tk 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. Tkinter 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Programming Languages Tkinter module, built with the Python buildsystem TopHat 2.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Biology TopHat is a fast splice junction mapper for RNA-Seq reads. Trinity 2.10.0 2019b broadwell, skylake iris Biology Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-Seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-Seq reads. UCX 1.9.0 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Unified Communication X An open-source production grade communication framework for data centric and high-performance applications UDUNITS 2.2.26 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Physics UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. ULHPC-bd 2020b 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for BigData Analytics software in use on the UL HPC Facility ULHPC-bio 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management & processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2019b, 2020b 2019b, 2020b gpu iris System-level software Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Misc tools, incl. - perf: Performance tools - tools: General purpose tools UnZip 6.0 2020b broadwell, epyc, skylake, gpu aion, iris Utilities UnZip is an extraction utility for archives compressed in .zip format (also called \"zipfiles\"). Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own Zip program, our primary objectives have been portability and non-MSDOS functionality. VASP 5.4.4, 6.2.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VMD 1.9.4a51 2020b broadwell, epyc, skylake aion, iris Visualisation VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. VTK 8.2.0, 9.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VTune 2019_update8, 2020_update3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. Valgrind 3.15.0, 3.16.1 2019b, 2020b broadwell, skylake, epyc iris, aion Debugging Valgrind: Debugging and profiling tools VirtualGL 2.6.2 2019b broadwell, skylake iris Visualisation VirtualGL is an open source toolkit that gives any Linux or Unix remote display software the ability to run OpenGL applications with full hardware acceleration. Voro++ 0.4.6 2019b broadwell, skylake iris Mathematics Voro++ is a software library for carrying out three-dimensional computations of the Voronoi tessellation. A distinguishing feature of the Voro++ library is that it carries out cell-based calculations, computing the Voronoi cell for each particle individually. It is particularly well-suited for applications that rely on cell-based statistics, where features of Voronoi cells (eg. volume, centroid, number of faces) can be used to analyze a system of particles. Wannier90 3.1.0 2020b broadwell, epyc, skylake aion, iris Chemistry A tool for obtaining maximally-localised Wannier functions X11 20190717, 20201008 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The X Window System (X11) is a windowing system for bitmap displays XML-LibXML 2.0201, 2.0206 2019b, 2020b broadwell, skylake, epyc iris, aion Data processing Perl binding for libxml2 XZ 5.2.4, 5.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities xz: XZ utilities Xerces-C++ 3.2.2 2019b broadwell, skylake iris Libraries Xerces-C++ is a validating XML parser written in a portable subset of C++. Xerces-C++ makes it easy to give your application the ability to read and write XML data. A shared library is provided for parsing, generating, manipulating, and validating XML documents using the DOM, SAX, and SAX2 APIs. Xvfb 1.20.9 2020b broadwell, epyc, skylake, gpu aion, iris Visualisation Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory. YACS 0.1.8 2020b broadwell, epyc, skylake aion, iris Libraries YACS was created as a lightweight library to define and manage system configurations, such as those commonly found in software designed for scientific experimentation. These \"configurations\" typically cover concepts like hyperparameters used in training a machine learning model or configurable model hyperparameters, such as the depth of a convolutional neural network. Yasm 1.3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Yasm: Complete rewrite of the NASM assembler with BSD license Z3 4.8.10 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Z3 is a theorem prover from Microsoft Research. Zip 3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality ant 1.10.6, 1.10.7, 1.10.9 2019b, 2020b broadwell, skylake, epyc iris, aion Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. archspec 0.1.0 2019b broadwell, skylake iris Utilities A library for detecting, labeling, and reasoning about microarchitectures arpack-ng 3.7.0, 3.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion Numerical libraries ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. at-spi2-atk 2.34.1, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation AT-SPI 2 toolkit bridge at-spi2-core 2.34.0, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Assistive Technology Service Provider Interface. binutils 2.32, 2.35 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities binutils: GNU binary utilities bokeh 2.2.3 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Statistical and novel interactive HTML plots for Python bzip2 1.0.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.66.0, 7.72.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. cairo 1.16.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB cuDNN 7.6.4.38, 8.0.4.30, 8.0.5.39 2019b, 2020b gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. dask 2021.2.0 2020b broadwell, epyc, skylake, gpu aion, iris Data processing Dask natively scales Python. Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. double-conversion 3.1.4, 3.1.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. elfutils 0.183 2020b gpu iris Libraries The elfutils project provides libraries and tools for ELF files and DWARF data. expat 2.2.7, 2.2.9 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) flatbuffers-python 1.12 2020b broadwell, epyc, skylake, gpu aion, iris Development Python Flatbuffers runtime library. flatbuffers 1.12.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development FlatBuffers: Memory Efficient Serialization Library flex 2.6.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. fontconfig 2.13.1, 2.13.92 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Fontconfig is a library designed to provide system-wide font configuration, customization and application access. foss 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GCC based compiler toolchain with CUDA support , and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. freetype 2.10.1, 2.10.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gc 7.6.12 2019b broadwell, skylake iris Libraries The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new. gcccuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gettext 0.19.8.1, 0.20.1, 0.21 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation gflags 2.2.2 2019b broadwell, skylake iris Development The gflags package contains a C++ library that implements commandline flags processing. It includes built-in support for standard types such as string and the ability to define flags in the source file in which they are used. giflib 5.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. git 2.23.0, 2.28.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. glog 0.4.0 2019b broadwell, skylake iris Development A C++ implementation of the Google logging module. gmsh 4.4.0 2019b broadwell, skylake iris CFD/Finite element modelling Salome is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components. gmsh 4.8.4 2020b broadwell, epyc, skylake aion, iris Mathematics Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor. gnuplot 5.2.8, 5.4.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Portable interactive, function plotting utility gocryptfs 1.7.1, 2.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. gompi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. googletest 1.10.0 2019b broadwell, skylake iris Development Google's framework for writing C++ tests on a variety of platforms gperf 3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. groff 1.22.4 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Groff (GNU troff) is a typesetting system that reads plain text mixed with formatting commands and produces formatted output. gzip 1.10 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities gzip (GNU zip) is a popular data compression program as a replacement for compress h5py 2.10.0, 3.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. help2man 1.47.16, 1.47.4, 1.47.8 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. hwloc 1.11.12, 2.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion System-level software The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. hypothesis 4.44.2, 5.41.2, 5.41.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. iccifort 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Compilers Intel C, C++ & Fortran compilers iccifortcuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel C, C++ & Fortran compilers with CUDA toolkit iimpi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. imkl 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. impi 2018.5.288, 2019.9.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion MPI Intel MPI Library, compatible with MPICH ABI intel 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Toolchains (software stacks) Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI & Intel MKL, with CUDA toolkit intltool 0.51.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. itac 2019.4.036 2019b broadwell, skylake iris Utilities The Intel Trace Collector is a low-overhead tracing library that performs event-based tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. jemalloc 5.2.1 2019b broadwell, skylake iris Libraries jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. kallisto 0.46.1 2019b broadwell, skylake iris Biology kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. kim-api 2.1.3 2019b broadwell, skylake iris Chemistry Open Knowledgebase of Interatomic Models. KIM is an API and OpenKIM is a collection of interatomic models (potentials) for atomistic simulations. This is a library that can be used by simulation programs to get access to the models in the OpenKIM database. This EasyBuild only installs the API, the models can be installed with the package openkim-models, or the user can install them manually by running kim-api-collections-management install user MODELNAME or kim-api-collections-management install user OpenKIM to install them all. libGLU 9.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. libarchive 3.4.3 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Multi-format archive and compression library libcerf 1.13, 1.14 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. libctl 4.0.0 2019b broadwell, skylake iris Chemistry libctl is a free Guile-based library implementing flexible control files for scientific simulations. libdrm 2.4.102, 2.4.99 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libraries Direct Rendering Manager runtime library. libepoxy 1.5.4 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.11, 2.1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.2.1, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.2.5, 2.3.0 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.5.1, 1.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.2.0, 1.3.2 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Libraries libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 2019b, 2020b gpu iris Libraries Library to manipulate tensors on the GPU. libiconv 1.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.3, 2.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libmatheval 1.1.11 2019b broadwell, skylake iris Libraries GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libogg 1.3.4 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Ogg is a multimedia container format, and the native file and stream format for the Xiph.org multimedia codecs. libpciaccess 0.14, 0.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion System-level software Generic PCI access library. libpng 1.6.37 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries libpng is the official PNG reference library libreadline 8.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtirpc 1.3.1 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Libtirpc is a port of Suns Transport-Independent RPC library to Linux. libtool 2.4.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunistring 0.9.10 2019b broadwell, skylake iris Libraries This library provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard. libunwind 1.3.1, 1.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libvorbis 1.3.7 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressed audio format libwebp 1.1.0 2020b broadwell, epyc, skylake aion, iris Libraries WebP is a modern image format that provides superior lossless and lossy compression for images on the web. Using WebP, webmasters and web developers can create smaller, richer images that make the web faster. libxc 4.3.4, 5.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxml2 2.9.10, 2.9.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libraries Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libxslt 1.1.34 2019b broadwell, skylake iris Libraries Libxslt is the XSLT C library developed for the GNOME project (but usable outside of the Gnome platform). libyaml 0.2.2, 0.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries LibYAML is a YAML parser and emitter written in C. lxml 4.4.2 2019b broadwell, skylake iris Libraries The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. lz4 1.9.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core. It features an extremely fast decoder, with speed in multiple GB/s per core. magma 2.5.1, 2.5.4 2019b, 2020b gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. makeinfo 6.7 2020b broadwell, epyc, skylake, gpu aion, iris Development makeinfo is part of the Texinfo project, the official documentation format of the GNU project. matplotlib 3.1.1, 3.3.3 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Visualisation matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. molmod 1.4.5 2019b broadwell, skylake iris Mathematics MolMod is a Python library with many compoments that are useful to write molecular modeling programs. ncurses 6.0, 6.1, 6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. netCDF-Fortran 4.5.2, 4.5.3 2019b, 2020b broadwell, skylake, epyc iris, aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.1, 4.7.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. nettle 3.5.1, 3.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. networkx 2.5 2020b broadwell, epyc, skylake, gpu aion, iris Utilities NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. nodejs 12.19.0 2020b broadwell, epyc, skylake, gpu aion, iris Programming Languages Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. nsync 1.24.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development nsync is a C library that exports various synchronization primitives, such as mutexes numactl 2.0.12, 2.0.13 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. numba 0.52.0 2020b broadwell, epyc, skylake, gpu aion, iris Programming Languages Numba is an Open Source NumPy-aware optimizing compiler for Python sponsored by Continuum Analytics, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code. phonopy 2.2.0 2019b broadwell, skylake iris Libraries Phonopy is an open source package of phonon calculations based on the supercell approach. pixman 0.38.4, 0.40.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. pkg-config 0.29.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c pkg-config --libs --cflags glib-2.0 for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development pkgconfig is a Python module to interface with the pkg-config command line tool pocl 1.4, 1.6 2019b, 2020b gpu iris Libraries Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard protobuf-python 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Python Protocol Buffers runtime library. protobuf 2.5.0, 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Development Google Protocol Buffers pybind11 2.4.3, 2.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. re2c 1.2.1, 2.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. scikit-build 0.11.1 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Scikit-Build, or skbuild, is an improved build system generator for CPython C/C++/Fortran/Cython extensions. scikit-image 0.18.1 2020b broadwell, epyc, skylake, gpu aion, iris Visualisation scikit-image is a collection of algorithms for image processing. scikit-learn 0.23.2 2020b broadwell, epyc, skylake, gpu aion, iris Data processing Scikit-learn integrates machine learning algorithms in the tightly-knit scientific Python world, building upon numpy, scipy, and matplotlib. As a machine-learning module, it provides versatile tools for data mining and analysis in any field of science and engineering. It strives to be simple and efficient, accessible to everybody, and reusable in various contexts. scipy 1.4.1 2019b broadwell, skylake, gpu iris Mathematics SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension for Python. setuptools 41.0.1 2019b broadwell, skylake iris Development Easily download, build, install, upgrade, and uninstall Python packages snappy 1.1.7, 1.1.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. sparsehash 2.0.3, 2.0.4 2019b, 2020b broadwell, skylake, epyc iris, aion Development An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. spglib-python 1.16.0 2020b broadwell, epyc, skylake, gpu aion, iris Chemistry Spglib for Python. Spglib is a library for finding and handling crystal symmetries written in C. tbb 2019_U9, 2020.2, 2020.3 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. texinfo 6.7 2019b broadwell, skylake iris Development Texinfo is the official documentation format of the GNU project. tqdm 4.56.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries A fast, extensible progress bar for Python and CLI typing-extensions 3.7.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Development Typing Extensions \u2013 Backported and Experimental Type Hints for Python util-linux 2.34, 2.36 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Set of Linux utilities x264 20190925, 20201026 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.2, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xorg-macros 1.19.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development X.org macros utilities. xprop 1.2.4, 1.2.5 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information. yaff 1.6.0 2019b broadwell, skylake iris Chemistry Yaff stands for 'Yet another force field'. It is a pythonic force-field code. zlib 1.2.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd 1.4.5 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set.","title":"List of all softwares"},{"location":"software/#list-of-all-softwares","text":"Software Versions Swsets Architectures Clusters Category Description ABAQUS 2018, 2021 2019b, 2020b broadwell, skylake, epyc iris, aion CFD/Finite element modelling Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ABINIT 9.4.1 2020b epyc aion Chemistry ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ABySS 2.2.5 2020b broadwell, epyc, skylake aion, iris Biology Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler ACTC 1.1 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries ACTC converts independent triangles into triangle strips or fans. ANSYS 19.4, 21.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. AOCC 3.1.0 2020b epyc aion Compilers AMD Optimized C/C++ & Fortran compilers (AOCC) based on LLVM 12.0 ASE 3.19.0, 3.20.1, 3.21.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE. ASE uses it automatically when installed. ATK 2.34.1, 2.36.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. Advisor 2019_update5 2019b broadwell, skylake iris Performance measurements Vectorization Optimization and Thread Prototyping - Vectorize & thread code or performance \u201cdies\u201d - Easy workflow + data + tips = faster code faster - Prioritize, Prototype & Predict performance gain Anaconda3 2020.02, 2020.11 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. ArmForge 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Armadillo 10.5.3, 9.900.1 2020b, 2019b broadwell, epyc, skylake aion, iris Numerical libraries Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Arrow 0.16.0 2019b broadwell, skylake iris Data processing Apache Arrow (incl. PyArrow Python bindings)), a cross-language development platform for in-memory data. Aspera-CLI 3.9.1, 3.9.6 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. Autoconf 2.69 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.1, 1.16.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Automake: GNU Standards-compliant Makefile generator Autotools 20180311, 20200321 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development This bundle collect the standard GNU build tools: Autoconf, Automake and libtool BEDTools 2.29.2, 2.30.0 2019b, 2020b broadwell, skylake, epyc iris, aion Biology BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.11.0, 2.9.0 2020b, 2019b broadwell, epyc, skylake aion, iris Biology Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion Biology BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Bazel 0.26.1, 0.29.1, 3.7.2 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. BioPerl 1.7.2, 1.7.8 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bison 3.3.2, 3.5.3, 3.7.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Boost.Python 1.74.0 2020b broadwell, epyc, skylake aion, iris Libraries Boost.Python is a C++ library which enables seamless interoperability between C++ and the Python programming language. Boost 1.71.0, 1.74.0 2019b, 2020b broadwell, skylake, epyc iris, aion Development Boost provides free peer-reviewed portable C++ source libraries. Bowtie2 2.3.5.1, 2.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. CGAL 4.14.1, 5.2 2019b, 2020b broadwell, skylake, epyc iris, aion Numerical libraries The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. CMake 3.15.3, 3.18.4, 3.20.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development CMake, the cross-platform, open-source build system. CMake is a family of tools designed to build, test and package software. CPLEX 12.10 2019b broadwell, skylake iris Mathematics IBM ILOG CPLEX Optimizer's mathematical programming technology enables analytical decision support for improving efficiency, reducing costs, and increasing profitability. CRYSTAL 17 2019b broadwell, skylake iris Chemistry The CRYSTAL package performs ab initio calculations of the ground state energy, energy gradient, electronic wave function and properties of periodic systems. Hartree-Fock or Kohn- Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used. CUDA 10.1.243, 11.1.1 2019b, 2020b gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. CUDAcore 11.1.1 2020b gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. Check 0.15.2 2020b gpu iris Libraries Check is a unit testing framework for C. It features a simple interface for defining unit tests, putting little in the way of the developer. Tests are run in a separate address space, so both assertion failures and code errors that cause segmentation faults or other signals can be caught. Test results are reportable in the following: Subunit, TAP, XML, and a generic logging format. Clang 11.0.1, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers C, C++, Objective-C compiler, based on LLVM. Does not include C++ standard library -- use libstdc++ from GCC. CubeGUI 4.4.4 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube graphical report explorer. CubeLib 4.4.4 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube general purpose C++ library component and command-line tools. CubeWriter 4.4.3 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube high-performance C writer library component. DB 18.1.32, 18.1.40 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DB_File 1.855 2020b broadwell, epyc, skylake aion, iris Data processing Perl5 access to Berkeley DB version 1.x. DBus 1.13.12, 1.13.18 2019b, 2020b broadwell, skylake, epyc iris, aion Development D-Bus is a message bus system, a simple way for applications to talk to one another. In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. DMTCP 2.5.2 2019b broadwell, skylake iris Utilities DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. Dakota 6.11.0, 6.15.0 2019b, 2020b broadwell, skylake iris Mathematics The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" Doxygen 1.8.16, 1.8.20 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. ELPA 2019.11.001, 2020.11.001 2019b, 2020b broadwell, epyc, skylake iris, aion Mathematics Eigenvalue SoLvers for Petaflop-Applications . EasyBuild 4.3.0, 4.3.3, 4.4.1, 4.4.2, 4.5.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. Eigen 3.3.7, 3.3.8, 3.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Elk 6.3.2, 7.0.12 2019b, 2020b broadwell, skylake, epyc iris, aion Physics An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.1, 6.7.6 2019b, 2020b broadwell, skylake, epyc iris, aion Physics Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. FFTW 3.3.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. FFmpeg 4.2.1, 4.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation A complete, cross-platform solution to record, convert and stream audio and video. FLAC 1.3.3 2020b broadwell, epyc, skylake, gpu aion, iris Libraries FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. FLTK 1.3.5 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FastQC 0.11.9 2019b, 2020b broadwell, skylake, epyc iris, aion Biology FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. Flask 1.1.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. This module includes the Flask extensions: Flask-Cors Flink 1.11.2 2020b broadwell, epyc, skylake aion, iris Development Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. FreeImage 3.18.0 2020b broadwell, epyc, skylake aion, iris Visualisation FreeImage is an Open Source library project for developers who would like to support popular graphics image formats like PNG, BMP, JPEG, TIFF and others as needed by today's multimedia applications. FreeImage is easy to use, fast, multithreading safe. FriBidi 1.0.10, 1.0.5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Programming Languages The Free Implementation of the Unicode Bidirectional Algorithm. GCC 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GDAL 3.0.2, 3.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. GDB 10.1, 9.1 2020b, 2019b broadwell, epyc, skylake aion, iris Debugging The GNU Project Debugger GDRCopy 2.1 2020b gpu iris Libraries A low-latency GPU memory copy library based on NVIDIA GPUDirect RDMA technology. GEOS 3.8.0, 3.9.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GLPK 4.65 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. GLib 2.62.0, 2.66.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation GLib is one of the base libraries of the GTK+ project GMP 6.1.2, 6.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. GObject-Introspection 1.63.1, 1.66.1 2019b, 2020b broadwell, skylake, epyc iris, aion Development GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. GPAW-setups 0.9.20000 2019b broadwell, skylake iris Chemistry PAW setup for the GPAW Density Functional Theory package. Users can install setups manually using 'gpaw install-data' or use setups from this package. The versions of GPAW and GPAW-setups can be intermixed. GPAW 20.1.0 2019b broadwell, skylake iris Chemistry GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). It uses real-space uniform grids and multigrid methods or atom-centered basis-functions. GROMACS 2019.4, 2019.6, 2020, 2021, 2021.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GSL 2.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. GTK+ 3.24.13, 3.24.23 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.38.2, 2.40.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. Ghostscript 9.50, 9.53.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Go 1.14.1, 1.16.6 2019b, 2020b broadwell, skylake, epyc iris, aion Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Guile 1.8.8, 2.2.4 2019b broadwell, skylake iris Programming Languages Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Gurobi 9.0.0, 9.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. HDF5 1.10.5, 1.10.7 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF 4.2.15 2020b broadwell, epyc, skylake, gpu aion, iris Data processing HDF (also known as HDF4) is a library and multi-object file format for storing and managing data between machines. HTSlib 1.10.2, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Biology A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix Hadoop 2.10.0 2020b broadwell, epyc, skylake aion, iris Utilities Hadoop MapReduce by Cloudera HarfBuzz 2.6.4, 2.6.7 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation HarfBuzz is an OpenType text shaping engine. Harminv 1.4.1 2019b broadwell, skylake iris Mathematics Harminv is a free program (and accompanying library) to solve the problem of harmonic inversion - given a discrete-time, finite-length signal that consists of a sum of finitely-many sinusoids (possibly exponentially decaying) in a given bandwidth, it determines the frequencies, decay constants, amplitudes, and phases of those sinusoids. Horovod 0.19.1, 0.22.0 2019b, 2020b broadwell, skylake, gpu iris Utilities Horovod is a distributed training framework for TensorFlow. Hypre 2.20.0 2020b broadwell, epyc, skylake aion, iris Numerical libraries Hypre is a library for solving large, sparse linear systems of equations on massively parallel computers. The problems of interest arise in the simulation codes being developed at LLNL and elsewhere to study physical phenomena in the defense, environmental, energy, and biological sciences. ICU 64.2, 67.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. ISL 0.23 2020b broadwell, epyc, skylake aion, iris Mathematics isl is a library for manipulating sets and relations of integer points bounded by linear constraints. ImageMagick 7.0.10-35, 7.0.9-5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation ImageMagick is a software suite to create, edit, compose, or convert bitmap images Inspector 2019_update5 2019b broadwell, skylake iris Utilities Intel Inspector XE is an easy to use memory error checker and thread checker for serial and parallel applications JasPer 2.0.14, 2.0.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. Java 1.8.0_241, 11.0.2, 13.0.2, 16.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Jellyfish 2.3.0 2019b broadwell, skylake iris Biology Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA. JsonCpp 1.9.3, 1.9.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. Julia 1.4.1, 1.6.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Julia is a high-level, high-performance dynamic programming language for numerical computing Keras 2.3.1, 2.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Mathematics Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. LAME 3.100 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. LLVM 10.0.1, 11.0.0, 9.0.0, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LMDB 0.9.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.0.10, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries tiff: Library and tools for reading and writing TIFF data files LittleCMS 2.11, 2.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Lua 5.1.5, 5.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. M4 1.4.18 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. MATLAB 2019b, 2020a, 2021a 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPC 1.2.1 2020b broadwell, epyc, skylake aion, iris Mathematics Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. It extends the principles of the IEEE-754 standard for fixed precision real floating point numbers to complex numbers, providing well-defined semantics for every operation. At the same time, speed of operation at high precision is a major design goal. MPFR 4.0.2, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. MUMPS 5.3.5 2020b broadwell, epyc, skylake aion, iris Mathematics A parallel sparse direct solver Mako 1.1.0, 1.1.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development A super-fast templating language that borrows the best ideas from the existing templating languages Mathematica 12.0.0, 12.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Maven 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Development Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. Meep 1.4.3 2019b broadwell, skylake iris Physics Meep (or MEEP) is a free finite-difference time-domain (FDTD) simulation software package developed at MIT to model electromagnetic systems. Mesa 19.1.7, 19.2.1, 20.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Meson 0.51.2, 0.55.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. Mesquite 2.3.0 2019b broadwell, skylake iris Mathematics Mesh-Quality Improvement Library NAMD 2.13 2019b broadwell, skylake iris Chemistry NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NASM 2.14.02, 2.15.05 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages NASM: General-purpose x86 assembler NCCL 2.4.8, 2.8.3 2019b, 2020b gpu iris Libraries The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NLopt 2.6.1, 2.6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. NSPR 4.21, 4.29 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.45, 3.57 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. Ninja 1.10.1, 1.9.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Utilities Ninja is a small build system with a focus on speed. OPARI2 2.0.5 2019b broadwell, skylake iris Performance measurements OPARI2, the successor of Forschungszentrum Juelich's OPARI, is a source-to-source instrumentation tool for OpenMP and hybrid codes. It surrounds OpenMP directives and runtime library calls with calls to the POMP2 measurement interface. OTF2 2.2 2019b broadwell, skylake iris Performance measurements The Open Trace Format 2 is a highly scalable, memory efficient event trace data format plus support library. It is the new standard trace format for Scalasca, Vampir, and TAU and is open for other tools. OpenBLAS 0.3.12, 0.3.7 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Numerical libraries OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. OpenCV 4.2.0, 4.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenEXR 2.5.5 2020b broadwell, epyc, skylake aion, iris Visualisation OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light & Magic for use in computer imaging applications OpenFOAM-Extend 4.1-20200408 2019b broadwell, skylake iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenFOAM 8, v1912 2020b, 2019b epyc, broadwell, skylake aion, iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenMPI 3.1.4, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion MPI The Open MPI Project is an open source MPI-3 implementation. PAPI 6.0.0 2019b, 2020b broadwell, skylake, epyc iris, aion Performance measurements PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PCRE2 10.33, 10.35 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.43, 8.44 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PDT 3.25 2019b broadwell, skylake iris Performance measurements Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program knowledge accessible to developers of static and dynamic analysis tools. PDT implements a standard program representation, the program database (PDB), that can be accessed in a uniform way through a class library supporting common PDB operations. PETSc 3.14.4 2020b broadwell, epyc, skylake aion, iris Numerical libraries PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. PGI 19.10 2019b broadwell, skylake iris Compilers C, C++ and Fortran compilers from The Portland Group - PGI PLUMED 2.5.3, 2.7.0 2019b, 2020b broadwell, skylake, epyc iris, aion Chemistry PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. POV-Ray 3.7.0.8 2020b broadwell, epyc, skylake aion, iris Visualisation The Persistence of Vision Raytracer, or POV-Ray, is a ray tracing program which generates images from a text-based scene description, and is available for a variety of computer platforms. POV-Ray is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports. PROJ 6.2.1, 7.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates Pango 1.44.7, 1.47.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParMETIS 4.0.3 2019b broadwell, skylake iris Mathematics ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes. ParMGridGen 1.0 2019b broadwell, skylake iris Mathematics ParMGridGen is an MPI-based parallel library that is based on the serial package MGridGen, that implements (serial) algorithms for obtaining a sequence of successive coarse grids that are well-suited for geometric multigrid methods. ParaView 5.6.2, 5.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation ParaView is a scientific parallel visualizer. Perl 5.30.0, 5.32.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Pillow 6.2.1, 8.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyOpenGL 3.1.5 2020b broadwell, epyc, skylake aion, iris Visualisation PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs. PyQt5 5.15.1 2020b broadwell, epyc, skylake aion, iris Visualisation PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company. This bundle includes PyQtWebEngine, a set of Python bindings for The Qt Company\u2019s Qt WebEngine framework. PyQtGraph 0.11.1 2020b broadwell, epyc, skylake aion, iris Visualisation PyQtGraph is a pure-python graphics and GUI library built on PyQt5/PySide2 and numpy. PyTorch-Geometric 1.6.3 2020b broadwell, epyc, skylake, gpu aion, iris Libraries PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch. PyTorch 1.4.0, 1.7.1, 1.8.1, 1.9.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyYAML 5.1.2, 5.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries PyYAML is a YAML parser and emitter for the Python programming language. Python 2.7.16, 2.7.18, 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Qt5 5.13.1, 5.14.2 2019b, 2020b broadwell, skylake, epyc iris, aion Development Qt is a comprehensive cross-platform C++ application framework. QuantumESPRESSO 6.7 2019b, 2020b broadwell, epyc, skylake iris, aion Chemistry Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). RDFlib 5.0.0 2020b broadwell, epyc, skylake, gpu aion, iris Libraries RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information. R 3.6.2, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages R is a free software environment for statistical computing and graphics. ReFrame 2.21, 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Development ReFrame is a framework for writing regression tests for HPC systems. Ruby 2.7.1, 2.7.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. Rust 1.37.0 2019b broadwell, skylake iris Programming Languages Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. SAMtools 1.10, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Biology SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. SCOTCH 6.0.9, 6.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. SDL2 2.0.14 2020b broadwell, epyc, skylake aion, iris Libraries SDL: Simple DirectMedia Layer, a cross-platform multimedia library SIONlib 1.7.6 2019b broadwell, skylake iris Libraries SIONlib is a scalable I/O library for parallel access to task-local files. The library not only supports writing and reading binary data to or from several thousands of processors into a single or a small number of physical files, but also provides global open and close functions to access SIONlib files in parallel. This package provides a stripped-down installation of SIONlib for use with performance tools (e.g., Score-P), with renamed symbols to avoid conflicts when an application using SIONlib itself is linked against a tool requiring a different SIONlib version. SLEPc 3.14.2 2020b broadwell, epyc, skylake aion, iris Numerical libraries SLEPc (Scalable Library for Eigenvalue Problem Computations) is a software library for the solution of large scale sparse eigenvalue problems on parallel computers. It is an extension of PETSc and can be used for either standard or generalized eigenproblems, with real or complex arithmetic. It can also be used for computing a partial SVD of a large, sparse, rectangular matrix, and to solve quadratic eigenvalue problems. SQLite 3.29.0, 3.33.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development SQLite: SQL Database Engine in a C Library SWIG 4.0.1, 4.0.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Salmon 1.1.0 2019b broadwell, skylake iris Biology Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data. Salome 8.5.0, 9.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion CFD/Finite element modelling The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. ScaLAPACK 2.0.2, 2.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. Scalasca 2.5 2019b broadwell, skylake iris Performance measurements Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks -- in particular those concerning communication and synchronization -- and offers guidance in exploring their causes. SciPy-bundle 2019.10, 2020.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Bundle of Python packages for scientific software Score-P 6.0 2019b broadwell, skylake iris Performance measurements The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Singularity 3.6.0, 3.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities SingularityCE is an open source container platform designed to be simple, fast, and secure. Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Spack 0.12.1 2019b, 2020b broadwell, skylake, epyc iris, aion Development Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Spark 2.4.3 2019b broadwell, skylake iris Development Spark is Hadoop MapReduce done in memory Stata 17 2020b broadwell, epyc, skylake aion, iris Mathematics Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics. SuiteSparse 5.8.1 2020b broadwell, epyc, skylake aion, iris Numerical libraries SuiteSparse is a collection of libraries manipulate sparse matrices. Sumo 1.3.1 2019b broadwell, skylake iris Utilities Sumo is an open source, highly portable, microscopic and continuous traffic simulation package designed to handle large road networks. Szip 2.1.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Szip compression software, providing lossless compression of scientific data Tcl 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Programming Languages Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. TensorFlow 1.15.5, 2.1.0, 2.4.1, 2.5.0 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Libraries An open-source software library for Machine Intelligence Theano 1.0.4, 1.1.2 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Mathematics Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Tk 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. Tkinter 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Programming Languages Tkinter module, built with the Python buildsystem TopHat 2.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Biology TopHat is a fast splice junction mapper for RNA-Seq reads. Trinity 2.10.0 2019b broadwell, skylake iris Biology Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-Seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-Seq reads. UCX 1.9.0 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Unified Communication X An open-source production grade communication framework for data centric and high-performance applications UDUNITS 2.2.26 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Physics UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. ULHPC-bd 2020b 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for BigData Analytics software in use on the UL HPC Facility ULHPC-bio 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management & processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2019b, 2020b 2019b, 2020b gpu iris System-level software Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Misc tools, incl. - perf: Performance tools - tools: General purpose tools UnZip 6.0 2020b broadwell, epyc, skylake, gpu aion, iris Utilities UnZip is an extraction utility for archives compressed in .zip format (also called \"zipfiles\"). Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own Zip program, our primary objectives have been portability and non-MSDOS functionality. VASP 5.4.4, 6.2.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VMD 1.9.4a51 2020b broadwell, epyc, skylake aion, iris Visualisation VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. VTK 8.2.0, 9.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VTune 2019_update8, 2020_update3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. Valgrind 3.15.0, 3.16.1 2019b, 2020b broadwell, skylake, epyc iris, aion Debugging Valgrind: Debugging and profiling tools VirtualGL 2.6.2 2019b broadwell, skylake iris Visualisation VirtualGL is an open source toolkit that gives any Linux or Unix remote display software the ability to run OpenGL applications with full hardware acceleration. Voro++ 0.4.6 2019b broadwell, skylake iris Mathematics Voro++ is a software library for carrying out three-dimensional computations of the Voronoi tessellation. A distinguishing feature of the Voro++ library is that it carries out cell-based calculations, computing the Voronoi cell for each particle individually. It is particularly well-suited for applications that rely on cell-based statistics, where features of Voronoi cells (eg. volume, centroid, number of faces) can be used to analyze a system of particles. Wannier90 3.1.0 2020b broadwell, epyc, skylake aion, iris Chemistry A tool for obtaining maximally-localised Wannier functions X11 20190717, 20201008 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The X Window System (X11) is a windowing system for bitmap displays XML-LibXML 2.0201, 2.0206 2019b, 2020b broadwell, skylake, epyc iris, aion Data processing Perl binding for libxml2 XZ 5.2.4, 5.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities xz: XZ utilities Xerces-C++ 3.2.2 2019b broadwell, skylake iris Libraries Xerces-C++ is a validating XML parser written in a portable subset of C++. Xerces-C++ makes it easy to give your application the ability to read and write XML data. A shared library is provided for parsing, generating, manipulating, and validating XML documents using the DOM, SAX, and SAX2 APIs. Xvfb 1.20.9 2020b broadwell, epyc, skylake, gpu aion, iris Visualisation Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory. YACS 0.1.8 2020b broadwell, epyc, skylake aion, iris Libraries YACS was created as a lightweight library to define and manage system configurations, such as those commonly found in software designed for scientific experimentation. These \"configurations\" typically cover concepts like hyperparameters used in training a machine learning model or configurable model hyperparameters, such as the depth of a convolutional neural network. Yasm 1.3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Yasm: Complete rewrite of the NASM assembler with BSD license Z3 4.8.10 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Z3 is a theorem prover from Microsoft Research. Zip 3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality ant 1.10.6, 1.10.7, 1.10.9 2019b, 2020b broadwell, skylake, epyc iris, aion Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. archspec 0.1.0 2019b broadwell, skylake iris Utilities A library for detecting, labeling, and reasoning about microarchitectures arpack-ng 3.7.0, 3.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion Numerical libraries ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. at-spi2-atk 2.34.1, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation AT-SPI 2 toolkit bridge at-spi2-core 2.34.0, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Assistive Technology Service Provider Interface. binutils 2.32, 2.35 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities binutils: GNU binary utilities bokeh 2.2.3 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Statistical and novel interactive HTML plots for Python bzip2 1.0.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.66.0, 7.72.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. cairo 1.16.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB cuDNN 7.6.4.38, 8.0.4.30, 8.0.5.39 2019b, 2020b gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. dask 2021.2.0 2020b broadwell, epyc, skylake, gpu aion, iris Data processing Dask natively scales Python. Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. double-conversion 3.1.4, 3.1.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. elfutils 0.183 2020b gpu iris Libraries The elfutils project provides libraries and tools for ELF files and DWARF data. expat 2.2.7, 2.2.9 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) flatbuffers-python 1.12 2020b broadwell, epyc, skylake, gpu aion, iris Development Python Flatbuffers runtime library. flatbuffers 1.12.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development FlatBuffers: Memory Efficient Serialization Library flex 2.6.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. fontconfig 2.13.1, 2.13.92 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Fontconfig is a library designed to provide system-wide font configuration, customization and application access. foss 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GCC based compiler toolchain with CUDA support , and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. freetype 2.10.1, 2.10.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gc 7.6.12 2019b broadwell, skylake iris Libraries The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new. gcccuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gettext 0.19.8.1, 0.20.1, 0.21 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation gflags 2.2.2 2019b broadwell, skylake iris Development The gflags package contains a C++ library that implements commandline flags processing. It includes built-in support for standard types such as string and the ability to define flags in the source file in which they are used. giflib 5.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. git 2.23.0, 2.28.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. glog 0.4.0 2019b broadwell, skylake iris Development A C++ implementation of the Google logging module. gmsh 4.4.0 2019b broadwell, skylake iris CFD/Finite element modelling Salome is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components. gmsh 4.8.4 2020b broadwell, epyc, skylake aion, iris Mathematics Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor. gnuplot 5.2.8, 5.4.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Portable interactive, function plotting utility gocryptfs 1.7.1, 2.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. gompi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. googletest 1.10.0 2019b broadwell, skylake iris Development Google's framework for writing C++ tests on a variety of platforms gperf 3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. groff 1.22.4 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Groff (GNU troff) is a typesetting system that reads plain text mixed with formatting commands and produces formatted output. gzip 1.10 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities gzip (GNU zip) is a popular data compression program as a replacement for compress h5py 2.10.0, 3.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. help2man 1.47.16, 1.47.4, 1.47.8 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. hwloc 1.11.12, 2.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion System-level software The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. hypothesis 4.44.2, 5.41.2, 5.41.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. iccifort 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Compilers Intel C, C++ & Fortran compilers iccifortcuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel C, C++ & Fortran compilers with CUDA toolkit iimpi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. imkl 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. impi 2018.5.288, 2019.9.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion MPI Intel MPI Library, compatible with MPICH ABI intel 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Toolchains (software stacks) Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI & Intel MKL, with CUDA toolkit intltool 0.51.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. itac 2019.4.036 2019b broadwell, skylake iris Utilities The Intel Trace Collector is a low-overhead tracing library that performs event-based tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. jemalloc 5.2.1 2019b broadwell, skylake iris Libraries jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. kallisto 0.46.1 2019b broadwell, skylake iris Biology kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. kim-api 2.1.3 2019b broadwell, skylake iris Chemistry Open Knowledgebase of Interatomic Models. KIM is an API and OpenKIM is a collection of interatomic models (potentials) for atomistic simulations. This is a library that can be used by simulation programs to get access to the models in the OpenKIM database. This EasyBuild only installs the API, the models can be installed with the package openkim-models, or the user can install them manually by running kim-api-collections-management install user MODELNAME or kim-api-collections-management install user OpenKIM to install them all. libGLU 9.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. libarchive 3.4.3 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Multi-format archive and compression library libcerf 1.13, 1.14 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. libctl 4.0.0 2019b broadwell, skylake iris Chemistry libctl is a free Guile-based library implementing flexible control files for scientific simulations. libdrm 2.4.102, 2.4.99 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libraries Direct Rendering Manager runtime library. libepoxy 1.5.4 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.11, 2.1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.2.1, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.2.5, 2.3.0 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.5.1, 1.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.2.0, 1.3.2 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Libraries libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 2019b, 2020b gpu iris Libraries Library to manipulate tensors on the GPU. libiconv 1.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.3, 2.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libmatheval 1.1.11 2019b broadwell, skylake iris Libraries GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libogg 1.3.4 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Ogg is a multimedia container format, and the native file and stream format for the Xiph.org multimedia codecs. libpciaccess 0.14, 0.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion System-level software Generic PCI access library. libpng 1.6.37 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries libpng is the official PNG reference library libreadline 8.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtirpc 1.3.1 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Libtirpc is a port of Suns Transport-Independent RPC library to Linux. libtool 2.4.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunistring 0.9.10 2019b broadwell, skylake iris Libraries This library provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard. libunwind 1.3.1, 1.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libvorbis 1.3.7 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressed audio format libwebp 1.1.0 2020b broadwell, epyc, skylake aion, iris Libraries WebP is a modern image format that provides superior lossless and lossy compression for images on the web. Using WebP, webmasters and web developers can create smaller, richer images that make the web faster. libxc 4.3.4, 5.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxml2 2.9.10, 2.9.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libraries Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libxslt 1.1.34 2019b broadwell, skylake iris Libraries Libxslt is the XSLT C library developed for the GNOME project (but usable outside of the Gnome platform). libyaml 0.2.2, 0.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries LibYAML is a YAML parser and emitter written in C. lxml 4.4.2 2019b broadwell, skylake iris Libraries The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. lz4 1.9.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core. It features an extremely fast decoder, with speed in multiple GB/s per core. magma 2.5.1, 2.5.4 2019b, 2020b gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. makeinfo 6.7 2020b broadwell, epyc, skylake, gpu aion, iris Development makeinfo is part of the Texinfo project, the official documentation format of the GNU project. matplotlib 3.1.1, 3.3.3 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Visualisation matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. molmod 1.4.5 2019b broadwell, skylake iris Mathematics MolMod is a Python library with many compoments that are useful to write molecular modeling programs. ncurses 6.0, 6.1, 6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. netCDF-Fortran 4.5.2, 4.5.3 2019b, 2020b broadwell, skylake, epyc iris, aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.1, 4.7.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. nettle 3.5.1, 3.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. networkx 2.5 2020b broadwell, epyc, skylake, gpu aion, iris Utilities NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. nodejs 12.19.0 2020b broadwell, epyc, skylake, gpu aion, iris Programming Languages Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. nsync 1.24.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development nsync is a C library that exports various synchronization primitives, such as mutexes numactl 2.0.12, 2.0.13 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. numba 0.52.0 2020b broadwell, epyc, skylake, gpu aion, iris Programming Languages Numba is an Open Source NumPy-aware optimizing compiler for Python sponsored by Continuum Analytics, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code. phonopy 2.2.0 2019b broadwell, skylake iris Libraries Phonopy is an open source package of phonon calculations based on the supercell approach. pixman 0.38.4, 0.40.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. pkg-config 0.29.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c pkg-config --libs --cflags glib-2.0 for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development pkgconfig is a Python module to interface with the pkg-config command line tool pocl 1.4, 1.6 2019b, 2020b gpu iris Libraries Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard protobuf-python 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Python Protocol Buffers runtime library. protobuf 2.5.0, 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Development Google Protocol Buffers pybind11 2.4.3, 2.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. re2c 1.2.1, 2.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. scikit-build 0.11.1 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Scikit-Build, or skbuild, is an improved build system generator for CPython C/C++/Fortran/Cython extensions. scikit-image 0.18.1 2020b broadwell, epyc, skylake, gpu aion, iris Visualisation scikit-image is a collection of algorithms for image processing. scikit-learn 0.23.2 2020b broadwell, epyc, skylake, gpu aion, iris Data processing Scikit-learn integrates machine learning algorithms in the tightly-knit scientific Python world, building upon numpy, scipy, and matplotlib. As a machine-learning module, it provides versatile tools for data mining and analysis in any field of science and engineering. It strives to be simple and efficient, accessible to everybody, and reusable in various contexts. scipy 1.4.1 2019b broadwell, skylake, gpu iris Mathematics SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension for Python. setuptools 41.0.1 2019b broadwell, skylake iris Development Easily download, build, install, upgrade, and uninstall Python packages snappy 1.1.7, 1.1.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. sparsehash 2.0.3, 2.0.4 2019b, 2020b broadwell, skylake, epyc iris, aion Development An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. spglib-python 1.16.0 2020b broadwell, epyc, skylake, gpu aion, iris Chemistry Spglib for Python. Spglib is a library for finding and handling crystal symmetries written in C. tbb 2019_U9, 2020.2, 2020.3 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. texinfo 6.7 2019b broadwell, skylake iris Development Texinfo is the official documentation format of the GNU project. tqdm 4.56.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries A fast, extensible progress bar for Python and CLI typing-extensions 3.7.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Development Typing Extensions \u2013 Backported and Experimental Type Hints for Python util-linux 2.34, 2.36 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Set of Linux utilities x264 20190925, 20201026 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.2, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xorg-macros 1.19.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development X.org macros utilities. xprop 1.2.4, 1.2.5 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information. yaff 1.6.0 2019b broadwell, skylake iris Chemistry Yaff stands for 'Yet another force field'. It is a pythonic force-field code. zlib 1.2.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd 1.4.5 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set.","title":"List of all softwares"},{"location":"software/build/","text":"Compiling/Building your own software \u00b6 We try to provide within the ULHPC software sets the most used application among our users. It may however happen that you may find a given software you expect to use to be either missing among the available software sets , or provided in a version you considered not enough recent. In that case, the RECOMMENDED approach is to rely on Easybuild to EXTEND the available software set. Below are guidelines to support that case. Alternatively, you can of course follow the installation guidelines provided on the software website to compile it the way it should be. For that case, you MUST rely on the provided toolchains and compilers . In all cases, NEVER compile or build softawre from the ULHPC frontends! Always perform these actions from the expected compute node, either reserved within an interactive job or through a passive submission Missing or Outdated Software \u00b6 You should first search if an existing Easyconfig exists for the software: # Typical check for user on ULHPC clusters $ si # get an interactive job - use 'si-gpu' for GPU nodes on iris $ module load tools/EasyBuild $ eb -S <name> It shoud match the available software set versions summarized below: Name Type 2019b ( legacy ) 2020a 2020b ( prod ) 2021a 2021b ( devel ) GCCCore compiler 8.3.0 9.3.0 10.2.0 10.3.0 11.2.0 foss toolchain 2019b 2020a 2020b 2021a 2021b intel toolchain 2019b 2020a 2020b 2021a 2021b binutils 2.32 2.34 2.35 2.36 2.37 Python 3.7.4 (and 2.7.16) 3.8.2 (and 2.7.18) 3.8.6 3.9.2 3.9.6 LLVM compiler 9.0.1 10.0.1 11.0.0 11.1.0 12.0.1 OpenMPI MPI 3.1.4 4.0.3 4.0.5 4.1.1 4.1.2 You will then be confronted to the following cases. An existing easyconfigs exists for the target toolchain version \u00b6 You're lucky but this is very likely to happen (and justify to rely on streamline Easyconfigs ) Typical Example: CMake-<version>-GCCcore-<gccversion>.eb : depends on GCCcore, thus common to both foss and intel . The same happens with GCC Go-<version>.eb (no dependency on any toolchain) Boost-<version>-{gompi,iimpi}-<toolchainversion>.eb , derived toolchains, compliant with foss (resp. intel ) ones; GDAL-<version>-{foss,intel}-<toolchainversion>-Python-<pythonversion>.eb In that case, you MUST test the build in your home or in a shared project using the resif-load-{home,project}-swset-{prod,devel} helpers to set a consistent environment for your builds compilant with the ULHPC software sets layout (in particular with regards the $EASYBUILD_PREFIX and $MODULEPATH environment variables). See below for building instructions. An outdated easyconfig exists \u00b6 Then the easiest way is to adapt the existing easyconfig file for the target softare version AND one of the available toolchain version. You may want also to ensure an ongoing Pull-Request is not dealing with the version you're looking for. Assuming you're looking for the software <name> (first letter <letter (in lower case), for instance if <name>=NWChem , then <letter>=n ), first copy the existing easyconfig file in a convenient place # Create host directory for your custom easyconfigs $ mkdir -p ~/easyconfigs/<letter>/<name> $ eb -S <name> ` # find the complete path to the easyconfig file CFGS1 =[ ... ] /path/to/easyconfigs * $CFGS1 /<letter>/<name>/<name>-<oldversion> [ ... ] .eb * $CFGS1 /<letter>/<name>/<name>- [ ... ] .patch # Eventual Patch file # copy/paste the definition of the CFGS1 variable (top line) CFGS1 =[ ... ] /path/to/easyconfigs # copy the eb file cp $CFGS1 /<letter>/<name>/<name>-<oldversion> [ ... ] .eb ~/easyconfigs/<letter>/<name> Now ( eventually ) check on the software website for the most up-to-date version <version> of the software released. Adapt the filename of the copied easyconfig to match the target version / toolchain cd ~/easyconfigs/<letter>/<name> mv <name>-<oldversion> [ ... ] .eb <name>-<version> [ ... ] .eb Example cd ~/easyconfigs/n/NWCHem mv NWChem-7.0.0-intel-2019b-Python-3.7.4.eb NWChem-7.0.2-intel-2021b.eb # Target 2021b intel toolchain, no more need for python suffix Now you shall edit the content of the easyconfig -- you'll typically have to adapt the version of the dependencies and the checksum(s) to match the static versions set for the target toolchain, enforce https urls etc. Below is a past complex exemple illustrating the adaptation done for GDB --- g/GDB/GDB-8.3-GCCcore-8.2.0-Python-3.7.2.eb 2020-03-31 12:17:03.000000000 +0200 +++ g/GDB/GDB-9.1-GCCcore-8.3.0-Python-3.7.4.eb 2020-05-08 15:49:41.000000000 +0200 @@ -1,31 +1,36 @@ easyblock = 'ConfigureMake' name = 'GDB' -version = '8.3' +version = '9.1' versionsuffix = '-Python-%(pyver)s' -homepage = 'http://www.gnu.org/software/gdb/gdb.html' +homepage = 'https://www.gnu.org/software/gdb/gdb.html' description = \"The GNU Project Debugger\" -toolchain = {'name': 'GCCcore', 'version': '8.2.0'} +toolchain = {'name': 'GCCcore', 'version': '8.3.0'} source_urls = [GNU_SOURCE] sources = [SOURCELOWER_TAR_XZ] -checksums = ['802f7ee309dcc547d65a68d61ebd6526762d26c3051f52caebe2189ac1ffd72e'] +checksums = ['699e0ec832fdd2f21c8266171ea5bf44024bd05164fdf064e4d10cc4cf0d1737'] builddependencies = [ - ('binutils', '2.31.1'), - ('texinfo', '6.6'), + ('binutils', '2.32'), + ('texinfo', '6.7'), ] dependencies = [ ('zlib', '1.2.11'), ('libreadline', '8.0'), ('ncurses', '6.1'), - ('expat', '2.2.6'), - ('Python', '3.7.2'), + ('expat', '2.2.7'), + ('Python', '3.7.4'), ] +preconfigopts = \"mkdir obj && cd obj && \" +configure_cmd_prefix = '../' +prebuildopts = \"cd obj && \" +preinstallopts = prebuildopts + configopts = '--with-system-zlib --with-python=$EBROOTPYTHON/bin/python --with-expat=$EBROOTEXPAT ' configopts += '--with-system-readline --enable-tui --enable-plugins --disable-install-libbfd ' Note on dependencies version: typically as in the above example, the version to use for dependencies are not obvious to guess (Ex: texinfo , expat etc.) and you need to be aware of the matching toolchain/GCC/binutils versions for the available prod or devel software sets recalled before -- use eb -S <dependency> to find the appropriate versions. None (or only very old/obsolete) easyconfigs are suggested \u00b6 Don't panic, it simply means that the official repositories do not hold any recent reciPY for the considered software. You may find a pending Pull-request addressing the software you're looking for. Otherwise, you can either try to create a new easyconfig file, or simply follow the installation guildes for the considered software to build it. Using Easybuild to Build software in your Home \u00b6 See also Technical documentation to better understand the Easybuild configuration. If upon Dry-run builds ( eb -Dr [...] ) you find most dependencies NOT satisfied, you've likely made an error and may be trying to build a software against a toolchain/software set not supported either as prod or devel . # BETTER work in a screen or tmux session ;) $ si [ -gpu ] [ -c <threads> ] # get an interactive job $ module load tools/EasyBuild # /!\\ IMPORTANT: ensure EASYBUILD_PREFIX is correctly set to [basedir]/<cluster>/<environment>/<arch> # and that MODULEPATH is prefixed accordingly $ resif-load-home-swset- { prod | devel } # adapt environment $ eb -S <softwarename> # confirm <filename>.eb == <softwarename>-<version>[-<toolchain>][-<suffix>].eb $ eb -Dr <filename>.eb # check dependencies, normally most MUST be satisfied $ eb -r <filename>.eb From that point, the compiled software and associated module is available in your home and can be used as follows in launchers etc. -- see ULHPC launcher Examples #!/bin/bash -l # <--- DO NOT FORGET '-l' to facilitate further access to ULHPC modules #SBATCH -p <partition> #SBATCH -N 1 #SBATCH --ntasks-per-node <#sockets * s> #SBATCH --ntasks-per-socket <s> #SBATCH -c <thread> print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } # Safeguard for NOT running this launcher on access/login nodes module purge || print_error_and_exit \"No 'module' command\" resif-load-home-swset-prod # OR resif-load-home-swset-devel module load <softwarename> [ /<version> ] [ ... ] Using Easybuild to Build software in the project \u00b6 Similarly to the above home builds, you should repeat the procedure this time using the helper script resif-load-project-swset-{prod | devel} . Don't forget Project Data Management instructions : to avoid quotas issues, you have to use sg # BETTER work in a screen or tmux session ;) $ si [ -gpu ] [ -c <threads> ] # get an interactive job $ module load tools/EasyBuild # /!\\ IMPORTANT: ensure EASYBUILD_PREFIX is correctly set to [basedir]/<cluster>/<environment>/<arch> # and that MODULEPATH is prefixed accordingly $ resif-load-project-swset- { prod | devel } $PROJECTHOME /<project> # /!\\ ADAPT environment and <project> accordingly $ sg <project> -c \"eb -S <softwarename>\" # confirm <filename>.eb == <softwarename>-<v>-<toolchain>.eb $ sg <project> -c \"eb -Dr <filename>.eb\" # check dependencies, normally most MUST be satisfied $ sg <project> -c \"eb -r <filename>.eb\" From that point, the compiled software and associated module is available in the project directoryand can be used by all project members as follows in launchers etc. -- see ULHPC launcher Examples #!/bin/bash -l # <--- DO NOT FORGET '-l' to facilitate further access to ULHPC modules #SBATCH -p <partition> #SBATCH -N 1 #SBATCH --ntasks-per-node <#sockets * s> #SBATCH --ntasks-per-socket <s> #SBATCH -c <thread> print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } demonstrate # Safeguard for NOT running this launcher on access/login nodes module purge || print_error_and_exit \"No 'module' command\" resif-load-project-swset-prod $PROJECTHOME /<project> # OR resif-load-project-swset-devel $PROJECTHOME/<project> module load <softwarename> [ /<version> ] [ ... ] Contribute back to Easybuild \u00b6 If you developped new easyconfig(s), you are expected to contribute them back to the Easybuilders community! Consider creating a Pull-Request. You can even do it by command-line assuming you have setup your Github integration . On iris or aion , you will likely need to install the possibly-insecure, alternate keyrings keyrings.alt packages -- see https://pypi.org/project/keyring/ # checking code style - see https://easybuild.readthedocs.io/en/latest/Code_style.html#code-style eb --check-contrib <ebfile> eb --new-pr <ebfile> You can can also consider using the script PR-create provided as part of the RESIF 3 project. Once the pull request is merged, you can inform the ULHPC team to consider adding the submitted Easyconfig as part of the ULHPC bundles and see it deployed within the next ULHPC software set release .","title":"Compiling/building your own software"},{"location":"software/build/#compilingbuilding-your-own-software","text":"We try to provide within the ULHPC software sets the most used application among our users. It may however happen that you may find a given software you expect to use to be either missing among the available software sets , or provided in a version you considered not enough recent. In that case, the RECOMMENDED approach is to rely on Easybuild to EXTEND the available software set. Below are guidelines to support that case. Alternatively, you can of course follow the installation guidelines provided on the software website to compile it the way it should be. For that case, you MUST rely on the provided toolchains and compilers . In all cases, NEVER compile or build softawre from the ULHPC frontends! Always perform these actions from the expected compute node, either reserved within an interactive job or through a passive submission","title":"Compiling/Building your own software"},{"location":"software/build/#missing-or-outdated-software","text":"You should first search if an existing Easyconfig exists for the software: # Typical check for user on ULHPC clusters $ si # get an interactive job - use 'si-gpu' for GPU nodes on iris $ module load tools/EasyBuild $ eb -S <name> It shoud match the available software set versions summarized below: Name Type 2019b ( legacy ) 2020a 2020b ( prod ) 2021a 2021b ( devel ) GCCCore compiler 8.3.0 9.3.0 10.2.0 10.3.0 11.2.0 foss toolchain 2019b 2020a 2020b 2021a 2021b intel toolchain 2019b 2020a 2020b 2021a 2021b binutils 2.32 2.34 2.35 2.36 2.37 Python 3.7.4 (and 2.7.16) 3.8.2 (and 2.7.18) 3.8.6 3.9.2 3.9.6 LLVM compiler 9.0.1 10.0.1 11.0.0 11.1.0 12.0.1 OpenMPI MPI 3.1.4 4.0.3 4.0.5 4.1.1 4.1.2 You will then be confronted to the following cases.","title":"Missing or Outdated Software"},{"location":"software/build/#an-existing-easyconfigs-exists-for-the-target-toolchain-version","text":"You're lucky but this is very likely to happen (and justify to rely on streamline Easyconfigs ) Typical Example: CMake-<version>-GCCcore-<gccversion>.eb : depends on GCCcore, thus common to both foss and intel . The same happens with GCC Go-<version>.eb (no dependency on any toolchain) Boost-<version>-{gompi,iimpi}-<toolchainversion>.eb , derived toolchains, compliant with foss (resp. intel ) ones; GDAL-<version>-{foss,intel}-<toolchainversion>-Python-<pythonversion>.eb In that case, you MUST test the build in your home or in a shared project using the resif-load-{home,project}-swset-{prod,devel} helpers to set a consistent environment for your builds compilant with the ULHPC software sets layout (in particular with regards the $EASYBUILD_PREFIX and $MODULEPATH environment variables). See below for building instructions.","title":"An existing easyconfigs exists for the target toolchain version"},{"location":"software/build/#an-outdated-easyconfig-exists","text":"Then the easiest way is to adapt the existing easyconfig file for the target softare version AND one of the available toolchain version. You may want also to ensure an ongoing Pull-Request is not dealing with the version you're looking for. Assuming you're looking for the software <name> (first letter <letter (in lower case), for instance if <name>=NWChem , then <letter>=n ), first copy the existing easyconfig file in a convenient place # Create host directory for your custom easyconfigs $ mkdir -p ~/easyconfigs/<letter>/<name> $ eb -S <name> ` # find the complete path to the easyconfig file CFGS1 =[ ... ] /path/to/easyconfigs * $CFGS1 /<letter>/<name>/<name>-<oldversion> [ ... ] .eb * $CFGS1 /<letter>/<name>/<name>- [ ... ] .patch # Eventual Patch file # copy/paste the definition of the CFGS1 variable (top line) CFGS1 =[ ... ] /path/to/easyconfigs # copy the eb file cp $CFGS1 /<letter>/<name>/<name>-<oldversion> [ ... ] .eb ~/easyconfigs/<letter>/<name> Now ( eventually ) check on the software website for the most up-to-date version <version> of the software released. Adapt the filename of the copied easyconfig to match the target version / toolchain cd ~/easyconfigs/<letter>/<name> mv <name>-<oldversion> [ ... ] .eb <name>-<version> [ ... ] .eb Example cd ~/easyconfigs/n/NWCHem mv NWChem-7.0.0-intel-2019b-Python-3.7.4.eb NWChem-7.0.2-intel-2021b.eb # Target 2021b intel toolchain, no more need for python suffix Now you shall edit the content of the easyconfig -- you'll typically have to adapt the version of the dependencies and the checksum(s) to match the static versions set for the target toolchain, enforce https urls etc. Below is a past complex exemple illustrating the adaptation done for GDB --- g/GDB/GDB-8.3-GCCcore-8.2.0-Python-3.7.2.eb 2020-03-31 12:17:03.000000000 +0200 +++ g/GDB/GDB-9.1-GCCcore-8.3.0-Python-3.7.4.eb 2020-05-08 15:49:41.000000000 +0200 @@ -1,31 +1,36 @@ easyblock = 'ConfigureMake' name = 'GDB' -version = '8.3' +version = '9.1' versionsuffix = '-Python-%(pyver)s' -homepage = 'http://www.gnu.org/software/gdb/gdb.html' +homepage = 'https://www.gnu.org/software/gdb/gdb.html' description = \"The GNU Project Debugger\" -toolchain = {'name': 'GCCcore', 'version': '8.2.0'} +toolchain = {'name': 'GCCcore', 'version': '8.3.0'} source_urls = [GNU_SOURCE] sources = [SOURCELOWER_TAR_XZ] -checksums = ['802f7ee309dcc547d65a68d61ebd6526762d26c3051f52caebe2189ac1ffd72e'] +checksums = ['699e0ec832fdd2f21c8266171ea5bf44024bd05164fdf064e4d10cc4cf0d1737'] builddependencies = [ - ('binutils', '2.31.1'), - ('texinfo', '6.6'), + ('binutils', '2.32'), + ('texinfo', '6.7'), ] dependencies = [ ('zlib', '1.2.11'), ('libreadline', '8.0'), ('ncurses', '6.1'), - ('expat', '2.2.6'), - ('Python', '3.7.2'), + ('expat', '2.2.7'), + ('Python', '3.7.4'), ] +preconfigopts = \"mkdir obj && cd obj && \" +configure_cmd_prefix = '../' +prebuildopts = \"cd obj && \" +preinstallopts = prebuildopts + configopts = '--with-system-zlib --with-python=$EBROOTPYTHON/bin/python --with-expat=$EBROOTEXPAT ' configopts += '--with-system-readline --enable-tui --enable-plugins --disable-install-libbfd ' Note on dependencies version: typically as in the above example, the version to use for dependencies are not obvious to guess (Ex: texinfo , expat etc.) and you need to be aware of the matching toolchain/GCC/binutils versions for the available prod or devel software sets recalled before -- use eb -S <dependency> to find the appropriate versions.","title":"An outdated easyconfig exists"},{"location":"software/build/#none-or-only-very-oldobsolete-easyconfigs-are-suggested","text":"Don't panic, it simply means that the official repositories do not hold any recent reciPY for the considered software. You may find a pending Pull-request addressing the software you're looking for. Otherwise, you can either try to create a new easyconfig file, or simply follow the installation guildes for the considered software to build it.","title":"None (or only very old/obsolete) easyconfigs are suggested"},{"location":"software/build/#using-easybuild-to-build-software-in-your-home","text":"See also Technical documentation to better understand the Easybuild configuration. If upon Dry-run builds ( eb -Dr [...] ) you find most dependencies NOT satisfied, you've likely made an error and may be trying to build a software against a toolchain/software set not supported either as prod or devel . # BETTER work in a screen or tmux session ;) $ si [ -gpu ] [ -c <threads> ] # get an interactive job $ module load tools/EasyBuild # /!\\ IMPORTANT: ensure EASYBUILD_PREFIX is correctly set to [basedir]/<cluster>/<environment>/<arch> # and that MODULEPATH is prefixed accordingly $ resif-load-home-swset- { prod | devel } # adapt environment $ eb -S <softwarename> # confirm <filename>.eb == <softwarename>-<version>[-<toolchain>][-<suffix>].eb $ eb -Dr <filename>.eb # check dependencies, normally most MUST be satisfied $ eb -r <filename>.eb From that point, the compiled software and associated module is available in your home and can be used as follows in launchers etc. -- see ULHPC launcher Examples #!/bin/bash -l # <--- DO NOT FORGET '-l' to facilitate further access to ULHPC modules #SBATCH -p <partition> #SBATCH -N 1 #SBATCH --ntasks-per-node <#sockets * s> #SBATCH --ntasks-per-socket <s> #SBATCH -c <thread> print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } # Safeguard for NOT running this launcher on access/login nodes module purge || print_error_and_exit \"No 'module' command\" resif-load-home-swset-prod # OR resif-load-home-swset-devel module load <softwarename> [ /<version> ] [ ... ]","title":"Using Easybuild to Build software in your Home"},{"location":"software/build/#using-easybuild-to-build-software-in-the-project","text":"Similarly to the above home builds, you should repeat the procedure this time using the helper script resif-load-project-swset-{prod | devel} . Don't forget Project Data Management instructions : to avoid quotas issues, you have to use sg # BETTER work in a screen or tmux session ;) $ si [ -gpu ] [ -c <threads> ] # get an interactive job $ module load tools/EasyBuild # /!\\ IMPORTANT: ensure EASYBUILD_PREFIX is correctly set to [basedir]/<cluster>/<environment>/<arch> # and that MODULEPATH is prefixed accordingly $ resif-load-project-swset- { prod | devel } $PROJECTHOME /<project> # /!\\ ADAPT environment and <project> accordingly $ sg <project> -c \"eb -S <softwarename>\" # confirm <filename>.eb == <softwarename>-<v>-<toolchain>.eb $ sg <project> -c \"eb -Dr <filename>.eb\" # check dependencies, normally most MUST be satisfied $ sg <project> -c \"eb -r <filename>.eb\" From that point, the compiled software and associated module is available in the project directoryand can be used by all project members as follows in launchers etc. -- see ULHPC launcher Examples #!/bin/bash -l # <--- DO NOT FORGET '-l' to facilitate further access to ULHPC modules #SBATCH -p <partition> #SBATCH -N 1 #SBATCH --ntasks-per-node <#sockets * s> #SBATCH --ntasks-per-socket <s> #SBATCH -c <thread> print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } demonstrate # Safeguard for NOT running this launcher on access/login nodes module purge || print_error_and_exit \"No 'module' command\" resif-load-project-swset-prod $PROJECTHOME /<project> # OR resif-load-project-swset-devel $PROJECTHOME/<project> module load <softwarename> [ /<version> ] [ ... ]","title":"Using Easybuild to Build software in the  project"},{"location":"software/build/#contribute-back-to-easybuild","text":"If you developped new easyconfig(s), you are expected to contribute them back to the Easybuilders community! Consider creating a Pull-Request. You can even do it by command-line assuming you have setup your Github integration . On iris or aion , you will likely need to install the possibly-insecure, alternate keyrings keyrings.alt packages -- see https://pypi.org/project/keyring/ # checking code style - see https://easybuild.readthedocs.io/en/latest/Code_style.html#code-style eb --check-contrib <ebfile> eb --new-pr <ebfile> You can can also consider using the script PR-create provided as part of the RESIF 3 project. Once the pull request is merged, you can inform the ULHPC team to consider adding the submitted Easyconfig as part of the ULHPC bundles and see it deployed within the next ULHPC software set release .","title":"Contribute back to Easybuild"},{"location":"software/swsets/","text":"ULHPC Software Sets \u00b6 We offer a YEARLY release of the ULHPC Software Set based on Easybuid release of toolchains -- see Component versions ( fixed per release ) in the foss and intel toolchains. However , count at least 6 months of validation/import after EB release before ULHPC release An overview of the currently available component versions is depicted below: Name Type 2019b ( legacy ) 2020a 2020b ( prod ) 2021a 2021b ( devel ) GCCCore compiler 8.3.0 9.3.0 10.2.0 10.3.0 11.2.0 foss toolchain 2019b 2020a 2020b 2021a 2021b intel toolchain 2019b 2020a 2020b 2021a 2021b binutils 2.32 2.34 2.35 2.36 2.37 Python 3.7.4 (and 2.7.16) 3.8.2 (and 2.7.18) 3.8.6 3.9.2 3.9.6 LLVM compiler 9.0.1 10.0.1 11.0.0 11.1.0 12.0.1 OpenMPI MPI 3.1.4 4.0.3 4.0.5 4.1.1 4.1.2 Once on a node, the current version of the ULHPC Software Set in production is stored in $RESIF_VERSION_PROD . You can use the variables $MODULEPATH_{LEGACY,PROD,DEVEL} to access or set the MODULEPATH command with the appropriate value. Yet we have define utility scripts to facilitate your quick reset of the module environment, i.e., resif-load-swset-{legacy,prod,devel} and resif-reset-swset For instance, if you want to use the legacy software set, proceed as follows in your launcher scripts: resif-load-swset-legacy # Eq. of export MODULEPATH=$MODULEPATH_LEGACY # [...] # Restore production settings resif-load-swset-prod # Eq. of export MODULEPATH=$MODULEPATH_PROD If on the contrary you want to test the (new) development software set, i.e., the devel version, stored in $RESIF_VERSION_DEVEL : resif-load-swset-devel # Eq. of export MODULEPATH=$MODULEPATH_DEVEL # [...] # Restore production settings resif-reset-swset # As resif-load-swset-prod (iris only) Skylake Optimized builds Skylake optimized build can be loaded on regular nodes using resif-load-swset-skylake # Eq. of export MODULEPATH=$MODULEPATH_PROD_SKYLAKE You MUST obviously be on a Skylake node ( sbatch -C skylake [...] ) to take benefit from it. Note that this action is not required on GPU nodes. GPU Optimized builds vs. CPU software set on GPU nodes On GPU nodes, be aware that the default MODULEPATH holds two directories: GPU Optimized builds ( i.e. typically against the {foss,intel}cuda toolchains) stored under /opt/apps/resif/<cluster>/<version>/gpu/modules/all CPU Optimized builds (ex: skylake on Iris )) stored under /opt/apps/resif/<cluster>/<version>/skylake/modules/all You may want to exclude CPU builds to ensure you take the most out of the GPU accelerators. In that case, you may want to run: # /!\\ ADAPT <version> accordingly module unuse /opt/apps/resif/ ${ ULHPC_CLUSTER } / ${ RESIF_VERSION_PROD } /skylake/modules/all List of available software Software List By category \u00b6 Biology CFD/Finite element modelling Chemistry Compilers Data processing Debugging Development Weather modelling Programming Languages Libraries Mathematics MPI Numerical libraries Performance measurements Physics System-level software Toolchains (software stacks) Utilities Visualisation","title":"ULHPC Software Sets"},{"location":"software/swsets/#ulhpc-software-sets","text":"We offer a YEARLY release of the ULHPC Software Set based on Easybuid release of toolchains -- see Component versions ( fixed per release ) in the foss and intel toolchains. However , count at least 6 months of validation/import after EB release before ULHPC release An overview of the currently available component versions is depicted below: Name Type 2019b ( legacy ) 2020a 2020b ( prod ) 2021a 2021b ( devel ) GCCCore compiler 8.3.0 9.3.0 10.2.0 10.3.0 11.2.0 foss toolchain 2019b 2020a 2020b 2021a 2021b intel toolchain 2019b 2020a 2020b 2021a 2021b binutils 2.32 2.34 2.35 2.36 2.37 Python 3.7.4 (and 2.7.16) 3.8.2 (and 2.7.18) 3.8.6 3.9.2 3.9.6 LLVM compiler 9.0.1 10.0.1 11.0.0 11.1.0 12.0.1 OpenMPI MPI 3.1.4 4.0.3 4.0.5 4.1.1 4.1.2 Once on a node, the current version of the ULHPC Software Set in production is stored in $RESIF_VERSION_PROD . You can use the variables $MODULEPATH_{LEGACY,PROD,DEVEL} to access or set the MODULEPATH command with the appropriate value. Yet we have define utility scripts to facilitate your quick reset of the module environment, i.e., resif-load-swset-{legacy,prod,devel} and resif-reset-swset For instance, if you want to use the legacy software set, proceed as follows in your launcher scripts: resif-load-swset-legacy # Eq. of export MODULEPATH=$MODULEPATH_LEGACY # [...] # Restore production settings resif-load-swset-prod # Eq. of export MODULEPATH=$MODULEPATH_PROD If on the contrary you want to test the (new) development software set, i.e., the devel version, stored in $RESIF_VERSION_DEVEL : resif-load-swset-devel # Eq. of export MODULEPATH=$MODULEPATH_DEVEL # [...] # Restore production settings resif-reset-swset # As resif-load-swset-prod (iris only) Skylake Optimized builds Skylake optimized build can be loaded on regular nodes using resif-load-swset-skylake # Eq. of export MODULEPATH=$MODULEPATH_PROD_SKYLAKE You MUST obviously be on a Skylake node ( sbatch -C skylake [...] ) to take benefit from it. Note that this action is not required on GPU nodes. GPU Optimized builds vs. CPU software set on GPU nodes On GPU nodes, be aware that the default MODULEPATH holds two directories: GPU Optimized builds ( i.e. typically against the {foss,intel}cuda toolchains) stored under /opt/apps/resif/<cluster>/<version>/gpu/modules/all CPU Optimized builds (ex: skylake on Iris )) stored under /opt/apps/resif/<cluster>/<version>/skylake/modules/all You may want to exclude CPU builds to ensure you take the most out of the GPU accelerators. In that case, you may want to run: # /!\\ ADAPT <version> accordingly module unuse /opt/apps/resif/ ${ ULHPC_CLUSTER } / ${ RESIF_VERSION_PROD } /skylake/modules/all List of available software","title":"ULHPC Software Sets"},{"location":"software/swsets/#software-list-by-category","text":"Biology CFD/Finite element modelling Chemistry Compilers Data processing Debugging Development Weather modelling Programming Languages Libraries Mathematics MPI Numerical libraries Performance measurements Physics System-level software Toolchains (software stacks) Utilities Visualisation","title":"Software List By category"},{"location":"software/cae/abaqus/","text":"The Abaqus Unified FEA product suite offers powerful and complete solutions for both routine and sophisticated engineering problems covering a vast spectrum of industrial applications. In the automotive industry engineering work groups are able to consider full vehicle loads, dynamic vibration, multibody systems, impact/crash, nonlinear static, thermal coupling, and acoustic-structural coupling using a common model data structure and integrated solver technology. Best-in-class companies are taking advantage of Abaqus Unified FEA to consolidate their processes and tools, reduce costs and inefficiencies, and gain a competitive advantage Available versions of Abaqus in ULHPC \u00b6 To check available versions of Abaqus at ULHPC, type module spider abaqus . It will list the available versions with the following format: cae/ABAQUS/<version> [ -hotfix-<hotfix> ] Don't forget to unset SLURM_GTIDS You MUST unset the SLURM environment variable SLURM_GTIDS for both interactive/GUI and batch jobs unset SLURM_GTIDS Failure to do so will cause Abaqus to get stuck due to the MPI that Abaqus ships witch is not supporting the SLURM scheduler. When using a general compute node for Abaqus 2018 or 2021, please run: abaqus cae -mesa to launch the GUI without support for hardware-accelerated graphics rendering. the option -mesa disables hardware-accelerated graphics rendering within Abaqus\u2019s GUI. For a Non-Graphical execution, use abaqus job=<my_job_name> input=<filename>.inp mp_mode=<mode> cpus=<cores> [gpus=<gpus>] scratch=$SCRATCH memory=\"<mem>gb\" Supported parallel mode \u00b6 Abaqus has two parallelization options which are mutually exclusive: MPI ( mp_mode=mpi ), which is generally preferred since this allows for scaling the job to multiple compute nodes. As for MPI jobs, use -N <nodes> --ntasks-per-node <cores> -c1 upon submission to use: abaqus mp_mode=mpi cpu=$SLURM_NTASKS [...] Shared memory / Threads ( mp_mode=threads ) for single node / multi-threaded executions. Typically use -N1 --ntasks-per-node 1 -c <threads> upon submission to use: abaqus mp_mode=threads cpus=${SLURM_CPUS_PER_TASK} [...] Shared memory for single node with GPU(s) / multi-threaded executions ( mp_mode=threads ). Typically use -N1 -G 1 --ntasks-per-node 1 -c <threads> upon submission on a GPU node to use: abaqus mp_mode=threads cpus=${SLURM_CPUS_PER_TASK} gpus=${SLURM_GPUS} [...] Abaqus example problems \u00b6 Abaqus contains a large number of example problems which can be used to become familiar with Abaqus on the system. These example problems are described in the Abaqus documentation and can be obtained using the abaqus fetch jobs=<name> command. For example, after loading the Abaqus module cae/ABAQUS , enter the following at the command line to extract the input file for test problem s4d: abaqus fetch job=s4d This will extract the input file s4d.inp See also Abaqus performance data . Interactive mode \u00b6 To open an Abaqus in the interactive mode, please follow the following steps: ( eventually ) connect to the ULHPC login node with the -X (or -Y ) option: Iris ssh -X iris-cluster # OR on Mac OS: ssh -Y iris-cluster Aion ssh -X aion-cluster # OR on Mac OS: ssh -Y aion-cluster Then you can reserve an interactive job , for instance with 8 MPI processes. Don't forget to use the --x11 option if you intend to use the GUI . $ si --x11 -c 8 # Abaqus mp_mode=threads test # OR $ si --x11 --ntask-per-node 8 # abaqus mp_mode=mpi test # Load the module ABAQUS and needed environment ( node ) $ module purge ( node ) $ module load cae/ABAQUS ( node ) $ unset SLURM_GTIDS # MANDATORY # /!\\ IMPORTANT: You MUST ADAPT the LM_LICENSE_FILE variable to point to YOUR licence server!!! ( node ) $ export LM_LICENSE_FILE = xyz # Check License server token available ( node ) $ abaqus licensing lmstat -a abaqus licensing lmstat -a lmutil - Copyright ( c ) 1989 -2019 Flexera. All Rights Reserved. Flexible License Manager status on Wed 4 /13/2022 22 :39 [ ... ] Non-graphical Abaqus \u00b6 Then the general format to run your Non-Graphical multithreaded interactive execution: Shared Memory (`-c <threads>`) Assuming a job submitted with {sbatch|srun|si...} -N1 -c <threads> : # /!\\ ADAPT $INPUTFILE accordingly abaqus job = \" ${ SLURM_JOB_NAME } \" verbose = 2 interactive \\ input = ${ INPUTFILE } \\ cpus = ${ SLURM_CPUS_PER_TASK } mp_mode = threads Distributed Memory (MPI) Assuming a job submitted with {sbatch|srun|si...} -N <N> --ntasks-per-node <npn> -c 1 : # /!\\ ADAPT $INPUTFILE accordingly abaqus job = \" ${ SLURM_JOB_NAME } \" verbose = 2 interactive \\ input = ${ INPUTFILE } \\ cpus = ${ SLURM_NTASKS } mp_mode = mpi GUI \u00b6 If you want to run the GUI, use: abaqus cae -mesa License information Assuming you have set the variable LM_LICENSE_FILE to point to YOUR licence server, you can check the available license and group you belongs to with: abaqus licensing lmstat -a If your server is hosted outside the ULHPC network, you will have to contact the HPC team to adapt the network firewalls to allow the connection towards your license server. Use the following options for simulation to stop and resume it: # /!\\ ADAPT <jobname> accordingly: abaqus job = <jobname> suspend abaqus job = <jobname> resume Batch mode \u00b6 Shared memory (mp_mode=threads) #!/bin/bash -l # <--- DO NOT FORGET '-l' #SBATCH -J <jobname> #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 4 # /!\\ ADAPT accordingly #SBATCH --time=0-03:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load cae/ABAQUS # export LM_LICENSE_FILE=[...] unset SLURM_GTIDS INPUTFILE = s4d.inp [ ! -f \" ${ INPUTFILE } \" ] && print_error_and_exit \"Unable to find input file ${ INPUTFILE } \" abaqus job = \" ${ SLURM_JOB_NAME } \" verbose = 2 interactive \\ input = ${ INPUTFILE } cpus = ${ SLURM_CPUS_PER_TASK } mp_mode = threads Distributed memory (mp_mode=mpi) #!/bin/bash -l # <--- DO NOT FORGET '-l' #SBATCH -J <jobname> #SBATCH -N 2 #SBATCH --ntasks-per-node=8 # /!\\ ADAPT accordingly #SBATCH -c 1 #SBATCH --time=0-03:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load cae/ABAQUS # export LM_LICENSE_FILE=[...] unset SLURM_GTIDS INPUTFILE = s4d.inp [ ! -f \" ${ INPUTFILE } \" ] && print_error_and_exit \"Unable to find input file ${ INPUTFILE } \" abaqus job = \" ${ SLURM_JOB_NAME } \" verbose = 2 interactive \\ input = ${ INPUTFILE } cpus = ${ SLURM_NTASKS } mp_mode = mpi Shared memory with GPU May not be supported depending on the software set #!/bin/bash -l # <--- DO NOT FORGET '-l' #SBATCH -J <jobname> #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 7 #SBATCH -G 1 #SBATCH --time=0-03:00:00 #SBATCH -p gpu print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load cae/ABAQUS # export LM_LICENSE_FILE=[...] unset SLURM_GTIDS INPUTFILE = s4d.inp [ ! -f \" ${ INPUTFILE } \" ] && print_error_and_exit \"Unable to find input file ${ INPUTFILE } \" abaqus job = \" ${ SLURM_JOB_NAME } \" verbose = 2 interactive \\ input = ${ INPUTFILE } cpus = ${ SLURM_CPUS_PER_TASK } gpus = ${ SLURM_GPUS } mp_mode = threads Additional information \u00b6 To know more about Abaqus documentation and tutorial, please refer Abaqus CAE Tutorial http://www.franc3d.com/wp-content/uploads/2012/05/FRANC3D_V7_ABAQUS_Tutorial.pdf https://sig.ias.edu/files/Abaqus%20tutorial.pdf https://sites.engineering.ucsb.edu/~tshugar/GET_STARTED.pdf?fbclid=IwAR2MQTzCTISqdPuM4D3PiDwXk9oVTBqZWXJUvMccVPYsd1kKPwPOZcnq078 Tip If you find some issues with the instructions above, please file a support ticket .","title":"Abaqus"},{"location":"software/cae/abaqus/#available-versions-of-abaqus-in-ulhpc","text":"To check available versions of Abaqus at ULHPC, type module spider abaqus . It will list the available versions with the following format: cae/ABAQUS/<version> [ -hotfix-<hotfix> ] Don't forget to unset SLURM_GTIDS You MUST unset the SLURM environment variable SLURM_GTIDS for both interactive/GUI and batch jobs unset SLURM_GTIDS Failure to do so will cause Abaqus to get stuck due to the MPI that Abaqus ships witch is not supporting the SLURM scheduler. When using a general compute node for Abaqus 2018 or 2021, please run: abaqus cae -mesa to launch the GUI without support for hardware-accelerated graphics rendering. the option -mesa disables hardware-accelerated graphics rendering within Abaqus\u2019s GUI. For a Non-Graphical execution, use abaqus job=<my_job_name> input=<filename>.inp mp_mode=<mode> cpus=<cores> [gpus=<gpus>] scratch=$SCRATCH memory=\"<mem>gb\"","title":"Available versions of Abaqus in ULHPC"},{"location":"software/cae/abaqus/#supported-parallel-mode","text":"Abaqus has two parallelization options which are mutually exclusive: MPI ( mp_mode=mpi ), which is generally preferred since this allows for scaling the job to multiple compute nodes. As for MPI jobs, use -N <nodes> --ntasks-per-node <cores> -c1 upon submission to use: abaqus mp_mode=mpi cpu=$SLURM_NTASKS [...] Shared memory / Threads ( mp_mode=threads ) for single node / multi-threaded executions. Typically use -N1 --ntasks-per-node 1 -c <threads> upon submission to use: abaqus mp_mode=threads cpus=${SLURM_CPUS_PER_TASK} [...] Shared memory for single node with GPU(s) / multi-threaded executions ( mp_mode=threads ). Typically use -N1 -G 1 --ntasks-per-node 1 -c <threads> upon submission on a GPU node to use: abaqus mp_mode=threads cpus=${SLURM_CPUS_PER_TASK} gpus=${SLURM_GPUS} [...]","title":"Supported parallel mode"},{"location":"software/cae/abaqus/#abaqus-example-problems","text":"Abaqus contains a large number of example problems which can be used to become familiar with Abaqus on the system. These example problems are described in the Abaqus documentation and can be obtained using the abaqus fetch jobs=<name> command. For example, after loading the Abaqus module cae/ABAQUS , enter the following at the command line to extract the input file for test problem s4d: abaqus fetch job=s4d This will extract the input file s4d.inp See also Abaqus performance data .","title":"Abaqus example problems"},{"location":"software/cae/abaqus/#interactive-mode","text":"To open an Abaqus in the interactive mode, please follow the following steps: ( eventually ) connect to the ULHPC login node with the -X (or -Y ) option: Iris ssh -X iris-cluster # OR on Mac OS: ssh -Y iris-cluster Aion ssh -X aion-cluster # OR on Mac OS: ssh -Y aion-cluster Then you can reserve an interactive job , for instance with 8 MPI processes. Don't forget to use the --x11 option if you intend to use the GUI . $ si --x11 -c 8 # Abaqus mp_mode=threads test # OR $ si --x11 --ntask-per-node 8 # abaqus mp_mode=mpi test # Load the module ABAQUS and needed environment ( node ) $ module purge ( node ) $ module load cae/ABAQUS ( node ) $ unset SLURM_GTIDS # MANDATORY # /!\\ IMPORTANT: You MUST ADAPT the LM_LICENSE_FILE variable to point to YOUR licence server!!! ( node ) $ export LM_LICENSE_FILE = xyz # Check License server token available ( node ) $ abaqus licensing lmstat -a abaqus licensing lmstat -a lmutil - Copyright ( c ) 1989 -2019 Flexera. All Rights Reserved. Flexible License Manager status on Wed 4 /13/2022 22 :39 [ ... ]","title":"Interactive mode"},{"location":"software/cae/abaqus/#non-graphical-abaqus","text":"Then the general format to run your Non-Graphical multithreaded interactive execution: Shared Memory (`-c <threads>`) Assuming a job submitted with {sbatch|srun|si...} -N1 -c <threads> : # /!\\ ADAPT $INPUTFILE accordingly abaqus job = \" ${ SLURM_JOB_NAME } \" verbose = 2 interactive \\ input = ${ INPUTFILE } \\ cpus = ${ SLURM_CPUS_PER_TASK } mp_mode = threads Distributed Memory (MPI) Assuming a job submitted with {sbatch|srun|si...} -N <N> --ntasks-per-node <npn> -c 1 : # /!\\ ADAPT $INPUTFILE accordingly abaqus job = \" ${ SLURM_JOB_NAME } \" verbose = 2 interactive \\ input = ${ INPUTFILE } \\ cpus = ${ SLURM_NTASKS } mp_mode = mpi","title":"Non-graphical Abaqus"},{"location":"software/cae/abaqus/#gui","text":"If you want to run the GUI, use: abaqus cae -mesa License information Assuming you have set the variable LM_LICENSE_FILE to point to YOUR licence server, you can check the available license and group you belongs to with: abaqus licensing lmstat -a If your server is hosted outside the ULHPC network, you will have to contact the HPC team to adapt the network firewalls to allow the connection towards your license server. Use the following options for simulation to stop and resume it: # /!\\ ADAPT <jobname> accordingly: abaqus job = <jobname> suspend abaqus job = <jobname> resume","title":"GUI"},{"location":"software/cae/abaqus/#batch-mode","text":"Shared memory (mp_mode=threads) #!/bin/bash -l # <--- DO NOT FORGET '-l' #SBATCH -J <jobname> #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 4 # /!\\ ADAPT accordingly #SBATCH --time=0-03:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load cae/ABAQUS # export LM_LICENSE_FILE=[...] unset SLURM_GTIDS INPUTFILE = s4d.inp [ ! -f \" ${ INPUTFILE } \" ] && print_error_and_exit \"Unable to find input file ${ INPUTFILE } \" abaqus job = \" ${ SLURM_JOB_NAME } \" verbose = 2 interactive \\ input = ${ INPUTFILE } cpus = ${ SLURM_CPUS_PER_TASK } mp_mode = threads Distributed memory (mp_mode=mpi) #!/bin/bash -l # <--- DO NOT FORGET '-l' #SBATCH -J <jobname> #SBATCH -N 2 #SBATCH --ntasks-per-node=8 # /!\\ ADAPT accordingly #SBATCH -c 1 #SBATCH --time=0-03:00:00 #SBATCH -p batch print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load cae/ABAQUS # export LM_LICENSE_FILE=[...] unset SLURM_GTIDS INPUTFILE = s4d.inp [ ! -f \" ${ INPUTFILE } \" ] && print_error_and_exit \"Unable to find input file ${ INPUTFILE } \" abaqus job = \" ${ SLURM_JOB_NAME } \" verbose = 2 interactive \\ input = ${ INPUTFILE } cpus = ${ SLURM_NTASKS } mp_mode = mpi Shared memory with GPU May not be supported depending on the software set #!/bin/bash -l # <--- DO NOT FORGET '-l' #SBATCH -J <jobname> #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -c 7 #SBATCH -G 1 #SBATCH --time=0-03:00:00 #SBATCH -p gpu print_error_and_exit () { echo \"***ERROR*** $* \" ; exit 1 ; } module purge || print_error_and_exit \"No 'module' command\" module load cae/ABAQUS # export LM_LICENSE_FILE=[...] unset SLURM_GTIDS INPUTFILE = s4d.inp [ ! -f \" ${ INPUTFILE } \" ] && print_error_and_exit \"Unable to find input file ${ INPUTFILE } \" abaqus job = \" ${ SLURM_JOB_NAME } \" verbose = 2 interactive \\ input = ${ INPUTFILE } cpus = ${ SLURM_CPUS_PER_TASK } gpus = ${ SLURM_GPUS } mp_mode = threads","title":"Batch mode"},{"location":"software/cae/abaqus/#additional-information","text":"To know more about Abaqus documentation and tutorial, please refer Abaqus CAE Tutorial http://www.franc3d.com/wp-content/uploads/2012/05/FRANC3D_V7_ABAQUS_Tutorial.pdf https://sig.ias.edu/files/Abaqus%20tutorial.pdf https://sites.engineering.ucsb.edu/~tshugar/GET_STARTED.pdf?fbclid=IwAR2MQTzCTISqdPuM4D3PiDwXk9oVTBqZWXJUvMccVPYsd1kKPwPOZcnq078 Tip If you find some issues with the instructions above, please file a support ticket .","title":"Additional information"},{"location":"software/cae/ansys/","text":"ANSYS offers a comprehensive software suite that spans the entire range of physics, providing access to virtually any field of engineering simulation that a design process requires. Organizations around the world trust Ansys to deliver the best value for their engineering simulation software investment. Available versions of ANSYS in ULHPC \u00b6 To check available versions of ANSYS at ULHPC type module spider ansys . The following versions of ANSYS are available in ULHPC: # Available versions tools/ANSYS/18.0 tools/ANSYS/19.0 tools/ANSYS/19.4 Interactive mode \u00b6 To open an ANSYS in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # Load the required version of ANSYS and needed environment $ module purge $ module load toolchain/intel/2019a $ module load tools/ANSYS/19.4 # To launch ANSYS workbench $ runwb2 Batch mode \u00b6 #!/bin/bash -l #SBATCH -J ANSYS-CFX #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --ntasks-per-socket=14 #SBATCH -c 1 #SBATCH --time=00:30:00 #SBATCH -p batch # Write out the stdout+stderr in a file #SBATCH -o output.txt # Mail me on job start & end #SBATCH --mail-user=myemailaddress@universityname.domain #SBATCH --mail-type=BEGIN,END # To get basic info. about the job echo \"== Starting run at $( date ) \" echo \"== Job ID: ${ SLURM_JOBID } \" echo \"== Node list: ${ SLURM_NODELIST } \" echo \"== Submit dir. : ${ SLURM_SUBMIT_DIR } \" # Load the required version of ANSYS and needed environment module purge module load toolchain/intel/2019a module load tools/ANSYS/19.4 # The Input file defFile = Benchmark.def MYHOSTLIST = $( srun hostname | sort | uniq -c | awk '{print $2 \"*\" $1}' | paste -sd, - ) echo $MYHOSTLIST cfx5solve -double -def $defFile -start-method \"Platform MPI Distributed Parallel\" -par-dist $MYHOSTLIST Additional information \u00b6 ANSYS provides the customer support , if you have a license key, you should be able to get all the support and needed documents. Tip If you find some issues with the instructions above, please file a support ticket .","title":"ANSYS"},{"location":"software/cae/ansys/#available-versions-of-ansys-in-ulhpc","text":"To check available versions of ANSYS at ULHPC type module spider ansys . The following versions of ANSYS are available in ULHPC: # Available versions tools/ANSYS/18.0 tools/ANSYS/19.0 tools/ANSYS/19.4","title":"Available versions of ANSYS in ULHPC"},{"location":"software/cae/ansys/#interactive-mode","text":"To open an ANSYS in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # Load the required version of ANSYS and needed environment $ module purge $ module load toolchain/intel/2019a $ module load tools/ANSYS/19.4 # To launch ANSYS workbench $ runwb2","title":"Interactive mode"},{"location":"software/cae/ansys/#batch-mode","text":"#!/bin/bash -l #SBATCH -J ANSYS-CFX #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --ntasks-per-socket=14 #SBATCH -c 1 #SBATCH --time=00:30:00 #SBATCH -p batch # Write out the stdout+stderr in a file #SBATCH -o output.txt # Mail me on job start & end #SBATCH --mail-user=myemailaddress@universityname.domain #SBATCH --mail-type=BEGIN,END # To get basic info. about the job echo \"== Starting run at $( date ) \" echo \"== Job ID: ${ SLURM_JOBID } \" echo \"== Node list: ${ SLURM_NODELIST } \" echo \"== Submit dir. : ${ SLURM_SUBMIT_DIR } \" # Load the required version of ANSYS and needed environment module purge module load toolchain/intel/2019a module load tools/ANSYS/19.4 # The Input file defFile = Benchmark.def MYHOSTLIST = $( srun hostname | sort | uniq -c | awk '{print $2 \"*\" $1}' | paste -sd, - ) echo $MYHOSTLIST cfx5solve -double -def $defFile -start-method \"Platform MPI Distributed Parallel\" -par-dist $MYHOSTLIST","title":"Batch mode"},{"location":"software/cae/ansys/#additional-information","text":"ANSYS provides the customer support , if you have a license key, you should be able to get all the support and needed documents. Tip If you find some issues with the instructions above, please file a support ticket .","title":"Additional information"},{"location":"software/cae/fds/","text":"Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. Available versions of FDS in ULHPC \u00b6 To check available versions of FDS at ULHPC type module spider abaqus . The following versions of FDS are available in ULHPC: # Available versions phys/FDS/6.7.1-intel-2018a phys/FDS/6.7.1-intel-2019a phys/FDS/6.7.3-intel-2019a Interactive mode \u00b6 To try FDS in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # Load the required version of FDS and needed environment $ module purge $ module load swenv/default-env/devel $ module load phys/FDS/6.7.3-intel-2019a # Example in fds $ fds example.fds Batch mode \u00b6 MPI only: \u00b6 #!/bin/bash -l #SBATCH -J FDS-mpi #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --ntasks-per-socket=14 #SBATCH -c 1 #SBATCH --time=00:30:00 #SBATCH -p batch # Write out the stdout+stderr in a file #SBATCH -o output.txt # Mail me on job start & end #SBATCH --mail-user=myemailaddress@universityname.domain #SBATCH --mail-type=BEGIN,END # To get basic info. about the job echo \"== Starting run at $( date ) \" echo \"== Job ID: ${ SLURM_JOBID } \" echo \"== Node list: ${ SLURM_NODELIST } \" echo \"== Submit dir. : ${ SLURM_SUBMIT_DIR } \" # Load the required version of FDS and needed environment module purge module load swenv/default-env/devel module load phys/FDS/6.7.3-intel-2019a srun fds example.fds MPI+OpenMP (hybrid): \u00b6 #!/bin/bash -l #SBATCH -J FDS-hybrid #SBATCH -N 2 #SBATCH --ntasks-per-node=56 #SBATCH --cpus-per-task=2 #SBATCH --time=00:30:00 #SBATCH -p batch # Write out the stdout+stderr in a file #SBATCH -o output.txt # Mail me on job start & end #SBATCH --mail-user=myemailaddress@universityname.domain #SBATCH --mail-type=BEGIN,END # To get basic info. about the job echo \"== Starting run at $( date ) \" echo \"== Job ID: ${ SLURM_JOBID } \" echo \"== Node list: ${ SLURM_NODELIST } \" echo \"== Submit dir. : ${ SLURM_SUBMIT_DIR } \" # Load the required version of FDS and needed environment module purge module load swenv/default-env/devel module load phys/FDS/6.7.3-intel-2019a srun fds_hyb example.fds Additional information \u00b6 To know more about FDS documentation and tutorial, please refer https://pages.nist.gov/fds-smv/manuals.html Tip If you find some issues with the instructions above, please file a support ticket .","title":"FDS"},{"location":"software/cae/fds/#available-versions-of-fds-in-ulhpc","text":"To check available versions of FDS at ULHPC type module spider abaqus . The following versions of FDS are available in ULHPC: # Available versions phys/FDS/6.7.1-intel-2018a phys/FDS/6.7.1-intel-2019a phys/FDS/6.7.3-intel-2019a","title":"Available versions of FDS in ULHPC"},{"location":"software/cae/fds/#interactive-mode","text":"To try FDS in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # Load the required version of FDS and needed environment $ module purge $ module load swenv/default-env/devel $ module load phys/FDS/6.7.3-intel-2019a # Example in fds $ fds example.fds","title":"Interactive mode"},{"location":"software/cae/fds/#batch-mode","text":"","title":"Batch mode"},{"location":"software/cae/fds/#mpi-only","text":"#!/bin/bash -l #SBATCH -J FDS-mpi #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --ntasks-per-socket=14 #SBATCH -c 1 #SBATCH --time=00:30:00 #SBATCH -p batch # Write out the stdout+stderr in a file #SBATCH -o output.txt # Mail me on job start & end #SBATCH --mail-user=myemailaddress@universityname.domain #SBATCH --mail-type=BEGIN,END # To get basic info. about the job echo \"== Starting run at $( date ) \" echo \"== Job ID: ${ SLURM_JOBID } \" echo \"== Node list: ${ SLURM_NODELIST } \" echo \"== Submit dir. : ${ SLURM_SUBMIT_DIR } \" # Load the required version of FDS and needed environment module purge module load swenv/default-env/devel module load phys/FDS/6.7.3-intel-2019a srun fds example.fds","title":"MPI only:"},{"location":"software/cae/fds/#mpiopenmp-hybrid","text":"#!/bin/bash -l #SBATCH -J FDS-hybrid #SBATCH -N 2 #SBATCH --ntasks-per-node=56 #SBATCH --cpus-per-task=2 #SBATCH --time=00:30:00 #SBATCH -p batch # Write out the stdout+stderr in a file #SBATCH -o output.txt # Mail me on job start & end #SBATCH --mail-user=myemailaddress@universityname.domain #SBATCH --mail-type=BEGIN,END # To get basic info. about the job echo \"== Starting run at $( date ) \" echo \"== Job ID: ${ SLURM_JOBID } \" echo \"== Node list: ${ SLURM_NODELIST } \" echo \"== Submit dir. : ${ SLURM_SUBMIT_DIR } \" # Load the required version of FDS and needed environment module purge module load swenv/default-env/devel module load phys/FDS/6.7.3-intel-2019a srun fds_hyb example.fds","title":"MPI+OpenMP (hybrid):"},{"location":"software/cae/fds/#additional-information","text":"To know more about FDS documentation and tutorial, please refer https://pages.nist.gov/fds-smv/manuals.html Tip If you find some issues with the instructions above, please file a support ticket .","title":"Additional information"},{"location":"software/cae/fenics/","text":"FEniCS is a popular open-source (LGPLv3) computing platform for solving partial differential equations (PDEs). FEniCS enables users to quickly translate scientific models into efficient finite element code. With the high-level Python and C++ interfaces to FEniCS, it is easy to get started, but FEniCS offers also powerful capabilities for more experienced programmers. FEniCS runs on a multitude of platforms ranging from laptops to high-performance clusters. How to access the FEniCS through Anaconda \u00b6 The following steps provides information about how to installed on your local path. # From your local computer $ ssh -X iris-cluster # OR ssh -Y iris-cluster on Mac # Reserve the node for interactive computation with grahics view (plots) $ si --x11 --ntasks-per-node 1 -c 4 # salloc -p interactive --qos debug -C batch --x11 --ntasks-per-node 1 -c 4 # Go to scratch directory $ cds /scratch/users/<login> $ Anaconda3-2020.07-Linux-x86_64.sh /scratch/users/<login> $ chmod +x Anaconda3-2020.07-Linux-x86_64.sh /scratch/users/<login> $ ./Anaconda3-2020.07-Linux-x86_64.sh Do you accept the license terms? [ yes | no ] yes Anaconda3 will now be installed into this location: /home/users/<login>/anaconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below # You can choose your path where you want to install it [ /home/users/<login>/anaconda3 ] >>> /scratch/users/<login>/Anaconda3 # To activate the anaconda /scratch/users/<login> $ source /scratch/users/<login>/Anaconda3/bin/activate # Install the fenics in anaconda environment /scratch/users/<login> $ conda create -n fenicsproject -c conda-forge fenics # Install matplotlib for the visualization /scratch/users/<login> $ conda install -c conda-forge matplotlib Once you have installed the anaconda, you can always activate it by calling the source activate path where anaconda has been installed. Working example \u00b6 Interactive mode \u00b6 # From your local computer $ ssh -X iris-cluster # or ssh -Y iris-cluster on Mac # Reserve the node for interactive computation with grahics view (plots) $ si --ntasks-per-node 1 -c 4 --x11 # salloc -p interactive --qos debug -C batch --x11 --ntasks-per-node 1 -c 4 # Activate anaconda $ source / ${ SCRATCH } /Anaconda3/bin/activate # activate the fenicsproject $ conda activate fenicsproject # execute the Poisson.py example (you can uncomment the plot lines in Poission.py example) $ python3 Poisson.py Batch script \u00b6 #!/bin/bash -l #SBATCH -J FEniCS #SBATCH -N 1 ###SBATCH -A <project name> ###SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --time=00:05:00 #SBATCH -p batch echo \"== Starting run at $( date ) \" echo \"== Job ID: ${ SLURM_JOBID } \" echo \"== Node list: ${ SLURM_NODELIST } \" echo \"== Submit dir. : ${ SLURM_SUBMIT_DIR } \" # activate the anaconda source source ${ SCRATCH } /Anaconda3/bin/activate # activate the fenicsproject from anaconda conda activate fenicsproject # execute the poission.py through python srun python3 Poisson.py Example (Poisson.py) \u00b6 # FEniCS tutorial demo program: Poisson equation with Dirichlet conditions. # Test problem is chosen to give an exact solution at all nodes of the mesh. # -Laplace(u) = f in the unit square # u = u_D on the boundary # u_D = 1 + x^2 + 2y^2 # f = -6 from __future__ import print_function from fenics import * import matplotlib.pyplot as plt # Create mesh and define function space mesh = UnitSquareMesh ( 8 , 8 ) V = FunctionSpace ( mesh, 'P' , 1 ) # Define boundary condition u_D = Expression ( '1 + x[0]*x[0] + 2*x[1]*x[1]' , degree = 2 ) def boundary ( x, on_boundary ) : return on_boundary bc = DirichletBC ( V, u_D, boundary ) # Define variational problem u = TrialFunction ( V ) v = TestFunction ( V ) f = Constant ( -6.0 ) a = dot ( grad ( u ) , grad ( v )) *dx L = f*v*dx # Compute solution u = Function ( V ) solve ( a == L, u, bc ) # Plot solution and mesh #plot(u) #plot(mesh) # Save solution to file in VTK format vtkfile = File ( 'poisson/solution.pvd' ) vtkfile << u # Compu te error in L2 norm error_L2 = errornorm ( u_D, u, 'L2' ) # Compute maximum error at vertices vertex_values_u_D = u_D.compute_vertex_values ( mesh ) vertex_values_u = u.compute_vertex_values ( mesh ) import numpy as np error_max = np.max ( np.abs ( vertex_values_u_D - vertex_values_u )) # Print errors print ( 'error_L2 =' , error_L2 ) print ( 'error_max =' , error_max ) # Hold plot #plt.show() Additional information \u00b6 FEniCS provides the technical documentation , and also it provides lots of communication channel for support and development. Tip If you find some issues with the instructions above, please file a support ticket .","title":"FEniCS"},{"location":"software/cae/fenics/#how-to-access-the-fenics-through-anaconda","text":"The following steps provides information about how to installed on your local path. # From your local computer $ ssh -X iris-cluster # OR ssh -Y iris-cluster on Mac # Reserve the node for interactive computation with grahics view (plots) $ si --x11 --ntasks-per-node 1 -c 4 # salloc -p interactive --qos debug -C batch --x11 --ntasks-per-node 1 -c 4 # Go to scratch directory $ cds /scratch/users/<login> $ Anaconda3-2020.07-Linux-x86_64.sh /scratch/users/<login> $ chmod +x Anaconda3-2020.07-Linux-x86_64.sh /scratch/users/<login> $ ./Anaconda3-2020.07-Linux-x86_64.sh Do you accept the license terms? [ yes | no ] yes Anaconda3 will now be installed into this location: /home/users/<login>/anaconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below # You can choose your path where you want to install it [ /home/users/<login>/anaconda3 ] >>> /scratch/users/<login>/Anaconda3 # To activate the anaconda /scratch/users/<login> $ source /scratch/users/<login>/Anaconda3/bin/activate # Install the fenics in anaconda environment /scratch/users/<login> $ conda create -n fenicsproject -c conda-forge fenics # Install matplotlib for the visualization /scratch/users/<login> $ conda install -c conda-forge matplotlib Once you have installed the anaconda, you can always activate it by calling the source activate path where anaconda has been installed.","title":"How to access the FEniCS through Anaconda"},{"location":"software/cae/fenics/#working-example","text":"","title":"Working example"},{"location":"software/cae/fenics/#interactive-mode","text":"# From your local computer $ ssh -X iris-cluster # or ssh -Y iris-cluster on Mac # Reserve the node for interactive computation with grahics view (plots) $ si --ntasks-per-node 1 -c 4 --x11 # salloc -p interactive --qos debug -C batch --x11 --ntasks-per-node 1 -c 4 # Activate anaconda $ source / ${ SCRATCH } /Anaconda3/bin/activate # activate the fenicsproject $ conda activate fenicsproject # execute the Poisson.py example (you can uncomment the plot lines in Poission.py example) $ python3 Poisson.py","title":"Interactive mode"},{"location":"software/cae/fenics/#batch-script","text":"#!/bin/bash -l #SBATCH -J FEniCS #SBATCH -N 1 ###SBATCH -A <project name> ###SBATCH --ntasks-per-node=1 #SBATCH -c 1 #SBATCH --time=00:05:00 #SBATCH -p batch echo \"== Starting run at $( date ) \" echo \"== Job ID: ${ SLURM_JOBID } \" echo \"== Node list: ${ SLURM_NODELIST } \" echo \"== Submit dir. : ${ SLURM_SUBMIT_DIR } \" # activate the anaconda source source ${ SCRATCH } /Anaconda3/bin/activate # activate the fenicsproject from anaconda conda activate fenicsproject # execute the poission.py through python srun python3 Poisson.py","title":"Batch script"},{"location":"software/cae/fenics/#example-poissonpy","text":"# FEniCS tutorial demo program: Poisson equation with Dirichlet conditions. # Test problem is chosen to give an exact solution at all nodes of the mesh. # -Laplace(u) = f in the unit square # u = u_D on the boundary # u_D = 1 + x^2 + 2y^2 # f = -6 from __future__ import print_function from fenics import * import matplotlib.pyplot as plt # Create mesh and define function space mesh = UnitSquareMesh ( 8 , 8 ) V = FunctionSpace ( mesh, 'P' , 1 ) # Define boundary condition u_D = Expression ( '1 + x[0]*x[0] + 2*x[1]*x[1]' , degree = 2 ) def boundary ( x, on_boundary ) : return on_boundary bc = DirichletBC ( V, u_D, boundary ) # Define variational problem u = TrialFunction ( V ) v = TestFunction ( V ) f = Constant ( -6.0 ) a = dot ( grad ( u ) , grad ( v )) *dx L = f*v*dx # Compute solution u = Function ( V ) solve ( a == L, u, bc ) # Plot solution and mesh #plot(u) #plot(mesh) # Save solution to file in VTK format vtkfile = File ( 'poisson/solution.pvd' ) vtkfile << u # Compu te error in L2 norm error_L2 = errornorm ( u_D, u, 'L2' ) # Compute maximum error at vertices vertex_values_u_D = u_D.compute_vertex_values ( mesh ) vertex_values_u = u.compute_vertex_values ( mesh ) import numpy as np error_max = np.max ( np.abs ( vertex_values_u_D - vertex_values_u )) # Print errors print ( 'error_L2 =' , error_L2 ) print ( 'error_max =' , error_max ) # Hold plot #plt.show()","title":"Example (Poisson.py)"},{"location":"software/cae/fenics/#additional-information","text":"FEniCS provides the technical documentation , and also it provides lots of communication channel for support and development. Tip If you find some issues with the instructions above, please file a support ticket .","title":"Additional information"},{"location":"software/cae/meshing-tools/","text":"Meshing Tools \u00b6 Gmsh \u00b6 Gmsh is an open source 3D finite element mesh generator with a built-in CAD engine and post-processor. Its design goal is to provide a fast, light and user-friendly meshing tool with parametric input and advanced visualization capabilities. Gmsh is built around four modules: geometry, mesh, solver and post-processing. The specification of any input to these modules is done either interactively using the graphical user interface, in ASCII text files using Gmsh's own scripting language (.geo files), or using the C++, C, Python or Julia Application Programming Interface (API). See this general presentation for a high-level overview of Gmsh and recent developments, the screencasts for a quick tour of Gmsh's graphical user interface, and the reference manual for a more thorough overview of Gmsh's capabilities, some frequently asked questions and the documentation of the C++, C, Python and Julia API. The source code repository contains many examples written using both the built-in script language and the API (see e.g. the tutorials and the and demos ). Available versions of Gmsh in ULHPC \u00b6 To check available versions of Gmsh at ULHPC type module spider gmsh . Below it shows list of available versions of Gmsh in ULHPC. cae/gmsh/4.3.0-intel-2018a cae/gmsh/4.4.0-intel-2019a To work with Gmsh interactively on ULHPC: \u00b6 # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # Load the module for Gmesh and neeed environment $ module purge $ module load swenv/default-env/v1.2-20191021-production $ module load cae/gmsh/4.4.0-intel-2019a $ gmsh example.geo Salome \u00b6 SALOME is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components. SALOME is a cross-platform solution. It is distributed under the terms of the GNU LGPL license. You can download both the source code and the executables from this site. To know more about salome documentation please refer https://www.salome-platform.org/user-section/salome-tutorials Available versions of SALOME in ULHPC \u00b6 To check available versions of SALOME at ULHPC type module spider salome . Below it shows list of available versions of SALOME in ULHPC. cae/Salome/8.5.0-intel-2018a cae/Salome/8.5.0-intel-2019a To work with SALOME interactively on ULHPC: \u00b6 # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ srun -p batch --time = 00 :30:00 --ntasks 1 -c 4 --x11 --pty bash -i # Load the module Salome and needed environment $ module purge $ module load swenv/default-env/v1.2-20191021-production $ module load cae/Salome/8.5.0-intel-2019a $ salome start Tip If you find some issues with the instructions above, please file a support ticket .","title":"Meshing-Tools"},{"location":"software/cae/meshing-tools/#meshing-tools","text":"","title":"Meshing Tools"},{"location":"software/cae/meshing-tools/#gmsh","text":"Gmsh is an open source 3D finite element mesh generator with a built-in CAD engine and post-processor. Its design goal is to provide a fast, light and user-friendly meshing tool with parametric input and advanced visualization capabilities. Gmsh is built around four modules: geometry, mesh, solver and post-processing. The specification of any input to these modules is done either interactively using the graphical user interface, in ASCII text files using Gmsh's own scripting language (.geo files), or using the C++, C, Python or Julia Application Programming Interface (API). See this general presentation for a high-level overview of Gmsh and recent developments, the screencasts for a quick tour of Gmsh's graphical user interface, and the reference manual for a more thorough overview of Gmsh's capabilities, some frequently asked questions and the documentation of the C++, C, Python and Julia API. The source code repository contains many examples written using both the built-in script language and the API (see e.g. the tutorials and the and demos ).","title":"Gmsh"},{"location":"software/cae/meshing-tools/#available-versions-of-gmsh-in-ulhpc","text":"To check available versions of Gmsh at ULHPC type module spider gmsh . Below it shows list of available versions of Gmsh in ULHPC. cae/gmsh/4.3.0-intel-2018a cae/gmsh/4.4.0-intel-2019a","title":"Available versions of Gmsh in ULHPC"},{"location":"software/cae/meshing-tools/#to-work-with-gmsh-interactively-on-ulhpc","text":"# From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # Load the module for Gmesh and neeed environment $ module purge $ module load swenv/default-env/v1.2-20191021-production $ module load cae/gmsh/4.4.0-intel-2019a $ gmsh example.geo","title":"To work with Gmsh interactively on ULHPC:"},{"location":"software/cae/meshing-tools/#salome","text":"SALOME is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components. SALOME is a cross-platform solution. It is distributed under the terms of the GNU LGPL license. You can download both the source code and the executables from this site. To know more about salome documentation please refer https://www.salome-platform.org/user-section/salome-tutorials","title":"Salome"},{"location":"software/cae/meshing-tools/#available-versions-of-salome-in-ulhpc","text":"To check available versions of SALOME at ULHPC type module spider salome . Below it shows list of available versions of SALOME in ULHPC. cae/Salome/8.5.0-intel-2018a cae/Salome/8.5.0-intel-2019a","title":"Available versions of SALOME in ULHPC"},{"location":"software/cae/meshing-tools/#to-work-with-salome-interactively-on-ulhpc","text":"# From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ srun -p batch --time = 00 :30:00 --ntasks 1 -c 4 --x11 --pty bash -i # Load the module Salome and needed environment $ module purge $ module load swenv/default-env/v1.2-20191021-production $ module load cae/Salome/8.5.0-intel-2019a $ salome start Tip If you find some issues with the instructions above, please file a support ticket .","title":"To work with SALOME interactively on ULHPC:"},{"location":"software/cae/openfoam/","text":"OpenFOAM is the free, open source CFD software developed primarily by OpenCFD Ltd since 2004. It has a large user base across most areas of engineering and science, from both commercial and academic organisations. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to acoustics, solid mechanics and electromagnetics Available versions of OpenFOAM in ULHPC \u00b6 To check available versions of OpenFOAM at ULHPC type module spider openfoam . The following versions of OpenFOAM are available in ULHPC: # Available versions cae/OpenFOAM/v1712-intel-2018a cae/OpenFOAM/v1812-foss-2019a Interactive mode \u00b6 To run an OpenFOAM in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p batch --time = 00 :30:00 --ntasks 1 -c 4 --x11 # Load the required version of OpenFOAM and Intel environment $ module load swenv/default-env/v1.1-20180716-production $ module load cae/OpenFOAM/v1712-intel-2018a # Load the OpenFOAM environment $ source $FOAM_BASH $ mkdir OpenFOAM $ cd OpenFOAM # Copy the example to your local folder (cavity example) $ cp -r /opt/apps/resif/data/production/v1.1-20180716/default/software/cae/OpenFOAM/v1712-intel-2018a/OpenFOAM-v1712/tutorials/incompressible/icoFoam/cavity/cavity . $ cd cavity # To initialize the mesh $ blockMesh # Run the simulation $ icoFoam # Visualize the solution $ paraFoam Batch mode \u00b6 Example of computational domain preparation (Dambreak example). $ mkdir OpenFOAM $ cd OpenFOAM $ cp -r /opt/apps/resif/data/production/v1.1-20180716/default/software/cae/OpenFOAM/v1712-intel-2018a/OpenFOAM-v1712/tutorials/multiphase/interFoam/laminar/damBreak/damBreak . $ blockMesh $ cd damBreak/system Open a decomposeParDict and set numberOfSubdomains 16 where n is number of MPI processor. And do blockMesh to prepare the computational domain (mesh) and finally do the decomposePar to repartition the mesh domain. #!/bin/bash -l #SBATCH -J OpenFOAM #SBATCH -N 1 #SBATCH --ntasks-per-node=28 #SBATCH --ntasks-per-socket=14 #SBATCH -c 1 #SBATCH --time=00:30:00 #SBATCH -p batch # Write out the stdout+stderr in a file #SBATCH -o output.txt # Mail me on job start & end #SBATCH --mail-user=myemailaddress@universityname.domain #SBATCH --mail-type=BEGIN,END # To get basic info. about the job echo \"== Starting run at $( date ) \" echo \"== Job ID: ${ SLURM_JOBID } \" echo \"== Node list: ${ SLURM_NODELIST } \" echo \"== Submit dir. : ${ SLURM_SUBMIT_DIR } \" # Load the required version of OpenFOAM and needed environment module purge module load swenv/default-env/v1.1-20180716-production module load cae/OpenFOAM/v1712-intel-2018a # Load the OpenFOAM environment source $FOAM_BASH srun interFoam -parallel Additional information \u00b6 To know more information about OpenFOAM tutorial/documentation, please refer https://www.openfoam.com/documentation/tutorial-guide/ Tip If you find some issues with the instructions above, please file a support ticket .","title":"OpenFOAM"},{"location":"software/cae/openfoam/#available-versions-of-openfoam-in-ulhpc","text":"To check available versions of OpenFOAM at ULHPC type module spider openfoam . The following versions of OpenFOAM are available in ULHPC: # Available versions cae/OpenFOAM/v1712-intel-2018a cae/OpenFOAM/v1812-foss-2019a","title":"Available versions of OpenFOAM in ULHPC"},{"location":"software/cae/openfoam/#interactive-mode","text":"To run an OpenFOAM in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p batch --time = 00 :30:00 --ntasks 1 -c 4 --x11 # Load the required version of OpenFOAM and Intel environment $ module load swenv/default-env/v1.1-20180716-production $ module load cae/OpenFOAM/v1712-intel-2018a # Load the OpenFOAM environment $ source $FOAM_BASH $ mkdir OpenFOAM $ cd OpenFOAM # Copy the example to your local folder (cavity example) $ cp -r /opt/apps/resif/data/production/v1.1-20180716/default/software/cae/OpenFOAM/v1712-intel-2018a/OpenFOAM-v1712/tutorials/incompressible/icoFoam/cavity/cavity . $ cd cavity # To initialize the mesh $ blockMesh # Run the simulation $ icoFoam # Visualize the solution $ paraFoam","title":"Interactive mode"},{"location":"software/cae/openfoam/#batch-mode","text":"Example of computational domain preparation (Dambreak example). $ mkdir OpenFOAM $ cd OpenFOAM $ cp -r /opt/apps/resif/data/production/v1.1-20180716/default/software/cae/OpenFOAM/v1712-intel-2018a/OpenFOAM-v1712/tutorials/multiphase/interFoam/laminar/damBreak/damBreak . $ blockMesh $ cd damBreak/system Open a decomposeParDict and set numberOfSubdomains 16 where n is number of MPI processor. And do blockMesh to prepare the computational domain (mesh) and finally do the decomposePar to repartition the mesh domain. #!/bin/bash -l #SBATCH -J OpenFOAM #SBATCH -N 1 #SBATCH --ntasks-per-node=28 #SBATCH --ntasks-per-socket=14 #SBATCH -c 1 #SBATCH --time=00:30:00 #SBATCH -p batch # Write out the stdout+stderr in a file #SBATCH -o output.txt # Mail me on job start & end #SBATCH --mail-user=myemailaddress@universityname.domain #SBATCH --mail-type=BEGIN,END # To get basic info. about the job echo \"== Starting run at $( date ) \" echo \"== Job ID: ${ SLURM_JOBID } \" echo \"== Node list: ${ SLURM_NODELIST } \" echo \"== Submit dir. : ${ SLURM_SUBMIT_DIR } \" # Load the required version of OpenFOAM and needed environment module purge module load swenv/default-env/v1.1-20180716-production module load cae/OpenFOAM/v1712-intel-2018a # Load the OpenFOAM environment source $FOAM_BASH srun interFoam -parallel","title":"Batch mode"},{"location":"software/cae/openfoam/#additional-information","text":"To know more information about OpenFOAM tutorial/documentation, please refer https://www.openfoam.com/documentation/tutorial-guide/ Tip If you find some issues with the instructions above, please file a support ticket .","title":"Additional information"},{"location":"software/computational-chemistry/electronics/abinit/","text":"ABINIT is a software suite to calculate the optical, mechanical, vibrational, and other observable properties of materials. Starting from the quantum equations of density functional theory, you can build up to advanced applications with perturbation theories based on DFT, and many-body Green's functions (GW and DMFT) . ABINIT can calculate molecules, nanostructures and solids with any chemical composition, and comes with several complete and robust tables of atomic potentials. On-line tutorials are available for the main features of the code, and several schools and workshops are organized each year. Available versions of ABINIT in ULHPC \u00b6 To check available versions of ABINIT at ULHPC type module spider abinit . The following list shows the available versions of ABINIT in ULHPC. chem/ABINIT/8.2.3-intel-2017a chem/ABINIT/8.6.3-intel-2018a-trio-nc chem/ABINIT/8.6.3-intel-2018a chem/ABINIT/8.10.2-intel-2019a Interactive mode \u00b6 To open an ABINIT in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module abinit and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/ABINIT/8.10.2-intel-2019a $ abinit < example.in Batch mode \u00b6 #!/bin/bash -l #SBATCH -J ABINIT #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module abinit and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load chem/ABINIT/8.10.2-intel-2019a srun -n ${ SLURM_NTASKS } abinit < input.files & > out Additional information \u00b6 To know more information about ABINIT tutorial and documentation, please refer to ABINIT tutorial . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"ABINIT"},{"location":"software/computational-chemistry/electronics/abinit/#available-versions-of-abinit-in-ulhpc","text":"To check available versions of ABINIT at ULHPC type module spider abinit . The following list shows the available versions of ABINIT in ULHPC. chem/ABINIT/8.2.3-intel-2017a chem/ABINIT/8.6.3-intel-2018a-trio-nc chem/ABINIT/8.6.3-intel-2018a chem/ABINIT/8.10.2-intel-2019a","title":"Available versions of ABINIT in ULHPC"},{"location":"software/computational-chemistry/electronics/abinit/#interactive-mode","text":"To open an ABINIT in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module abinit and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/ABINIT/8.10.2-intel-2019a $ abinit < example.in","title":"Interactive mode"},{"location":"software/computational-chemistry/electronics/abinit/#batch-mode","text":"#!/bin/bash -l #SBATCH -J ABINIT #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module abinit and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load chem/ABINIT/8.10.2-intel-2019a srun -n ${ SLURM_NTASKS } abinit < input.files & > out","title":"Batch mode"},{"location":"software/computational-chemistry/electronics/abinit/#additional-information","text":"To know more information about ABINIT tutorial and documentation, please refer to ABINIT tutorial . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/computational-chemistry/electronics/ase/","text":"The Atomic Simulation Environment (ASE) is a set of tools and Python modules for setting up, manipulating, running, visualizing and analyzing atomistic simulations. The code is freely available under the GNU LGPL license. ASE provides interfaces to different codes through Calculators which are used together with the central Atoms object and the many available algorithms in ASE. Available versions of ASE in ULHPC \u00b6 To check available versions of ASE at ULHPC type module spider ase . The following list shows the available versions of ASE in ULHPC. chem/ASE/3.13.0-intel-2017a-Python-2.7.13 chem/ASE/3.16.0-foss-2018a-Python-2.7.14 chem/ASE/3.16.0-intel-2018a-Python-2.7.14 chem/ASE/3.17.0-foss-2019a-Python-3.7.2 chem/ASE/3.17.0-intel-2019a-Python-2.7.15 chem/ASE/3.17.0-intel-2019a-Python-3.7.2 Interactive mode \u00b6 To open an ASE in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module ase and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/ASE/3.17.0-intel-2019a-Python-3.7.2 $ python3 example.py Batch mode \u00b6 #!/bin/bash -l #SBATCH -J ASE #SBATCH -N 1 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=1 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module ase and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/ASE/3.17.0-intel-2019a-Python-3.7.2 python3 example.py Additional information \u00b6 To know more information about ASE tutorial and documentation, please refer to ASE tutorials . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"ASE"},{"location":"software/computational-chemistry/electronics/ase/#available-versions-of-ase-in-ulhpc","text":"To check available versions of ASE at ULHPC type module spider ase . The following list shows the available versions of ASE in ULHPC. chem/ASE/3.13.0-intel-2017a-Python-2.7.13 chem/ASE/3.16.0-foss-2018a-Python-2.7.14 chem/ASE/3.16.0-intel-2018a-Python-2.7.14 chem/ASE/3.17.0-foss-2019a-Python-3.7.2 chem/ASE/3.17.0-intel-2019a-Python-2.7.15 chem/ASE/3.17.0-intel-2019a-Python-3.7.2","title":"Available versions of ASE in ULHPC"},{"location":"software/computational-chemistry/electronics/ase/#interactive-mode","text":"To open an ASE in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module ase and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/ASE/3.17.0-intel-2019a-Python-3.7.2 $ python3 example.py","title":"Interactive mode"},{"location":"software/computational-chemistry/electronics/ase/#batch-mode","text":"#!/bin/bash -l #SBATCH -J ASE #SBATCH -N 1 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=1 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module ase and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/ASE/3.17.0-intel-2019a-Python-3.7.2 python3 example.py","title":"Batch mode"},{"location":"software/computational-chemistry/electronics/ase/#additional-information","text":"To know more information about ASE tutorial and documentation, please refer to ASE tutorials . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/computational-chemistry/electronics/crystal/","text":"The CRYSTAL package performs ab initio calculations of the ground state energy, energy gradient, electronic wave function and properties of periodic systems. Hartree-Fock or Kohn-Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used. Systems periodic in 0 (molecules, 0D), 1 (polymers,1D), 2 (slabs, 2D), and 3 dimensions (crystals, 3D) are treated on an equal footing. In eachcase the fundamental approximation made is the expansion of the single particle wave functions(\u2019Crystalline Orbital\u2019, CO) as a linear combination of Bloch functions (BF) defined in terms of local functions (hereafter indicated as \u2019Atomic Orbitals\u2019, AOs). Available versions of CRYSTAL in ULHPC \u00b6 To check available versions of CRYSTAL at UL-HPC type module spider crystal . The following list shows the available versions of CRYSTAL in ULHPC. chem/CRYSTAL/17-intel-2017a-1.0.1 chem/CRYSTAL/17-intel-2018a-1.0.1 chem/CRYSTAL/17-intel-2019a-1.0.2 Interactive mode \u00b6 To test CRYTAL in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module crytal and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/CRYSTAL/17-intel-2019a-1.0.2 $ Pcrystal > & log.out Warning Please note your input file should be named just as INPUT . Pcrytal automatically will recognize the INPUT file from the folder where you are currently in. Batch mode \u00b6 #!/bin/bash -l #SBATCH -J CRYSTAL #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module crytal and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/CRYSTAL/17-intel-2019a-1.0.2 srun -n ${ SLURM_NTASKS } Pcrystal > & log.out Additional information \u00b6 To know more information about CRYSTAL tutorial and documentation, please refer to CRYSTAL solutions tutorials . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Crystal"},{"location":"software/computational-chemistry/electronics/crystal/#available-versions-of-crystal-in-ulhpc","text":"To check available versions of CRYSTAL at UL-HPC type module spider crystal . The following list shows the available versions of CRYSTAL in ULHPC. chem/CRYSTAL/17-intel-2017a-1.0.1 chem/CRYSTAL/17-intel-2018a-1.0.1 chem/CRYSTAL/17-intel-2019a-1.0.2","title":"Available versions of CRYSTAL in ULHPC"},{"location":"software/computational-chemistry/electronics/crystal/#interactive-mode","text":"To test CRYTAL in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module crytal and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/CRYSTAL/17-intel-2019a-1.0.2 $ Pcrystal > & log.out Warning Please note your input file should be named just as INPUT . Pcrytal automatically will recognize the INPUT file from the folder where you are currently in.","title":"Interactive mode"},{"location":"software/computational-chemistry/electronics/crystal/#batch-mode","text":"#!/bin/bash -l #SBATCH -J CRYSTAL #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module crytal and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/CRYSTAL/17-intel-2019a-1.0.2 srun -n ${ SLURM_NTASKS } Pcrystal > & log.out","title":"Batch mode"},{"location":"software/computational-chemistry/electronics/crystal/#additional-information","text":"To know more information about CRYSTAL tutorial and documentation, please refer to CRYSTAL solutions tutorials . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/computational-chemistry/electronics/meep/","text":"Meep is a free and open-source software package for electromagnetics simulation via the finite-difference time-domain (FDTD) method spanning a broad range of applications. Available versions of Meep in ULHPC \u00b6 To check available versions of Meep at ULHPC type module spider meep . The following list shows the available versions of Meep in ULHPC. phys/Meep/1.3-intel-2017a phys/Meep/1.4.3-intel-2018a phys/Meep/1.4.3-intel-2019a Interactive mode \u00b6 To try Meep in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module meep and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load toolchain/intel/2019a $ module load phys/Meep/1.4.3-intel-2019a $ meep example.ctl > result_output Batch mode \u00b6 #!/bin/bash -l #SBATCH -J Meep #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module meep and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load toolchain/intel/2019a module load phys/Meep/1.4.3-intel-2019a srun -n ${ SLURM_NTASKS } meep example.ctl > result_output Additional information \u00b6 To know more information about Meep tutorial and documentation, please refer to Meep tutorial . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"MEEP"},{"location":"software/computational-chemistry/electronics/meep/#available-versions-of-meep-in-ulhpc","text":"To check available versions of Meep at ULHPC type module spider meep . The following list shows the available versions of Meep in ULHPC. phys/Meep/1.3-intel-2017a phys/Meep/1.4.3-intel-2018a phys/Meep/1.4.3-intel-2019a","title":"Available versions of Meep in ULHPC"},{"location":"software/computational-chemistry/electronics/meep/#interactive-mode","text":"To try Meep in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module meep and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load toolchain/intel/2019a $ module load phys/Meep/1.4.3-intel-2019a $ meep example.ctl > result_output","title":"Interactive mode"},{"location":"software/computational-chemistry/electronics/meep/#batch-mode","text":"#!/bin/bash -l #SBATCH -J Meep #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module meep and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load toolchain/intel/2019a module load phys/Meep/1.4.3-intel-2019a srun -n ${ SLURM_NTASKS } meep example.ctl > result_output","title":"Batch mode"},{"location":"software/computational-chemistry/electronics/meep/#additional-information","text":"To know more information about Meep tutorial and documentation, please refer to Meep tutorial . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/computational-chemistry/electronics/quantum-espresso/","text":"Quantum ESPRESSO is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. Quantum ESPRESSO has evolved into a distribution of independent and inter-operable codes in the spirit of an open-source project. The Quantum ESPRESSO distribution consists of a \u201chistorical\u201d core set of components, and a set of plug-ins that perform more advanced tasks, plus a number of third-party packages designed to be inter-operable with the core components. Researchers active in the field of electronic-structure calculations are encouraged to participate in the project by contributing their own codes or by implementing their own ideas into existing codes. Available versions of Quantum ESPRESSO in ULHPC \u00b6 To check available versions of Quantum ESPRESSO at ULHPC type module spider quantum espresso . The following list shows the available versions of Quantum ESPRESSO in ULHPC. chem/QuantumESPRESSO/6.1-intel-2017a chem/QuantumESPRESSO/6.1-intel-2018a-maxter500 chem/QuantumESPRESSO/6.1-intel-2018a chem/QuantumESPRESSO/6.2.1-intel-2018a chem/QuantumESPRESSO/6.4.1-intel-2019a Interactive mode \u00b6 To open an Quantum ESPRESSO in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module quantumespresso and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/QuantumESPRESSO/6.4.1-intel-2019a $ pw.x -input example.in Batch mode \u00b6 #!/bin/bash -l #SBATCH -J QuantumESPRESSO #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module quantumespresso and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load chem/QuantumESPRESSO/6.4.1-intel-2019a srun -n ${ SLURM_NTASKS } pw.x -input example.inp Additional information \u00b6 To know more information about Quantum ESPRESSO tutorial and documentation, please refer to Quantum ESPRESSO user manual . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Quantum Espresso"},{"location":"software/computational-chemistry/electronics/quantum-espresso/#available-versions-of-quantum-espresso-in-ulhpc","text":"To check available versions of Quantum ESPRESSO at ULHPC type module spider quantum espresso . The following list shows the available versions of Quantum ESPRESSO in ULHPC. chem/QuantumESPRESSO/6.1-intel-2017a chem/QuantumESPRESSO/6.1-intel-2018a-maxter500 chem/QuantumESPRESSO/6.1-intel-2018a chem/QuantumESPRESSO/6.2.1-intel-2018a chem/QuantumESPRESSO/6.4.1-intel-2019a","title":"Available versions of Quantum ESPRESSO in ULHPC"},{"location":"software/computational-chemistry/electronics/quantum-espresso/#interactive-mode","text":"To open an Quantum ESPRESSO in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module quantumespresso and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/QuantumESPRESSO/6.4.1-intel-2019a $ pw.x -input example.in","title":"Interactive mode"},{"location":"software/computational-chemistry/electronics/quantum-espresso/#batch-mode","text":"#!/bin/bash -l #SBATCH -J QuantumESPRESSO #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module quantumespresso and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load chem/QuantumESPRESSO/6.4.1-intel-2019a srun -n ${ SLURM_NTASKS } pw.x -input example.inp","title":"Batch mode"},{"location":"software/computational-chemistry/electronics/quantum-espresso/#additional-information","text":"To know more information about Quantum ESPRESSO tutorial and documentation, please refer to Quantum ESPRESSO user manual . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/computational-chemistry/electronics/vasp/","text":"VASP is a package for performing ab initio quantum-mechanical molecular dynamics (MD) using pseudopotentials and a plane wave basis set. The approach implemented in VASP is based on a finite-temperature local-density approximation (with the free energy as variational quantity) and an exact evaluation of the instantaneous electronic ground state at each MD step using efficient matrix diagonalization schemes and an efficient Pulay mixing. Available versions of VASP in ULHPC \u00b6 To check available versions of VASP at ULHPC type module spider vasp . The following list shows the available versions of VASP in ULHPC. phys/VASP/5.4.4-intel-2017a phys/VASP/5.4.4-intel-2018a phys/VASP/5.4.4-intel-2019a Interactive mode \u00b6 To open VASP in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module vasp and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load phys/VASP/5.4.4-intel-2019a $ vasp_ [ std/gam/ncl ] Batch mode \u00b6 #!/bin/bash -l #SBATCH -J VASP #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module vasp and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load phys/VASP/5.4.4-intel-2019a srun -n ${ SLURM_NTASKS } vasp_ [ std/gam/ncl ] Additional information \u00b6 To know more information about VASP tutorial and documentation, please refer to VASP manual . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"VASP"},{"location":"software/computational-chemistry/electronics/vasp/#available-versions-of-vasp-in-ulhpc","text":"To check available versions of VASP at ULHPC type module spider vasp . The following list shows the available versions of VASP in ULHPC. phys/VASP/5.4.4-intel-2017a phys/VASP/5.4.4-intel-2018a phys/VASP/5.4.4-intel-2019a","title":"Available versions of VASP in ULHPC"},{"location":"software/computational-chemistry/electronics/vasp/#interactive-mode","text":"To open VASP in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module vasp and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load phys/VASP/5.4.4-intel-2019a $ vasp_ [ std/gam/ncl ]","title":"Interactive mode"},{"location":"software/computational-chemistry/electronics/vasp/#batch-mode","text":"#!/bin/bash -l #SBATCH -J VASP #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module vasp and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load phys/VASP/5.4.4-intel-2019a srun -n ${ SLURM_NTASKS } vasp_ [ std/gam/ncl ]","title":"Batch mode"},{"location":"software/computational-chemistry/electronics/vasp/#additional-information","text":"To know more information about VASP tutorial and documentation, please refer to VASP manual . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/computational-chemistry/molecular-dynamics/cp2k/","text":"CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems. CP2K provides a general framework for different modeling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFTB, LDA, GGA, MP2, RPA, semi-empirical methods (AM1, PM3, PM6, RM1, MNDO, \u2026), and classical force fields (AMBER, CHARMM, \u2026). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimization, and transition state optimization using NEB or dimer method. CP2K is written in Fortran 2008 and can be run efficiently in parallel using a combination of multi-threading, MPI, and CUDA. It is freely available under the GPL license. It is therefore easy to give the code a try, and to make modifications as needed. Available versions of CP2K in ULHPC \u00b6 To check available versions of CP2K at ULHPC type module spider cp2k . The following list shows the available versions of CP2K in ULHPC. chem/CP2K/6.1-foss-2019a chem/CP2K/6.1-intel-2018a Interactive mode \u00b6 To open CP2K in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module cp2k and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/CP2K/6.1-intel-2018a $ cp2k.popt -i example.inp Batch mode \u00b6 #!/bin/bash -l #SBATCH -J CP2K #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module cp2k and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load chem/CP2K/6.1-intel-2018a srun -n ${ SLURM_NTASKS } cp2k.popt -i example.inp > outputfile.out Additional information \u00b6 To know more information about CP2K tutorial and documentation, please refer to CP2K HOWTOs . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"CP2K"},{"location":"software/computational-chemistry/molecular-dynamics/cp2k/#available-versions-of-cp2k-in-ulhpc","text":"To check available versions of CP2K at ULHPC type module spider cp2k . The following list shows the available versions of CP2K in ULHPC. chem/CP2K/6.1-foss-2019a chem/CP2K/6.1-intel-2018a","title":"Available versions of CP2K in ULHPC"},{"location":"software/computational-chemistry/molecular-dynamics/cp2k/#interactive-mode","text":"To open CP2K in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module cp2k and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/CP2K/6.1-intel-2018a $ cp2k.popt -i example.inp","title":"Interactive mode"},{"location":"software/computational-chemistry/molecular-dynamics/cp2k/#batch-mode","text":"#!/bin/bash -l #SBATCH -J CP2K #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module cp2k and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load chem/CP2K/6.1-intel-2018a srun -n ${ SLURM_NTASKS } cp2k.popt -i example.inp > outputfile.out","title":"Batch mode"},{"location":"software/computational-chemistry/molecular-dynamics/cp2k/#additional-information","text":"To know more information about CP2K tutorial and documentation, please refer to CP2K HOWTOs . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/computational-chemistry/molecular-dynamics/gromacs/","text":"GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers. Available versions of GROMACS in ULHPC \u00b6 To check available versions of GROMACS at ULHPC type module spider gromacs . The following list shows the available versions of GROMACS in ULHPC. bio/GROMACS/2016.3-intel-2017a-hybrid bio/GROMACS/2016.5-intel-2018a-hybrid bio/GROMACS/2019.2-foss-2019a bio/GROMACS/2019.2-fosscuda-2019a bio/GROMACS/2019.2-intel-2019a bio/GROMACS/2019.2-intelcuda-2019a Interactive mode \u00b6 To try GROMACS in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module gromacs and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load bio/GROMACS/2019.2-intel-2019a $ gmx_mpi mdrun <all your GMX job specification options in here> Batch mode \u00b6 #!/bin/bash -l #SBATCH -J GROMAC #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module gromacs and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load bio/GROMACS/2019.2-intel-2019a srun -n ${ SLURM_NTASKS } gmx_mpi mdrun <all your GMX job specification options in here> Additional information \u00b6 To know more information about GROMACS tutorial and documentation, please refer to GROMACS documentation . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"GROMACS"},{"location":"software/computational-chemistry/molecular-dynamics/gromacs/#available-versions-of-gromacs-in-ulhpc","text":"To check available versions of GROMACS at ULHPC type module spider gromacs . The following list shows the available versions of GROMACS in ULHPC. bio/GROMACS/2016.3-intel-2017a-hybrid bio/GROMACS/2016.5-intel-2018a-hybrid bio/GROMACS/2019.2-foss-2019a bio/GROMACS/2019.2-fosscuda-2019a bio/GROMACS/2019.2-intel-2019a bio/GROMACS/2019.2-intelcuda-2019a","title":"Available versions of GROMACS in ULHPC"},{"location":"software/computational-chemistry/molecular-dynamics/gromacs/#interactive-mode","text":"To try GROMACS in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module gromacs and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load bio/GROMACS/2019.2-intel-2019a $ gmx_mpi mdrun <all your GMX job specification options in here>","title":"Interactive mode"},{"location":"software/computational-chemistry/molecular-dynamics/gromacs/#batch-mode","text":"#!/bin/bash -l #SBATCH -J GROMAC #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module gromacs and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load bio/GROMACS/2019.2-intel-2019a srun -n ${ SLURM_NTASKS } gmx_mpi mdrun <all your GMX job specification options in here>","title":"Batch mode"},{"location":"software/computational-chemistry/molecular-dynamics/gromacs/#additional-information","text":"To know more information about GROMACS tutorial and documentation, please refer to GROMACS documentation . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/","text":"libctl \u00b6 This is libctl , a Guile-based library for supporting flexible control files in scientific simulations. For more information about libctl, please refer to libctl Documentation . Available versions of libctl in ULHPC: \u00b6 chem/libctl/3.2.2-intel-2017a chem/libctl/4.0.0-intel-2018a chem/libctl/4.0.0-intel-2019a Libint \u00b6 Libint library is used to evaluate the traditional (electron repulsion) and certain novel two-body matrix elements (integrals) over Cartesian Gaussian functions used in modern atomic and molecular theory. Available versions of Libint in ULHPC: \u00b6 chem/Libint/1.1.6-GCC-8.2.0-2.31.1 chem/Libint/1.2.1-intel-2018a Libxc \u00b6 Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals that can be used by all the ETSF codes and also other codes. Available versions of Libxc in ULHPC: \u00b6 chem/libxc/3.0.0-intel-2017a chem/libxc/3.0.1-intel-2018a chem/libxc/4.2.3-intel-2019a chem/libxc/4.3.4-GCC-8.2.0-2.31.1 chem/libxc/4.3.4-iccifort-2019.1.144-GCC-8.2.0-2.31.1 PLUMED \u00b6 PLUMED works together with some of the most popular MD engines, such as ACEMD, Amber, DL_POLY, GROMACS, LAMMPS, NAMD, OpenMM, DFTB+, ABIN, CP2K, i-PI, PINY-MD, and Quantum Espresso. In addition, PLUMED can be used to augment the capabilities of analysis tools such as VMD, HTMD, OpenPathSampling, and as a standalone utility to analyze pre-calculated MD trajectories. PLUMED can be interfaced with the host code using a single well-documented API that enables the PLUMED functionalities to be imported. The API is accessible from multiple languages (C, C++, FORTRAN, and Python), and is thus compatible with the majority of the codes used in the community. The PLUMED license (L-GPL) also allows it to be interfaced with proprietary software. Available versions of PLUMED in ULHPC: \u00b6 chem/PLUMED/2.4.2-intel-2018a chem/PLUMED/2.5.1-foss-2019a chem/PLUMED/2.5.1-intel-2019a To know more information about PLUMMED tutorial and documentation, please refer to PLUMMED Cambridge tutorial . ESPResSo \u00b6 ESPResSo is a highly versatile software package for performing and analyzing scientific Molecular Dynamics many-particle simulations of coarse-grained atomistic or bead-spring models as they are used in soft matter research in physics, chemistry and molecular biology. It can be used to simulate systems such as polymers, liquid crystals, colloids, polyelectrolytes, ferrofluids and biological systems, for example DNA and lipid membranes. It also has a DPD and lattice Boltzmann solver for hydrodynamic interactions, and allows several particle couplings to the LB fluid. ESPResSo is free, open-source software published under the GNU General Public License (GPL3). It is parallelized and can be employed on desktop machines, convenience clusters as well as on supercomputers with hundreds of CPUs, and some modules have also support for GPU acceleration. The parallel code is controlled via the scripting language Python, which gives the software its great flexibility. Available versions of ESPResSo in ULHPC: \u00b6 phys/ESPResSo/3.3.1-intel-2017a-parallel phys/ESPResSo/3.3.1-intel-2018a-parallel phys/ESPResSo/4.0.2-intel-2019a phys/ESPResSo/4.0.2-intelcuda-2019a To know more information about ESPResSo tutorial and documentation, please refer to ESPRessSo documentation . UDUNITS \u00b6 The UDUNITS package supports units of physical quantities. Its C library provides for arithmetic manipulation of units and for conversion of numeric values between compatible units. The package contains an extensive unit database, which is in XML format and user-extendable. The package also contains a command-line utility for investigating units and converting values. Available version of UDUNITS in ULHPC: \u00b6 phys/UDUNITS/2.2.26-GCCcore-8.2.0 To know more information about UDUNITS tutorial and documentation, please refer to UDUNITS 2.2.26 Manual . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Helping Libraries"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#libctl","text":"This is libctl , a Guile-based library for supporting flexible control files in scientific simulations. For more information about libctl, please refer to libctl Documentation .","title":"libctl"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-versions-of-libctl-in-ulhpc","text":"chem/libctl/3.2.2-intel-2017a chem/libctl/4.0.0-intel-2018a chem/libctl/4.0.0-intel-2019a","title":"Available versions of libctl in ULHPC:"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#libint","text":"Libint library is used to evaluate the traditional (electron repulsion) and certain novel two-body matrix elements (integrals) over Cartesian Gaussian functions used in modern atomic and molecular theory.","title":"Libint"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-versions-of-libint-in-ulhpc","text":"chem/Libint/1.1.6-GCC-8.2.0-2.31.1 chem/Libint/1.2.1-intel-2018a","title":"Available versions of Libint in ULHPC:"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#libxc","text":"Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals that can be used by all the ETSF codes and also other codes.","title":"Libxc"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-versions-of-libxc-in-ulhpc","text":"chem/libxc/3.0.0-intel-2017a chem/libxc/3.0.1-intel-2018a chem/libxc/4.2.3-intel-2019a chem/libxc/4.3.4-GCC-8.2.0-2.31.1 chem/libxc/4.3.4-iccifort-2019.1.144-GCC-8.2.0-2.31.1","title":"Available versions of Libxc in ULHPC:"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#plumed","text":"PLUMED works together with some of the most popular MD engines, such as ACEMD, Amber, DL_POLY, GROMACS, LAMMPS, NAMD, OpenMM, DFTB+, ABIN, CP2K, i-PI, PINY-MD, and Quantum Espresso. In addition, PLUMED can be used to augment the capabilities of analysis tools such as VMD, HTMD, OpenPathSampling, and as a standalone utility to analyze pre-calculated MD trajectories. PLUMED can be interfaced with the host code using a single well-documented API that enables the PLUMED functionalities to be imported. The API is accessible from multiple languages (C, C++, FORTRAN, and Python), and is thus compatible with the majority of the codes used in the community. The PLUMED license (L-GPL) also allows it to be interfaced with proprietary software.","title":"PLUMED"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-versions-of-plumed-in-ulhpc","text":"chem/PLUMED/2.4.2-intel-2018a chem/PLUMED/2.5.1-foss-2019a chem/PLUMED/2.5.1-intel-2019a To know more information about PLUMMED tutorial and documentation, please refer to PLUMMED Cambridge tutorial .","title":"Available versions of PLUMED in ULHPC:"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#espresso","text":"ESPResSo is a highly versatile software package for performing and analyzing scientific Molecular Dynamics many-particle simulations of coarse-grained atomistic or bead-spring models as they are used in soft matter research in physics, chemistry and molecular biology. It can be used to simulate systems such as polymers, liquid crystals, colloids, polyelectrolytes, ferrofluids and biological systems, for example DNA and lipid membranes. It also has a DPD and lattice Boltzmann solver for hydrodynamic interactions, and allows several particle couplings to the LB fluid. ESPResSo is free, open-source software published under the GNU General Public License (GPL3). It is parallelized and can be employed on desktop machines, convenience clusters as well as on supercomputers with hundreds of CPUs, and some modules have also support for GPU acceleration. The parallel code is controlled via the scripting language Python, which gives the software its great flexibility.","title":"ESPResSo"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-versions-of-espresso-in-ulhpc","text":"phys/ESPResSo/3.3.1-intel-2017a-parallel phys/ESPResSo/3.3.1-intel-2018a-parallel phys/ESPResSo/4.0.2-intel-2019a phys/ESPResSo/4.0.2-intelcuda-2019a To know more information about ESPResSo tutorial and documentation, please refer to ESPRessSo documentation .","title":"Available versions of ESPResSo in ULHPC:"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#udunits","text":"The UDUNITS package supports units of physical quantities. Its C library provides for arithmetic manipulation of units and for conversion of numeric values between compatible units. The package contains an extensive unit database, which is in XML format and user-extendable. The package also contains a command-line utility for investigating units and converting values.","title":"UDUNITS"},{"location":"software/computational-chemistry/molecular-dynamics/helping-libraries/#available-version-of-udunits-in-ulhpc","text":"phys/UDUNITS/2.2.26-GCCcore-8.2.0 To know more information about UDUNITS tutorial and documentation, please refer to UDUNITS 2.2.26 Manual . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Available version of UDUNITS in ULHPC:"},{"location":"software/computational-chemistry/molecular-dynamics/namd/","text":"NAMD , recipient of a 2002 Gordon Bell Award and a 2012 Sidney Fernbach Award, is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 500,000 cores for the largest simulations. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR. NAMD is distributed free of charge with source code. You can build NAMD yourself or download binaries for a wide variety of platforms. Our tutorials show you how to use NAMD and VMD for biomolecular modeling. Available versions of NAMD in ULHPC \u00b6 To check available versions of NAMD at ULHPC type module spider namd . The following list shows the available versions of NAMD in ULHPC. chem/NAMD/2.12-intel-2017a-mpi chem/NAMD/2.12-intel-2018a-mpi chem/NAMD/2.13-foss-2019a-mpi Interactive mode \u00b6 To open NAMD in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module namd and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/NAMD/2.12-intel-2018a-mpi $ namd2 +setcpuaffinity +p4 config_file > output_file Batch mode \u00b6 #!/bin/bash -l #SBATCH -J NAMD #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module namd and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load chem/NAMD/2.12-intel-2018a-mpi srun -n ${ SLURM_NTASKS } namd2 +setcpuaffinity +p56 config_file.namd > output_file Additional information \u00b6 To know more information about NAMD tutorial and documentation, please refer to NAMD User's Guide . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"NAMD"},{"location":"software/computational-chemistry/molecular-dynamics/namd/#available-versions-of-namd-in-ulhpc","text":"To check available versions of NAMD at ULHPC type module spider namd . The following list shows the available versions of NAMD in ULHPC. chem/NAMD/2.12-intel-2017a-mpi chem/NAMD/2.12-intel-2018a-mpi chem/NAMD/2.13-foss-2019a-mpi","title":"Available versions of NAMD in ULHPC"},{"location":"software/computational-chemistry/molecular-dynamics/namd/#interactive-mode","text":"To open NAMD in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module namd and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/NAMD/2.12-intel-2018a-mpi $ namd2 +setcpuaffinity +p4 config_file > output_file","title":"Interactive mode"},{"location":"software/computational-chemistry/molecular-dynamics/namd/#batch-mode","text":"#!/bin/bash -l #SBATCH -J NAMD #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module namd and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load chem/NAMD/2.12-intel-2018a-mpi srun -n ${ SLURM_NTASKS } namd2 +setcpuaffinity +p56 config_file.namd > output_file","title":"Batch mode"},{"location":"software/computational-chemistry/molecular-dynamics/namd/#additional-information","text":"To know more information about NAMD tutorial and documentation, please refer to NAMD User's Guide . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/computational-chemistry/molecular-dynamics/nwchem/","text":"NWChem aims to provide its users with computational chemistry tools that are scalable both in their ability to efficiently treat large scientific problems, and in their use of available computing resources from high-performance parallel supercomputers to conventional workstation clusters. Available versions of NWChem in ULHPC \u00b6 To check available versions of NWChem at ULHPC type module spider nwchem . The following list shows the available versions of NWChem in ULHPC. chem/NWChem/6.6.revision27746-intel-2017a-2015-10-20-patches-20170814-Python-2.7.13 chem/NWChem/6.8.revision47-intel-2018a-Python-2.7.14 chem/NWChem/6.8.revision47-intel-2019a-Python-2.7.15 Interactive mode \u00b6 To try NWChem in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module nwchem and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/NWChem/6.8.revision47-intel-2019a-Python-2.7.15 $ nwchem example naming input file Please note example file should be named with extension like example.nw . Batch mode \u00b6 #!/bin/bash -l #SBATCH -J NWChem #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module nwchem and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load chem/NWChem/6.8.revision47-intel-2019a-Python-2.7.15 srun -n ${ SLURM_NTASKS } nwchem example Additional information \u00b6 To know more information about NWChem tutorial and documentation, please refer to NWChem User Documentation . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"NWCHEM"},{"location":"software/computational-chemistry/molecular-dynamics/nwchem/#available-versions-of-nwchem-in-ulhpc","text":"To check available versions of NWChem at ULHPC type module spider nwchem . The following list shows the available versions of NWChem in ULHPC. chem/NWChem/6.6.revision27746-intel-2017a-2015-10-20-patches-20170814-Python-2.7.13 chem/NWChem/6.8.revision47-intel-2018a-Python-2.7.14 chem/NWChem/6.8.revision47-intel-2019a-Python-2.7.15","title":"Available versions of NWChem in ULHPC"},{"location":"software/computational-chemistry/molecular-dynamics/nwchem/#interactive-mode","text":"To try NWChem in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module nwchem and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load chem/NWChem/6.8.revision47-intel-2019a-Python-2.7.15 $ nwchem example naming input file Please note example file should be named with extension like example.nw .","title":"Interactive mode"},{"location":"software/computational-chemistry/molecular-dynamics/nwchem/#batch-mode","text":"#!/bin/bash -l #SBATCH -J NWChem #SBATCH -N 2 #SBATCH -A <project name> #SBATCH -M --cluster iris #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module nwchem and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load chem/NWChem/6.8.revision47-intel-2019a-Python-2.7.15 srun -n ${ SLURM_NTASKS } nwchem example","title":"Batch mode"},{"location":"software/computational-chemistry/molecular-dynamics/nwchem/#additional-information","text":"To know more information about NWChem tutorial and documentation, please refer to NWChem User Documentation . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/maths/julia/","text":"Scientific computing has traditionally required the highest performance, yet domain experts have largely moved to slower dynamic languages for daily work. We believe there are many good reasons to prefer dynamic languages for these applications, and we do not expect their use to diminish. Fortunately, modern language design and compiler techniques make it possible to mostly eliminate the performance trade-off and provide a single environment productive enough for prototyping and efficient enough for deploying performance-intensive applications. The Julia programming language fills this role: it is a flexible dynamic language, appropriate for scientific and numerical computing, with performance comparable to traditional statically-typed languages. Available versions of Julia in ULHPC \u00b6 To check available versions of Julia at ULHPC type module spider julia . The following list shows the available versions of Julia in ULHPC. lang/Julia/1.1.1 lang/Julia/1.3.0 Interactive mode \u00b6 To open an MATLAB in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 # OR si [...] # Load the module Julia and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load lang/Julia/1.3.0 $ julia Batch mode \u00b6 An example for serial code \u00b6 #!/bin/bash -l #SBATCH -J Julia ###SBATCH -A <project name> #SBATCH --ntasks-per-node 1 #SBATCH -c 1 #SBATCH --time=00:15:00 #SBATCH -p batch # Load the module Julia and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load lang/Julia/1.3.0 julia { example } .jl An example for parallel code \u00b6 #!/bin/bash -l #SBATCH -J Julia ###SBATCH -A <project name> #SBATCH -N 1 #SBATCH --ntasks-per-node 28 #SBATCH --time=00:10:00 #SBATCH -p batch # Load the module Julia and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load lang/Julia/1.3.0 srun -n ${ SLURM_NTASKS } julia { example } .jl Example using Distributed # launch worker processes num_cores = parse ( Int , ENV [ \"SLURM_CPUS_PER_TASK\" ]) addprocs ( num_cores ) println ( \"Number of cores: \" , nprocs ()) println ( \"Number of workers: \" , nworkers ()) # each worker gets its id, process id and hostname for i in workers () id , pid , host = fetch ( @spawnat i ( myid (), getpid (), gethostname ())) println ( id , \" \" , pid , \" \" , host ) end # remove the workers for i in workers () rmprocs ( i ) end Additional information \u00b6 To know more information about Julia tutorial and documentation, please refer to Julia tutorial . Tip If you find some issues with the instructions above, please file a support ticket .","title":"Julia"},{"location":"software/maths/julia/#available-versions-of-julia-in-ulhpc","text":"To check available versions of Julia at ULHPC type module spider julia . The following list shows the available versions of Julia in ULHPC. lang/Julia/1.1.1 lang/Julia/1.3.0","title":"Available versions of Julia in ULHPC"},{"location":"software/maths/julia/#interactive-mode","text":"To open an MATLAB in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 # OR si [...] # Load the module Julia and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load lang/Julia/1.3.0 $ julia","title":"Interactive mode"},{"location":"software/maths/julia/#batch-mode","text":"","title":"Batch mode"},{"location":"software/maths/julia/#an-example-for-serial-code","text":"#!/bin/bash -l #SBATCH -J Julia ###SBATCH -A <project name> #SBATCH --ntasks-per-node 1 #SBATCH -c 1 #SBATCH --time=00:15:00 #SBATCH -p batch # Load the module Julia and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load lang/Julia/1.3.0 julia { example } .jl","title":"An example for serial code"},{"location":"software/maths/julia/#an-example-for-parallel-code","text":"#!/bin/bash -l #SBATCH -J Julia ###SBATCH -A <project name> #SBATCH -N 1 #SBATCH --ntasks-per-node 28 #SBATCH --time=00:10:00 #SBATCH -p batch # Load the module Julia and needed environment module purge module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) module load lang/Julia/1.3.0 srun -n ${ SLURM_NTASKS } julia { example } .jl Example using Distributed # launch worker processes num_cores = parse ( Int , ENV [ \"SLURM_CPUS_PER_TASK\" ]) addprocs ( num_cores ) println ( \"Number of cores: \" , nprocs ()) println ( \"Number of workers: \" , nworkers ()) # each worker gets its id, process id and hostname for i in workers () id , pid , host = fetch ( @spawnat i ( myid (), getpid (), gethostname ())) println ( id , \" \" , pid , \" \" , host ) end # remove the workers for i in workers () rmprocs ( i ) end","title":"An example for parallel code"},{"location":"software/maths/julia/#additional-information","text":"To know more information about Julia tutorial and documentation, please refer to Julia tutorial . Tip If you find some issues with the instructions above, please file a support ticket .","title":"Additional information"},{"location":"software/maths/mathematica/","text":"For three decades, MATHEMATICA has defined the state of the art in technical computing-and provided the principal computation environment for millions of innovators, educators, students, and others around the world. Widely admired for both its technical prowess and elegant ease of use, Mathematica provides a single integrated, continually expanding system that covers the breadth and depth of technical computing-and seamlessly available in the cloud through any web browser, as well as natively on all modern desktop systems. Available versions of MATHEMATICA in ULHPC \u00b6 To check available versions of MATHEMATICA at ULHPC type module spider mathematica . The following list shows the available versions of MATHEMATICA in ULHPC. math/Mathematica/11.0.0 math/Mathematica/11.3.0 math/Mathematica/12.0.0 Interactive mode \u00b6 To open an MATHEMATICA in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 # OR si [...] # Load the module MATHEMATICA and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load math/Mathematica/12.0.0 $ math Batch mode \u00b6 An example for serial case \u00b6 #!/bin/bash -l #SBATCH -J MATHEMATICA #SBATCH --ntasks-per-node 1 #SBATCH -c 1 #SBATCH --time=00:15:00 #SBATCH -p batch ### SBATCH -A <project_name> # Load the module MATHEMATICA and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load math/Mathematica/12.0.0 srun -n ${ SLURM_NTASKS } math -run < { mathematica-script-file } .m An example for parallel case \u00b6 #!/bin/bash -l #SBATCH -J MATHEMATICA #SBATCH -N 1 #SBATCH -c 28 #SBATCH --time=00:10:00 #SBATCH -p batch ### SBATCH -A <project_name> # Load the module MATHEMATICA and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load math/Mathematica/12.0.0 srun -n ${ SLURM_NTASKS } math -run < { mathematica-script-file } .m Exmaple # example for MATHEMATICA prallel (mathematica_script_file.m) //Limits Mathematica to requested resources Unprotect [ $ProcessorCount ] ; $ProcessorCount = 28 ; //Prints the machine name that each kernel is running on Print [ ParallelEvaluate [ $MachineName ]] ; //Prints all Prime numbers less than 3000 Print [ Parallelize [ Select [ Range [ 3000 ] ,PrimeQ [ 2 ^#-1 ] & ]]] ; Additional information \u00b6 To know more information about MATHEMATICA tutorial and documentation, please refer to MATHEMATICA tutorial . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"MATHEMATICA"},{"location":"software/maths/mathematica/#available-versions-of-mathematica-in-ulhpc","text":"To check available versions of MATHEMATICA at ULHPC type module spider mathematica . The following list shows the available versions of MATHEMATICA in ULHPC. math/Mathematica/11.0.0 math/Mathematica/11.3.0 math/Mathematica/12.0.0","title":"Available versions of MATHEMATICA in ULHPC"},{"location":"software/maths/mathematica/#interactive-mode","text":"To open an MATHEMATICA in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 # OR si [...] # Load the module MATHEMATICA and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load math/Mathematica/12.0.0 $ math","title":"Interactive mode"},{"location":"software/maths/mathematica/#batch-mode","text":"","title":"Batch mode"},{"location":"software/maths/mathematica/#an-example-for-serial-case","text":"#!/bin/bash -l #SBATCH -J MATHEMATICA #SBATCH --ntasks-per-node 1 #SBATCH -c 1 #SBATCH --time=00:15:00 #SBATCH -p batch ### SBATCH -A <project_name> # Load the module MATHEMATICA and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load math/Mathematica/12.0.0 srun -n ${ SLURM_NTASKS } math -run < { mathematica-script-file } .m","title":"An example for serial case"},{"location":"software/maths/mathematica/#an-example-for-parallel-case","text":"#!/bin/bash -l #SBATCH -J MATHEMATICA #SBATCH -N 1 #SBATCH -c 28 #SBATCH --time=00:10:00 #SBATCH -p batch ### SBATCH -A <project_name> # Load the module MATHEMATICA and needed environment $ module purge $ module load swenv/default-env/devel # Eventually (only relevant on 2019a software environment) $ module load math/Mathematica/12.0.0 srun -n ${ SLURM_NTASKS } math -run < { mathematica-script-file } .m Exmaple # example for MATHEMATICA prallel (mathematica_script_file.m) //Limits Mathematica to requested resources Unprotect [ $ProcessorCount ] ; $ProcessorCount = 28 ; //Prints the machine name that each kernel is running on Print [ ParallelEvaluate [ $MachineName ]] ; //Prints all Prime numbers less than 3000 Print [ Parallelize [ Select [ Range [ 3000 ] ,PrimeQ [ 2 ^#-1 ] & ]]] ;","title":"An example for parallel case"},{"location":"software/maths/mathematica/#additional-information","text":"To know more information about MATHEMATICA tutorial and documentation, please refer to MATHEMATICA tutorial . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/maths/matlab/","text":"MATLAB\u00ae combines a desktop environment tuned for iterative analysis and design processes with a programming language that expresses matrix and array mathematics directly. It includes the Live Editor for creating scripts that combine code, output, and formatted text in an executable notebook. Available versions of MATLAB in ULHPC \u00b6 To check available versions of MATLAB at ULHPC type module spider matlab . It will list the available versions: math/MATLAB/<version> Interactive mode \u00b6 To open MATLAB in the interactive mode, please follow the following steps: ( eventually ) connect to the ULHPC login node with the -X (or -Y ) option: Iris ssh -X iris-cluster # OR on Mac OS: ssh -Y iris-cluster Aion ssh -X aion-cluster # OR on Mac OS: ssh -Y aion-cluster Then you can reserve an interactive job , for instance with 4 cores. Don't forget to use the --x11 option if you intend to use the GUI . $ si --x11 -c4 # Load the module MATLAB and needed environment ( node ) $ module purge ( node ) $ module load math/MATLAB # Non-Graphical version (CLI) ( node ) $ matlab -nodisplay -nosplash < M A T L A B ( R ) > Copyright 1984 -2021 The MathWorks, Inc. R2021a ( 9 .10.0.1602886 ) 64 -bit ( glnxa64 ) February 17 , 2021 To get started, type doc. For product information, visit www.mathworks.com. >> version () ans = '9.10.0.1602886 (R2021a)' # List of installed add-ons >> matlab.addons.installedAddons ans = 96x4 table Name Version Enabled Identifier ___________________ ________ _______ __________ \"Mapping Toolbox\" \"5.1\" true \"MG\" \"Simulink Test\" \"3.4\" true \"SZ\" [ ... ] >> quit () # To run the GUI version, over X11 ( node ) $ matlab & Batch mode \u00b6 For non-interactive or long executions, MATLAB can be ran in passive or batch mode , reading all commands from an input file (with .m extension) you provide (Ex: inputfile.m ) and saving the results into an output file, for instance outputfile.out ). You have two ways to proceed: using redirection operators matlab -nodisplay -nosplash < inputfile.m > outputfile.out Use command-line options `-r` and `-logfile` # /!\\ IMPORTANT: notice the **missing** '.m' extension on -r !!! matlab -nodisplay -nosplash -r inputfile -logfile outputfile.out #!/bin/bash -l #SBATCH -J MATLAB ###SBATCH -A <project_name> #SBATCH --ntasks-per-node 1 #SBATCH -c 1 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module MATLAB module purge module load math/MATLAB # second form with CLI options '-r <input>' and '-logfile <output>.out' srun -c $SLURM_CPUS_PER_TASK matlab -nodisplay -r my_matlab_script -logfile output.out # example for if you need to have a input parameters for the computations # matlab_script_serial_file(x,y,z) srun matlab -nodisplay -r my_matlab_script ( 2 ,2,1 ) ' -logfile output.out # safeguard (!) afterwards rm -rf $HOME /.matlab rm -rf $HOME /java* In matlab, you can create a parallel pool of thread workers on the local computing node by using the parpool function. After you create the pool, parallel pool features, such as parfor or parfeval , run on the workers. With the ThreadPool object, you can interact with the parallel pool. Example # example for MATLAB ParFor (matlab_script_parallel_file.m) parpool ( 'local' , str2num ( getenv ( 'SLURM_CPUS_PER_TASK' ))) % set the default cores %as number of threads tic n = 50 ; A = 50 ; a = zeros ( 1 ,n ) ; parfor i = 1 :n a ( i ) = max ( abs ( eig ( rand ( A )))) ; end toc delete ( gcp ) ; % you have to delete the parallel region after the work is done exit ; Additional information \u00b6 To know more information about MATLAB tutorial and documentation, please refer to MATLAB tutorial . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"MATLAB"},{"location":"software/maths/matlab/#available-versions-of-matlab-in-ulhpc","text":"To check available versions of MATLAB at ULHPC type module spider matlab . It will list the available versions: math/MATLAB/<version>","title":"Available versions of MATLAB in ULHPC"},{"location":"software/maths/matlab/#interactive-mode","text":"To open MATLAB in the interactive mode, please follow the following steps: ( eventually ) connect to the ULHPC login node with the -X (or -Y ) option: Iris ssh -X iris-cluster # OR on Mac OS: ssh -Y iris-cluster Aion ssh -X aion-cluster # OR on Mac OS: ssh -Y aion-cluster Then you can reserve an interactive job , for instance with 4 cores. Don't forget to use the --x11 option if you intend to use the GUI . $ si --x11 -c4 # Load the module MATLAB and needed environment ( node ) $ module purge ( node ) $ module load math/MATLAB # Non-Graphical version (CLI) ( node ) $ matlab -nodisplay -nosplash < M A T L A B ( R ) > Copyright 1984 -2021 The MathWorks, Inc. R2021a ( 9 .10.0.1602886 ) 64 -bit ( glnxa64 ) February 17 , 2021 To get started, type doc. For product information, visit www.mathworks.com. >> version () ans = '9.10.0.1602886 (R2021a)' # List of installed add-ons >> matlab.addons.installedAddons ans = 96x4 table Name Version Enabled Identifier ___________________ ________ _______ __________ \"Mapping Toolbox\" \"5.1\" true \"MG\" \"Simulink Test\" \"3.4\" true \"SZ\" [ ... ] >> quit () # To run the GUI version, over X11 ( node ) $ matlab &","title":"Interactive mode"},{"location":"software/maths/matlab/#batch-mode","text":"For non-interactive or long executions, MATLAB can be ran in passive or batch mode , reading all commands from an input file (with .m extension) you provide (Ex: inputfile.m ) and saving the results into an output file, for instance outputfile.out ). You have two ways to proceed: using redirection operators matlab -nodisplay -nosplash < inputfile.m > outputfile.out Use command-line options `-r` and `-logfile` # /!\\ IMPORTANT: notice the **missing** '.m' extension on -r !!! matlab -nodisplay -nosplash -r inputfile -logfile outputfile.out #!/bin/bash -l #SBATCH -J MATLAB ###SBATCH -A <project_name> #SBATCH --ntasks-per-node 1 #SBATCH -c 1 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module MATLAB module purge module load math/MATLAB # second form with CLI options '-r <input>' and '-logfile <output>.out' srun -c $SLURM_CPUS_PER_TASK matlab -nodisplay -r my_matlab_script -logfile output.out # example for if you need to have a input parameters for the computations # matlab_script_serial_file(x,y,z) srun matlab -nodisplay -r my_matlab_script ( 2 ,2,1 ) ' -logfile output.out # safeguard (!) afterwards rm -rf $HOME /.matlab rm -rf $HOME /java* In matlab, you can create a parallel pool of thread workers on the local computing node by using the parpool function. After you create the pool, parallel pool features, such as parfor or parfeval , run on the workers. With the ThreadPool object, you can interact with the parallel pool. Example # example for MATLAB ParFor (matlab_script_parallel_file.m) parpool ( 'local' , str2num ( getenv ( 'SLURM_CPUS_PER_TASK' ))) % set the default cores %as number of threads tic n = 50 ; A = 50 ; a = zeros ( 1 ,n ) ; parfor i = 1 :n a ( i ) = max ( abs ( eig ( rand ( A )))) ; end toc delete ( gcp ) ; % you have to delete the parallel region after the work is done exit ;","title":"Batch mode"},{"location":"software/maths/matlab/#additional-information","text":"To know more information about MATLAB tutorial and documentation, please refer to MATLAB tutorial . Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"software/maths/stata/","text":"Stata is a commercial statistical package, which provides a complete solution for data analysis, data management, and graphics. The University of Luxembourg contributes to a campus-wide license -- see SIU / Service Now Knowledge Base ticket on Stata MP2 Available versions of Stata on ULHPC platforms \u00b6 To check available versions of Stata at ULHPC, type module spider stata . math/Stata/<version> Once loaded, the modules brings to you the following binaries: Binary Description stata Non-graphical standard Stata/IC. For better performance and support for larger databases, stata-se should be used. stata-se Non-graphical Stata/SE designed for large databases. Can be used to run tasks automatically with the batch flag -b and a Stata ' *.do file xstata Graphical standard Stata/IC. For better performance and support for larger databases, xstata-se should be used. xstata-se Graphical Stata/SE designed for large databases. Can be used interactively in a similar working environment to Windows and Mac versions. Interactive Mode \u00b6 To open a Stata session in interactive mode, please follow the following steps: ( eventually ) connect to the ULHPC login node with the -X (or -Y ) option: Iris ssh -X iris-cluster # OR on Mac OS: ssh -Y iris-cluster Aion ssh -X aion-cluster # OR on Mac OS: ssh -Y aion-cluster Then you can reserve an interactive job , for instance with 2 cores. Don't forget to use the --x11 option if you intend to use the GUI . $ si --x11 -c2 # You CANNOT use more than 2 cores # Load the module Stata and needed environment ( node ) $ module purge ( node ) $ module load math/Stata # Non-Graphical version (CLI) ( node ) $ stata ___ ____ ____ ____ ____ \u00ae /__ / ____/ / ____/ 17 .0 ___/ / /___/ / /___/ BE\u2014Basic Edition Statistics and Data Science Copyright 1985 -2021 StataCorp LLC StataCorp 4905 Lakeway Drive College Station, Texas 77845 USA 800 -STATA-PC https://www.stata.com 979 -696-4600 stata@stata.com Stata license: Unlimited-user network, expiring 31 Dec 2022 Serial number: <serial> Licensed to: University of Luxembourg Campus License - see KB0010885 ( Service Now ) . # To quit Stata . exit, clear # To run the GUI version, over X11 ( node ) $ stata & Location of your ado files \u00b6 Run the sysdir command to see the search path for ado files: . sysdir STATA: /opt/apps/resif/<cluster>/<version>/<arch>/software/Stata/<stataversion>/ BASE: /opt/apps/resif/<cluster>/<version>/<arch>/software/Stata/<stataversion>/ado/base/ SITE: /opt/apps/resif/<cluster>/<version>/<arch>/software/Stata/<stataversion>/software/Stata/ado/ PLUS: ~/ado/plus/ PERSONAL: ~/ado/personal/ You should thus store ado files in `$HOME/ado/personal. For more see this document. Batch mode \u00b6 To run Stata in batch mode, you need to create do-files which contain the series of commands you would like to run. With a do file ( filename.do ) in hand, you can run it from the shell in the command line with: stata -b do filename.do With the -b flag, outputs will be automatically saved to the outputfile filename.log . Serial Stata #!/bin/bash -l #SBATCH -J Stata ###SBATCH -A <project_name> #SBATCH --ntasks-per-node 1 #SBATCH -c 1 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module Stata module purge module load math/Stata srun stata -b do INPUTFILE.do Parallel Stata (Stata/MP) #!/bin/bash -l #SBATCH -J Stata ###SBATCH -A <project_name> #SBATCH --ntasks-per-node 1 #SBATCH -c 2 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module Stata module purge module load math/Stata # Use stata-mp to run across multiple cores srun -c $SLURM_CPUS_PER_TASK stata-mp -b do INPUTFILE.do Running Stata in Parallel \u00b6 Stata/MP \u00b6 You can use Stata/MP to advantage of the advanced multiprocessing capabilities of Stata/MP. Stata/MP provides the most extensive multicore support of any statistics and data management package. Note however that the current license limits the maximum number of cores (to 2 !) . Example of interactive usage: $ si --x11 -c2 # You CANNOT use more than 2 cores # Load the module Stata and needed environment ( node ) $ module purge ( node ) $ module load math/Stata # Non-Graphical version (CLI) ( node ) $ stata-mp ___ ____ ____ ____ ____ \u00ae /__ / ____/ / ____/ 17 .0 ___/ / /___/ / /___/ MP\u2014Parallel Edition Statistics and Data Science Copyright 1985 -2021 StataCorp LLC StataCorp 4905 Lakeway Drive College Station, Texas 77845 USA 800 -STATA-PC https://www.stata.com 979 -696-4600 stata@stata.com Stata license: Unlimited-user 2 -core network, expiring 31 Dec 2022 Serial number: <serial> Licensed to: University of Luxembourg Campus License - see KB0010885 ( Service Now ) . set processors 2 # or use env SLURM_CPU_PER_TASKS . [ ... ] . exit, clear Note that using the stata-mp executable, Stata will automatically use the requested number of cores from Slurm's --cpus-per-task option. This implicit parallelism does not require any changes to your code. User-packages parallel and gtools \u00b6 User-developed Stata packages can be installed from a login node using one of the Stata commands net install <package> These packages will be installed in your home directory by default. Among others, the parallel package implements parallel for loops. Also, the gtools provides faster alternatives to some Stata commands when working with big data. ( node ) $ stata # installation . net install parallel, from ( https://raw.github.com/gvegayon/parallel/stable/ ) replace checking parallel consistency and verifying not already installed... installing into /home/users/svarrette/ado/plus/... installation complete. # update index of the installed packages . mata mata mlib index .mlib libraries to be searched are now lmatabase ; lmatasvy ; lmatabma ; lmatapath ; lmatatab ; lmatanumlib ; lmatacollect ; lmatafc ; lmatapss ; lmat > asem ; lmatamixlog ; lmatamcmc ; lmatasp ; lmatameta ; lmataopt ; lmataado ; lmatagsem ; lmatami ; lmatapostest ; l > matalasso ; lmataerm ; lparallel # initial - ADAPT with SLURM_CPU_PER_TASKS . parallel initialize 4 , f # Or (better) find a way to use env SLURM_CPU_PER_TASKS N Child processes: 4 Stata dir: /mnt/irisgpfs/apps/resif/iris/2020b/broadwell/software/Stata/17/stata . sysuse auto ( 1978 automobile data ) . parallel, by ( foreign ) : egen maxp = max ( price ) Small workload/num groups. Temporarily setting number of child processes to 2 -------------------------------------------------------------------------------- Parallel Computing with Stata Child processes: 2 pll_id : bcrpvqtoi1 Running at : /mnt/irisgpfs/users/svarrette Randtype : datetime Waiting for the child processes to finish... child process 0002 has exited without error... child process 0001 has exited without error... -------------------------------------------------------------------------------- Enter -parallel printlog #- to checkout logfiles. -------------------------------------------------------------------------------- . tab maxp maxp | Freq. Percent Cum. ------------+----------------------------------- 12990 | 22 29 .73 29 .73 15906 | 52 70 .27 100 .00 ------------+----------------------------------- Total | 74 100 .00 . exit, clear","title":"Stata"},{"location":"software/maths/stata/#available-versions-of-stata-on-ulhpc-platforms","text":"To check available versions of Stata at ULHPC, type module spider stata . math/Stata/<version> Once loaded, the modules brings to you the following binaries: Binary Description stata Non-graphical standard Stata/IC. For better performance and support for larger databases, stata-se should be used. stata-se Non-graphical Stata/SE designed for large databases. Can be used to run tasks automatically with the batch flag -b and a Stata ' *.do file xstata Graphical standard Stata/IC. For better performance and support for larger databases, xstata-se should be used. xstata-se Graphical Stata/SE designed for large databases. Can be used interactively in a similar working environment to Windows and Mac versions.","title":"Available versions of Stata on ULHPC platforms"},{"location":"software/maths/stata/#interactive-mode","text":"To open a Stata session in interactive mode, please follow the following steps: ( eventually ) connect to the ULHPC login node with the -X (or -Y ) option: Iris ssh -X iris-cluster # OR on Mac OS: ssh -Y iris-cluster Aion ssh -X aion-cluster # OR on Mac OS: ssh -Y aion-cluster Then you can reserve an interactive job , for instance with 2 cores. Don't forget to use the --x11 option if you intend to use the GUI . $ si --x11 -c2 # You CANNOT use more than 2 cores # Load the module Stata and needed environment ( node ) $ module purge ( node ) $ module load math/Stata # Non-Graphical version (CLI) ( node ) $ stata ___ ____ ____ ____ ____ \u00ae /__ / ____/ / ____/ 17 .0 ___/ / /___/ / /___/ BE\u2014Basic Edition Statistics and Data Science Copyright 1985 -2021 StataCorp LLC StataCorp 4905 Lakeway Drive College Station, Texas 77845 USA 800 -STATA-PC https://www.stata.com 979 -696-4600 stata@stata.com Stata license: Unlimited-user network, expiring 31 Dec 2022 Serial number: <serial> Licensed to: University of Luxembourg Campus License - see KB0010885 ( Service Now ) . # To quit Stata . exit, clear # To run the GUI version, over X11 ( node ) $ stata &","title":"Interactive Mode"},{"location":"software/maths/stata/#location-of-your-ado-files","text":"Run the sysdir command to see the search path for ado files: . sysdir STATA: /opt/apps/resif/<cluster>/<version>/<arch>/software/Stata/<stataversion>/ BASE: /opt/apps/resif/<cluster>/<version>/<arch>/software/Stata/<stataversion>/ado/base/ SITE: /opt/apps/resif/<cluster>/<version>/<arch>/software/Stata/<stataversion>/software/Stata/ado/ PLUS: ~/ado/plus/ PERSONAL: ~/ado/personal/ You should thus store ado files in `$HOME/ado/personal. For more see this document.","title":"Location of your ado files"},{"location":"software/maths/stata/#batch-mode","text":"To run Stata in batch mode, you need to create do-files which contain the series of commands you would like to run. With a do file ( filename.do ) in hand, you can run it from the shell in the command line with: stata -b do filename.do With the -b flag, outputs will be automatically saved to the outputfile filename.log . Serial Stata #!/bin/bash -l #SBATCH -J Stata ###SBATCH -A <project_name> #SBATCH --ntasks-per-node 1 #SBATCH -c 1 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module Stata module purge module load math/Stata srun stata -b do INPUTFILE.do Parallel Stata (Stata/MP) #!/bin/bash -l #SBATCH -J Stata ###SBATCH -A <project_name> #SBATCH --ntasks-per-node 1 #SBATCH -c 2 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module Stata module purge module load math/Stata # Use stata-mp to run across multiple cores srun -c $SLURM_CPUS_PER_TASK stata-mp -b do INPUTFILE.do","title":"Batch mode"},{"location":"software/maths/stata/#running-stata-in-parallel","text":"","title":"Running Stata in Parallel"},{"location":"software/maths/stata/#statamp","text":"You can use Stata/MP to advantage of the advanced multiprocessing capabilities of Stata/MP. Stata/MP provides the most extensive multicore support of any statistics and data management package. Note however that the current license limits the maximum number of cores (to 2 !) . Example of interactive usage: $ si --x11 -c2 # You CANNOT use more than 2 cores # Load the module Stata and needed environment ( node ) $ module purge ( node ) $ module load math/Stata # Non-Graphical version (CLI) ( node ) $ stata-mp ___ ____ ____ ____ ____ \u00ae /__ / ____/ / ____/ 17 .0 ___/ / /___/ / /___/ MP\u2014Parallel Edition Statistics and Data Science Copyright 1985 -2021 StataCorp LLC StataCorp 4905 Lakeway Drive College Station, Texas 77845 USA 800 -STATA-PC https://www.stata.com 979 -696-4600 stata@stata.com Stata license: Unlimited-user 2 -core network, expiring 31 Dec 2022 Serial number: <serial> Licensed to: University of Luxembourg Campus License - see KB0010885 ( Service Now ) . set processors 2 # or use env SLURM_CPU_PER_TASKS . [ ... ] . exit, clear Note that using the stata-mp executable, Stata will automatically use the requested number of cores from Slurm's --cpus-per-task option. This implicit parallelism does not require any changes to your code.","title":"Stata/MP"},{"location":"software/maths/stata/#user-packages-parallel-and-gtools","text":"User-developed Stata packages can be installed from a login node using one of the Stata commands net install <package> These packages will be installed in your home directory by default. Among others, the parallel package implements parallel for loops. Also, the gtools provides faster alternatives to some Stata commands when working with big data. ( node ) $ stata # installation . net install parallel, from ( https://raw.github.com/gvegayon/parallel/stable/ ) replace checking parallel consistency and verifying not already installed... installing into /home/users/svarrette/ado/plus/... installation complete. # update index of the installed packages . mata mata mlib index .mlib libraries to be searched are now lmatabase ; lmatasvy ; lmatabma ; lmatapath ; lmatatab ; lmatanumlib ; lmatacollect ; lmatafc ; lmatapss ; lmat > asem ; lmatamixlog ; lmatamcmc ; lmatasp ; lmatameta ; lmataopt ; lmataado ; lmatagsem ; lmatami ; lmatapostest ; l > matalasso ; lmataerm ; lparallel # initial - ADAPT with SLURM_CPU_PER_TASKS . parallel initialize 4 , f # Or (better) find a way to use env SLURM_CPU_PER_TASKS N Child processes: 4 Stata dir: /mnt/irisgpfs/apps/resif/iris/2020b/broadwell/software/Stata/17/stata . sysuse auto ( 1978 automobile data ) . parallel, by ( foreign ) : egen maxp = max ( price ) Small workload/num groups. Temporarily setting number of child processes to 2 -------------------------------------------------------------------------------- Parallel Computing with Stata Child processes: 2 pll_id : bcrpvqtoi1 Running at : /mnt/irisgpfs/users/svarrette Randtype : datetime Waiting for the child processes to finish... child process 0002 has exited without error... child process 0001 has exited without error... -------------------------------------------------------------------------------- Enter -parallel printlog #- to checkout logfiles. -------------------------------------------------------------------------------- . tab maxp maxp | Freq. Percent Cum. ------------+----------------------------------- 12990 | 22 29 .73 29 .73 15906 | 52 70 .27 100 .00 ------------+----------------------------------- Total | 74 100 .00 . exit, clear","title":"User-packages parallel and gtools"},{"location":"software/optim/","text":"Optimizers \u00b6 Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element (with regard to some criterion) from some set of available alternatives. Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries Mathematical programming with Cplex and Gurobi \u00b6 Cplex is an optimization software for mathematical programming. The Cplex optimizer can solve: Mixed-Integer programming problems (MIP) Very large linear programming problems (LP) Non-convex quadratic programming problems (QP) Convex quadratically constrained problems (QCP) Gurobi is a powerful optimization software and an alternative to Cplex for solving. Gurobi has some additionnal features compared to Cplex. For example, it can perform Mixed-Integer Quadratic Programming (MIQP) and Mixed-Integer Quadratic Constrained Programming (MIQCP). Loading Cplex or Gurobi \u00b6 To use these optimization sfotwares, you need to load the corresponding Lmod module. For Cplex >$ module load maths/Cplex or for Gurobi >$ module load math/Gurobi Warning Modules are not allowed on the access servers. To test interactively Singularity, rememerber to ask for an interactive job first. salloc -p interactive # OR, use the helper script: si Using Cplex \u00b6 In order to test cplex and gurobi, we need an optimization instance. Hereafter, we are going to rely on instances from the miplib . For example, let us the following instance ex10.mps.gz described in details here for the interested readers. Multi-threaded optimization with Cplex \u00b6 In order to solve mathematical programs, cplex allows users to define a command line script that can be passed to the executable. On the Iris cluster, the following launcher can be used to perform multi-threaded MIP optimzation. A good practice is to request as many threads as available cores on the node. If you need more computing power, you have to consider a distributed version. #!/bin/bash -l #SBATCH -J Multi-threaded_cplex #SBATCH --ntasks=1 #SBATCH --cpus-per-task=28 #SBATCH --time=0-01:00:00 #SBATCH -p batch #SBATCH --qos=normal # Load cplex module load math/CPLEX # Some variable MPS_FILE = $1 RES_FILE = $2 CPLEX_COMMAND_SCRIPT = \"command_job ${ SLURM_JOBID } .lst\" # Create cplex command script cat << EOF > ${CPLEX_COMMAND_SCRIPT} set threads ${SLURM_CPUS_PER_TASK} read ${MPS_FILE} mipopt write \"${RES_FILE}.sol\" quit EOF chmod +x ${ CPLEX_COMMAND_SCRIPT } # Cplex will use the required number of thread cplex -f ${ CPLEX_COMMAND_SCRIPT } rm ${ CPLEX_COMMAND_SCRIPT } Using the script cplex_mtt.slurm , you can launch a batch job with the sbatch command as follows sbatch cplex_mtt.slurm ex10.mps.gz cplex_mtt . Distributed optimization with Cplex \u00b6 When you require more computing power (e.g. more cores), distributed computations is the way to go. The cplex optimization software embeds a feature that allows you to perform distributed MIP. Using the Message Passing Interface (MPI), cplex will distribute the exploration of the tree search to multiple workers. The below launcher is an example showing how to reserve ressources on multiple nodes through the Slurm scheduler. In this example, 31 tasks will be distributed over 2 nodes. #!/bin/bash -l #SBATCH -J Distrbuted\\_cplex #SBATCH --nodes=2 #SBATCH --ntasks-per-node=14 #SBATCH -c 2 # multithreading -- #threads (slurm cpu) per task #SBATCH --time=0-01:00:00 #SBATCH -p batch #SBATCH --qos=normal module load math/CPLEX # Some variables MPS_FILE = $1 RES_FILE = $2 CPLEX_COMMAND_SCRIPT = \"command_job ${ SLURM_JOBID } .lst\" # Create cplex command script cat << EOF > ${CPLEX_COMMAND_SCRIPT} set distmip config mpi set threads ${SLURM_CPUS_PER_TASK} read ${MPS_FILE} mipopt write \"${RES_FILE}.sol\" quit EOF chmod +x ${ CPLEX_COMMAND_SCRIPT } # Start Cplex with MPI # On first host, the master is running mpirun -np 1 cplex -f ${ CPLEX_COMMAND_SCRIPT } -mpi : -np $(( SLURM_NTASKS - 1 )) cplex -mpi rm ${ CPLEX_COMMAND_SCRIPT } Using the script cplex_dist.slurm , you can launch a batch job with the sbatch command as follows sbatch cplex_dist.slurm ex10.mps.gz cplex_dist . Gurobi \u00b6 Multi-threaded optimization with Gurobi \u00b6 The script below allows you to start multi-threaded MIP optimization with Gurobi. #!/bin/bash -l #SBATCH -J Multi-threaded_gurobi #SBATCH --ntasks-per-node=1 #SBATCH -c 28 # multithreading -- #threads (slurm cpu) per task #SBATCH --time=0-01:00:00 #SBATCH -p batch #SBATCH --qos=normal # Load Gurobi module load math/Gurobi # Some variable MPS_FILE = $1 RES_FILE = $2 # Gurobi will access use the required number of thread gurobi_cl Threads = ${ SLURM_CPUS_PER_TASK } ResultFile = \" ${ RES_FILE } .sol\" ${ MPS_FILE } Using the script gurobi_mtt.slurm , you can launch a batch job with the sbatch command as follows sbatch gurobi_mtt.slurm ex10.mps.gz gurobi_mtt . Distributed optimization with Gurobi \u00b6 #!/bin/bash -l #SBATCH -J Distrbuted_gurobi #SBATCH -N 3 # Number of nodes #SBATCH --ntasks-per-node=1 #SBATCH -c 5 # multithreading -- #threads (slurm cpu) per task #SBATCH --time=00:15:00 #SBATCH -p batch #SBATCH --qos normal #SBATCH -o %x-%j.log # Load personal modules mu # Load gurobi module load math/Gurobi export MASTER_PORT = 61000 export SLAVE_PORT = 61000 export MPS_FILE = $1 export RES_FILE = $2 export GUROBI_INNER_LAUNCHER = \"inner_job ${ SLURM_JOBID } .sh\" if [[ -f \"grb_rs.cnf\" ]] ; then sed -i \"s/^THREADLIMIT.* $ /THREADLIMIT= ${ SLURM_CPUS_PER_TASK } /g\" grb_rs.cnf else $GUROBI_REMOTE_BIN_PATH /grb_rs init echo \"THREADLIMIT= ${ SLURM_CPUS_PER_TASK } \" >> grb_rs.cnf fi cat << 'EOF' > ${GUROBI_INNER_LAUNCHER} #!/bin/bash MASTER_NODE=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1) ## Load configuration and environment if [[ ${SLURM_PROCID} -eq 0 ]]; then ## Start Gurobi master worker in background $GUROBI_REMOTE_BIN_PATH/grb_rs --worker --port ${MASTER_PORT} & wait elif [[ ${SLURM_PROCID} -eq 1 ]]; then sleep 5 grbcluster nodes --server ${MASTER_NODE}:${MASTER_PORT} gurobi_cl Threads=${SLURM_CPUS_PER_TASK} ResultFile=\"${RES_FILE}.sol\" Workerpool=${MASTER_NODE}:${MASTER_PORT} DistributedMIPJobs=$((SLURM_NNODES -1)) ${MPS_FILE} else sleep 2 ## Start Gurobi slave worker in background $GUROBI_REMOTE_BIN_PATH/grb_rs --worker --port ${MASTER_PORT} --join ${MASTER_NODE}:${MASTER_PORT} & wait fi EOF chmod +x ${ GUROBI_INNER_LAUNCHER } ## Launch Gurobi and wait for it to start srun ${ GUROBI_INNER_LAUNCHER } & while [[ ! -e \" ${ RES_FILE } .sol\" ]] ; do sleep 5 done rm ${ GUROBI_INNER_LAUNCHER } Using the script gurobi_dist.slurm , you can launch a batch job with the sbatch command as follows sbatch gurobi_dist.slurm ex10.mps.gz gurobi_dist .","title":"Optimizers"},{"location":"software/optim/#optimizers","text":"Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element (with regard to some criterion) from some set of available alternatives. Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries","title":"Optimizers"},{"location":"software/optim/#mathematical-programming-with-cplex-and-gurobi","text":"Cplex is an optimization software for mathematical programming. The Cplex optimizer can solve: Mixed-Integer programming problems (MIP) Very large linear programming problems (LP) Non-convex quadratic programming problems (QP) Convex quadratically constrained problems (QCP) Gurobi is a powerful optimization software and an alternative to Cplex for solving. Gurobi has some additionnal features compared to Cplex. For example, it can perform Mixed-Integer Quadratic Programming (MIQP) and Mixed-Integer Quadratic Constrained Programming (MIQCP).","title":"Mathematical programming with Cplex and Gurobi"},{"location":"software/optim/#loading-cplex-or-gurobi","text":"To use these optimization sfotwares, you need to load the corresponding Lmod module. For Cplex >$ module load maths/Cplex or for Gurobi >$ module load math/Gurobi Warning Modules are not allowed on the access servers. To test interactively Singularity, rememerber to ask for an interactive job first. salloc -p interactive # OR, use the helper script: si","title":"Loading Cplex or Gurobi"},{"location":"software/optim/#using-cplex","text":"In order to test cplex and gurobi, we need an optimization instance. Hereafter, we are going to rely on instances from the miplib . For example, let us the following instance ex10.mps.gz described in details here for the interested readers.","title":"Using Cplex"},{"location":"software/optim/#multi-threaded-optimization-with-cplex","text":"In order to solve mathematical programs, cplex allows users to define a command line script that can be passed to the executable. On the Iris cluster, the following launcher can be used to perform multi-threaded MIP optimzation. A good practice is to request as many threads as available cores on the node. If you need more computing power, you have to consider a distributed version. #!/bin/bash -l #SBATCH -J Multi-threaded_cplex #SBATCH --ntasks=1 #SBATCH --cpus-per-task=28 #SBATCH --time=0-01:00:00 #SBATCH -p batch #SBATCH --qos=normal # Load cplex module load math/CPLEX # Some variable MPS_FILE = $1 RES_FILE = $2 CPLEX_COMMAND_SCRIPT = \"command_job ${ SLURM_JOBID } .lst\" # Create cplex command script cat << EOF > ${CPLEX_COMMAND_SCRIPT} set threads ${SLURM_CPUS_PER_TASK} read ${MPS_FILE} mipopt write \"${RES_FILE}.sol\" quit EOF chmod +x ${ CPLEX_COMMAND_SCRIPT } # Cplex will use the required number of thread cplex -f ${ CPLEX_COMMAND_SCRIPT } rm ${ CPLEX_COMMAND_SCRIPT } Using the script cplex_mtt.slurm , you can launch a batch job with the sbatch command as follows sbatch cplex_mtt.slurm ex10.mps.gz cplex_mtt .","title":"Multi-threaded optimization with Cplex"},{"location":"software/optim/#distributed-optimization-with-cplex","text":"When you require more computing power (e.g. more cores), distributed computations is the way to go. The cplex optimization software embeds a feature that allows you to perform distributed MIP. Using the Message Passing Interface (MPI), cplex will distribute the exploration of the tree search to multiple workers. The below launcher is an example showing how to reserve ressources on multiple nodes through the Slurm scheduler. In this example, 31 tasks will be distributed over 2 nodes. #!/bin/bash -l #SBATCH -J Distrbuted\\_cplex #SBATCH --nodes=2 #SBATCH --ntasks-per-node=14 #SBATCH -c 2 # multithreading -- #threads (slurm cpu) per task #SBATCH --time=0-01:00:00 #SBATCH -p batch #SBATCH --qos=normal module load math/CPLEX # Some variables MPS_FILE = $1 RES_FILE = $2 CPLEX_COMMAND_SCRIPT = \"command_job ${ SLURM_JOBID } .lst\" # Create cplex command script cat << EOF > ${CPLEX_COMMAND_SCRIPT} set distmip config mpi set threads ${SLURM_CPUS_PER_TASK} read ${MPS_FILE} mipopt write \"${RES_FILE}.sol\" quit EOF chmod +x ${ CPLEX_COMMAND_SCRIPT } # Start Cplex with MPI # On first host, the master is running mpirun -np 1 cplex -f ${ CPLEX_COMMAND_SCRIPT } -mpi : -np $(( SLURM_NTASKS - 1 )) cplex -mpi rm ${ CPLEX_COMMAND_SCRIPT } Using the script cplex_dist.slurm , you can launch a batch job with the sbatch command as follows sbatch cplex_dist.slurm ex10.mps.gz cplex_dist .","title":"Distributed optimization with Cplex"},{"location":"software/optim/#gurobi","text":"","title":"Gurobi"},{"location":"software/optim/#multi-threaded-optimization-with-gurobi","text":"The script below allows you to start multi-threaded MIP optimization with Gurobi. #!/bin/bash -l #SBATCH -J Multi-threaded_gurobi #SBATCH --ntasks-per-node=1 #SBATCH -c 28 # multithreading -- #threads (slurm cpu) per task #SBATCH --time=0-01:00:00 #SBATCH -p batch #SBATCH --qos=normal # Load Gurobi module load math/Gurobi # Some variable MPS_FILE = $1 RES_FILE = $2 # Gurobi will access use the required number of thread gurobi_cl Threads = ${ SLURM_CPUS_PER_TASK } ResultFile = \" ${ RES_FILE } .sol\" ${ MPS_FILE } Using the script gurobi_mtt.slurm , you can launch a batch job with the sbatch command as follows sbatch gurobi_mtt.slurm ex10.mps.gz gurobi_mtt .","title":"Multi-threaded optimization with Gurobi"},{"location":"software/optim/#distributed-optimization-with-gurobi","text":"#!/bin/bash -l #SBATCH -J Distrbuted_gurobi #SBATCH -N 3 # Number of nodes #SBATCH --ntasks-per-node=1 #SBATCH -c 5 # multithreading -- #threads (slurm cpu) per task #SBATCH --time=00:15:00 #SBATCH -p batch #SBATCH --qos normal #SBATCH -o %x-%j.log # Load personal modules mu # Load gurobi module load math/Gurobi export MASTER_PORT = 61000 export SLAVE_PORT = 61000 export MPS_FILE = $1 export RES_FILE = $2 export GUROBI_INNER_LAUNCHER = \"inner_job ${ SLURM_JOBID } .sh\" if [[ -f \"grb_rs.cnf\" ]] ; then sed -i \"s/^THREADLIMIT.* $ /THREADLIMIT= ${ SLURM_CPUS_PER_TASK } /g\" grb_rs.cnf else $GUROBI_REMOTE_BIN_PATH /grb_rs init echo \"THREADLIMIT= ${ SLURM_CPUS_PER_TASK } \" >> grb_rs.cnf fi cat << 'EOF' > ${GUROBI_INNER_LAUNCHER} #!/bin/bash MASTER_NODE=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1) ## Load configuration and environment if [[ ${SLURM_PROCID} -eq 0 ]]; then ## Start Gurobi master worker in background $GUROBI_REMOTE_BIN_PATH/grb_rs --worker --port ${MASTER_PORT} & wait elif [[ ${SLURM_PROCID} -eq 1 ]]; then sleep 5 grbcluster nodes --server ${MASTER_NODE}:${MASTER_PORT} gurobi_cl Threads=${SLURM_CPUS_PER_TASK} ResultFile=\"${RES_FILE}.sol\" Workerpool=${MASTER_NODE}:${MASTER_PORT} DistributedMIPJobs=$((SLURM_NNODES -1)) ${MPS_FILE} else sleep 2 ## Start Gurobi slave worker in background $GUROBI_REMOTE_BIN_PATH/grb_rs --worker --port ${MASTER_PORT} --join ${MASTER_NODE}:${MASTER_PORT} & wait fi EOF chmod +x ${ GUROBI_INNER_LAUNCHER } ## Launch Gurobi and wait for it to start srun ${ GUROBI_INNER_LAUNCHER } & while [[ ! -e \" ${ RES_FILE } .sol\" ]] ; do sleep 5 done rm ${ GUROBI_INNER_LAUNCHER } Using the script gurobi_dist.slurm , you can launch a batch job with the sbatch command as follows sbatch gurobi_dist.slurm ex10.mps.gz gurobi_dist .","title":"Distributed optimization with Gurobi"},{"location":"software/swsets/","text":"Supported Software Sets \u00b6 You can find here the list of the supported software modules that you can use on the ULHPC facility. Full list of software (in alphabetical order) Software list by ULHPC software set release : 2019b (legacy) 2020b (prod) Software list by category *: Biology CFD/Finite element modelling Chemistry Compilers Data processing Debugging Development Weather modelling Programming Languages Libraries Mathematics MPI Numerical libraries Performance measurements Physics System-level software Toolchains (software stacks) Utilities Visualisation","title":"Overview"},{"location":"software/swsets/#supported-software-sets","text":"You can find here the list of the supported software modules that you can use on the ULHPC facility. Full list of software (in alphabetical order) Software list by ULHPC software set release : 2019b (legacy) 2020b (prod) Software list by category *: Biology CFD/Finite element modelling Chemistry Compilers Data processing Debugging Development Weather modelling Programming Languages Libraries Mathematics MPI Numerical libraries Performance measurements Physics System-level software Toolchains (software stacks) Utilities Visualisation","title":"Supported Software Sets"},{"location":"software/swsets/2019b/","text":"Alphabetical list of available ULHPC software belonging to the '2019b' software set. To load a software of this set, use: # Eventually: resif-load-swset-[...] module load <category>/<software> [ /<version> ] Software Architectures Clusters Category Description ABAQUS 2018 broadwell, skylake iris CFD/Finite element modelling Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ACTC 1.1 broadwell, skylake iris Libraries ACTC converts independent triangles into triangle strips or fans. ANSYS 19.4 broadwell, skylake iris Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. ANSYS 21.1 broadwell, skylake iris Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. ASE 3.19.0 broadwell, skylake iris Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE. ASE uses it automatically when installed. ATK 2.34.1 broadwell, skylake iris Visualisation ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. Advisor 2019_update5 broadwell, skylake iris Performance measurements Vectorization Optimization and Thread Prototyping - Vectorize & thread code or performance \u201cdies\u201d - Easy workflow + data + tips = faster code faster - Prioritize, Prototype & Predict performance gain Anaconda3 2020.02 broadwell, skylake iris Programming Languages Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. ArmForge 20.0.3 broadwell, skylake iris Utilities The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 broadwell, skylake iris Utilities Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Armadillo 9.900.1 broadwell, skylake iris Numerical libraries Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Arrow 0.16.0 broadwell, skylake iris Data processing Apache Arrow (incl. PyArrow Python bindings)), a cross-language development platform for in-memory data. Aspera-CLI 3.9.1 broadwell, skylake iris Utilities IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. Autoconf 2.69 broadwell, skylake, gpu iris Development Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.1 broadwell, skylake, gpu iris Development Automake: GNU Standards-compliant Makefile generator Autotools 20180311 broadwell, skylake, gpu iris Development This bundle collect the standard GNU build tools: Autoconf, Automake and libtool BEDTools 2.29.2 broadwell, skylake iris Biology BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.9.0 broadwell, skylake iris Biology Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 broadwell, skylake iris Biology Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 broadwell, skylake iris Biology BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Bazel 0.26.1 gpu iris Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. Bazel 0.29.1 gpu iris Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. BioPerl 1.7.2 broadwell, skylake iris Biology Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bison 3.3.2 broadwell, skylake, gpu iris Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Boost 1.71.0 broadwell, skylake iris Development Boost provides free peer-reviewed portable C++ source libraries. Bowtie2 2.3.5.1 broadwell, skylake iris Biology Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. CGAL 4.14.1 broadwell, skylake iris Numerical libraries The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. CMake 3.15.3 broadwell, skylake, gpu iris Development CMake, the cross-platform, open-source build system. CMake is a family of tools designed to build, test and package software. CPLEX 12.10 broadwell, skylake iris Mathematics IBM ILOG CPLEX Optimizer's mathematical programming technology enables analytical decision support for improving efficiency, reducing costs, and increasing profitability. CRYSTAL 17 broadwell, skylake iris Chemistry The CRYSTAL package performs ab initio calculations of the ground state energy, energy gradient, electronic wave function and properties of periodic systems. Hartree-Fock or Kohn- Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used. CUDA 10.1.243 gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. Clang 9.0.1 broadwell, skylake, gpu iris Compilers C, C++, Objective-C compiler, based on LLVM. Does not include C++ standard library -- use libstdc++ from GCC. CubeGUI 4.4.4 broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube graphical report explorer. CubeLib 4.4.4 broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube general purpose C++ library component and command-line tools. CubeWriter 4.4.3 broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube high-performance C writer library component. DB 18.1.32 broadwell, skylake iris Utilities Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DBus 1.13.12 broadwell, skylake iris Development D-Bus is a message bus system, a simple way for applications to talk to one another. In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. DMTCP 2.5.2 broadwell, skylake iris Utilities DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. Dakota 6.11.0 broadwell, skylake iris Mathematics The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" Doxygen 1.8.16 broadwell, skylake, gpu iris Development Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. ELPA 2019.11.001 broadwell iris Mathematics Eigenvalue SoLvers for Petaflop-Applications . EasyBuild 4.3.0 broadwell, skylake, gpu iris Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. EasyBuild 4.3.3 broadwell, skylake, gpu iris Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. Eigen 3.3.7 broadwell, skylake, gpu iris Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Elk 6.3.2 broadwell, skylake iris Physics An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.1 broadwell, skylake iris Physics Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. FFTW 3.3.8 broadwell, skylake, gpu iris Numerical libraries FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. FFmpeg 4.2.1 broadwell, skylake, gpu iris Visualisation A complete, cross-platform solution to record, convert and stream audio and video. FLTK 1.3.5 broadwell, skylake iris Visualisation FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FastQC 0.11.9 broadwell, skylake iris Biology FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. FriBidi 1.0.5 broadwell, skylake, gpu iris Programming Languages The Free Implementation of the Unicode Bidirectional Algorithm. GCC 8.3.0 broadwell, skylake, gpu iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 8.3.0 broadwell, skylake, gpu iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GDAL 3.0.2 broadwell, skylake, gpu iris Data processing GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. GDB 9.1 broadwell, skylake iris Debugging The GNU Project Debugger GEOS 3.8.0 broadwell, skylake, gpu iris Mathematics GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GLPK 4.65 broadwell, skylake iris Utilities The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. GLib 2.62.0 broadwell, skylake, gpu iris Visualisation GLib is one of the base libraries of the GTK+ project GMP 6.1.2 broadwell, skylake, gpu iris Mathematics GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. GObject-Introspection 1.63.1 broadwell, skylake iris Development GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. GPAW-setups 0.9.20000 broadwell, skylake iris Chemistry PAW setup for the GPAW Density Functional Theory package. Users can install setups manually using 'gpaw install-data' or use setups from this package. The versions of GPAW and GPAW-setups can be intermixed. GPAW 20.1.0 broadwell, skylake iris Chemistry GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). It uses real-space uniform grids and multigrid methods or atom-centered basis-functions. GROMACS 2019.4 broadwell, skylake iris Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GROMACS 2019.6 broadwell, skylake iris Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GROMACS 2020 broadwell, skylake iris Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GSL 2.6 broadwell, skylake, gpu iris Numerical libraries The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. GTK+ 3.24.13 broadwell, skylake iris Visualisation GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.38.2 broadwell, skylake iris Visualisation The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. Ghostscript 9.50 broadwell, skylake, gpu iris Utilities Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Go 1.14.1 broadwell, skylake iris Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Guile 1.8.8 broadwell, skylake iris Programming Languages Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Guile 2.2.4 broadwell, skylake iris Programming Languages Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Gurobi 9.0.0 broadwell, skylake iris Mathematics The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. HDF5 1.10.5 broadwell, skylake, gpu iris Data processing HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HTSlib 1.10.2 broadwell, skylake iris Biology A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix HarfBuzz 2.6.4 broadwell, skylake iris Visualisation HarfBuzz is an OpenType text shaping engine. Harminv 1.4.1 broadwell, skylake iris Mathematics Harminv is a free program (and accompanying library) to solve the problem of harmonic inversion - given a discrete-time, finite-length signal that consists of a sum of finitely-many sinusoids (possibly exponentially decaying) in a given bandwidth, it determines the frequencies, decay constants, amplitudes, and phases of those sinusoids. Horovod 0.19.1 broadwell, skylake, gpu iris Utilities Horovod is a distributed training framework for TensorFlow. ICU 64.2 broadwell, skylake, gpu iris Libraries ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. ImageMagick 7.0.9-5 broadwell, skylake, gpu iris Visualisation ImageMagick is a software suite to create, edit, compose, or convert bitmap images Inspector 2019_update5 broadwell, skylake iris Utilities Intel Inspector XE is an easy to use memory error checker and thread checker for serial and parallel applications JasPer 2.0.14 broadwell, skylake, gpu iris Visualisation The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. Java 1.8.0_241 broadwell, skylake, gpu iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Java 11.0.2 broadwell, skylake, gpu iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Java 13.0.2 broadwell, skylake, gpu iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Jellyfish 2.3.0 broadwell, skylake iris Biology Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA. JsonCpp 1.9.3 broadwell, skylake, gpu iris Libraries JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. Julia 1.4.1 broadwell, skylake iris Programming Languages Julia is a high-level, high-performance dynamic programming language for numerical computing Keras 2.3.1 gpu iris Mathematics Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. LAME 3.100 broadwell, skylake, gpu iris Data processing LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. LLVM 9.0.0 broadwell, skylake, gpu iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LLVM 9.0.1 broadwell, skylake, gpu iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LMDB 0.9.24 broadwell, skylake, gpu iris Libraries LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.0.10 broadwell, skylake, gpu iris Libraries tiff: Library and tools for reading and writing TIFF data files LittleCMS 2.9 broadwell, skylake, gpu iris Visualisation Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Lua 5.1.5 broadwell, skylake iris Programming Languages Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. M4 1.4.18 broadwell, skylake, gpu iris Development GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. MATLAB 2019b broadwell, skylake iris Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. MATLAB 2020a broadwell, skylake iris Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 broadwell, skylake iris Mathematics METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPFR 4.0.2 broadwell, skylake, gpu iris Mathematics The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. Mako 1.1.0 broadwell, skylake, gpu iris Development A super-fast templating language that borrows the best ideas from the existing templating languages Mathematica 12.0.0 broadwell, skylake iris Mathematics Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Maven 3.6.3 broadwell, skylake iris Development Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. Meep 1.4.3 broadwell, skylake iris Physics Meep (or MEEP) is a free finite-difference time-domain (FDTD) simulation software package developed at MIT to model electromagnetic systems. Mesa 19.1.7 broadwell, skylake, gpu iris Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Mesa 19.2.1 broadwell, skylake, gpu iris Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Meson 0.51.2 broadwell, skylake, gpu iris Utilities Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. Mesquite 2.3.0 broadwell, skylake iris Mathematics Mesh-Quality Improvement Library NAMD 2.13 broadwell, skylake iris Chemistry NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NASM 2.14.02 broadwell, skylake, gpu iris Programming Languages NASM: General-purpose x86 assembler NCCL 2.4.8 gpu iris Libraries The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NLopt 2.6.1 broadwell, skylake, gpu iris Numerical libraries NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. NSPR 4.21 broadwell, skylake iris Libraries Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.45 broadwell, skylake iris Libraries Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. Ninja 1.9.0 broadwell, skylake, gpu iris Utilities Ninja is a small build system with a focus on speed. OPARI2 2.0.5 broadwell, skylake iris Performance measurements OPARI2, the successor of Forschungszentrum Juelich's OPARI, is a source-to-source instrumentation tool for OpenMP and hybrid codes. It surrounds OpenMP directives and runtime library calls with calls to the POMP2 measurement interface. OTF2 2.2 broadwell, skylake iris Performance measurements The Open Trace Format 2 is a highly scalable, memory efficient event trace data format plus support library. It is the new standard trace format for Scalasca, Vampir, and TAU and is open for other tools. OpenBLAS 0.3.7 broadwell, skylake, gpu iris Numerical libraries OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. OpenCV 4.2.0 broadwell, skylake iris Visualisation OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenFOAM-Extend 4.1-20200408 broadwell, skylake iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenFOAM v1912 broadwell, skylake iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenMPI 3.1.4 broadwell, skylake, gpu iris MPI The Open MPI Project is an open source MPI-3 implementation. PAPI 6.0.0 broadwell, skylake iris Performance measurements PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PCRE2 10.33 broadwell, skylake iris Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.43 broadwell, skylake, gpu iris Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PDT 3.25 broadwell, skylake iris Performance measurements Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program knowledge accessible to developers of static and dynamic analysis tools. PDT implements a standard program representation, the program database (PDB), that can be accessed in a uniform way through a class library supporting common PDB operations. PGI 19.10 broadwell, skylake iris Compilers C, C++ and Fortran compilers from The Portland Group - PGI PLUMED 2.5.3 broadwell, skylake iris Chemistry PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. PROJ 6.2.1 broadwell, skylake, gpu iris Libraries Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates Pango 1.44.7 broadwell, skylake iris Visualisation Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParMETIS 4.0.3 broadwell, skylake iris Mathematics ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes. ParMGridGen 1.0 broadwell, skylake iris Mathematics ParMGridGen is an MPI-based parallel library that is based on the serial package MGridGen, that implements (serial) algorithms for obtaining a sequence of successive coarse grids that are well-suited for geometric multigrid methods. ParaView 5.6.2 broadwell, skylake iris Visualisation ParaView is a scientific parallel visualizer. Perl 5.30.0 broadwell, skylake, gpu iris Programming Languages Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Pillow 6.2.1 broadwell, skylake, gpu iris Visualisation Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyTorch 1.4.0 broadwell, skylake iris Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyTorch 1.7.1 broadwell, skylake iris Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyYAML 5.1.2 broadwell, skylake, gpu iris Libraries PyYAML is a YAML parser and emitter for the Python programming language. Python 2.7.16 broadwell, skylake, gpu iris Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Python 3.7.4 broadwell, skylake, gpu iris Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Qt5 5.13.1 broadwell, skylake iris Development Qt is a comprehensive cross-platform C++ application framework. QuantumESPRESSO 6.7 broadwell iris Chemistry Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). R 3.6.2 broadwell, skylake, gpu iris Programming Languages R is a free software environment for statistical computing and graphics. ReFrame 2.21 broadwell, skylake iris Development ReFrame is a framework for writing regression tests for HPC systems. Ruby 2.7.1 broadwell, skylake iris Programming Languages Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. Rust 1.37.0 broadwell, skylake iris Programming Languages Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. SAMtools 1.10 broadwell, skylake iris Biology SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. SCOTCH 6.0.9 broadwell, skylake iris Mathematics Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. SIONlib 1.7.6 broadwell, skylake iris Libraries SIONlib is a scalable I/O library for parallel access to task-local files. The library not only supports writing and reading binary data to or from several thousands of processors into a single or a small number of physical files, but also provides global open and close functions to access SIONlib files in parallel. This package provides a stripped-down installation of SIONlib for use with performance tools (e.g., Score-P), with renamed symbols to avoid conflicts when an application using SIONlib itself is linked against a tool requiring a different SIONlib version. SQLite 3.29.0 broadwell, skylake, gpu iris Development SQLite: SQL Database Engine in a C Library SWIG 4.0.1 broadwell, skylake, gpu iris Development SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Salmon 1.1.0 broadwell, skylake iris Biology Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data. Salome 8.5.0 broadwell, skylake iris CFD/Finite element modelling The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. ScaLAPACK 2.0.2 broadwell, skylake, gpu iris Numerical libraries The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. Scalasca 2.5 broadwell, skylake iris Performance measurements Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks -- in particular those concerning communication and synchronization -- and offers guidance in exploring their causes. SciPy-bundle 2019.10 broadwell, skylake, gpu iris Programming Languages Bundle of Python packages for scientific software Score-P 6.0 broadwell, skylake iris Performance measurements The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Singularity 3.6.0 broadwell, skylake iris Utilities SingularityCE is an open source container platform designed to be simple, fast, and secure. Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Spack 0.12.1 broadwell, skylake iris Development Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Spark 2.4.3 broadwell, skylake iris Development Spark is Hadoop MapReduce done in memory Sumo 1.3.1 broadwell, skylake iris Utilities Sumo is an open source, highly portable, microscopic and continuous traffic simulation package designed to handle large road networks. Szip 2.1.1 broadwell, skylake, gpu iris Utilities Szip compression software, providing lossless compression of scientific data Tcl 8.6.9 broadwell, skylake, gpu iris Programming Languages Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. TensorFlow 1.15.5 gpu iris Libraries An open-source software library for Machine Intelligence TensorFlow 2.1.0 gpu iris Libraries An open-source software library for Machine Intelligence Theano 1.0.4 gpu iris Mathematics Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Tk 8.6.9 broadwell, skylake, gpu iris Visualisation Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. Tkinter 3.7.4 broadwell, skylake iris Programming Languages Tkinter module, built with the Python buildsystem TopHat 2.1.2 broadwell, skylake iris Biology TopHat is a fast splice junction mapper for RNA-Seq reads. Trinity 2.10.0 broadwell, skylake iris Biology Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-Seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-Seq reads. UDUNITS 2.2.26 broadwell, skylake, gpu iris Physics UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. ULHPC-bio 2019b broadwell, skylake iris System-level software Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2019b broadwell, skylake iris System-level software Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management & processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2019b broadwell, skylake iris System-level software Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2019b gpu iris System-level software Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2019b broadwell, skylake iris System-level software Generic Module bundle for High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2019b broadwell, skylake iris System-level software Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2019b broadwell, skylake iris System-level software Misc tools, incl. - perf: Performance tools - tools: General purpose tools VASP 5.4.4 broadwell, skylake iris Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VTK 8.2.0 broadwell, skylake iris Visualisation The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VTune 2019_update8 broadwell, skylake iris Utilities Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. Valgrind 3.15.0 broadwell, skylake iris Debugging Valgrind: Debugging and profiling tools VirtualGL 2.6.2 broadwell, skylake iris Visualisation VirtualGL is an open source toolkit that gives any Linux or Unix remote display software the ability to run OpenGL applications with full hardware acceleration. Voro++ 0.4.6 broadwell, skylake iris Mathematics Voro++ is a software library for carrying out three-dimensional computations of the Voronoi tessellation. A distinguishing feature of the Voro++ library is that it carries out cell-based calculations, computing the Voronoi cell for each particle individually. It is particularly well-suited for applications that rely on cell-based statistics, where features of Voronoi cells (eg. volume, centroid, number of faces) can be used to analyze a system of particles. X11 20190717 broadwell, skylake, gpu iris Visualisation The X Window System (X11) is a windowing system for bitmap displays XML-LibXML 2.0201 broadwell, skylake iris Data processing Perl binding for libxml2 XZ 5.2.4 broadwell, skylake, gpu iris Utilities xz: XZ utilities Xerces-C++ 3.2.2 broadwell, skylake iris Libraries Xerces-C++ is a validating XML parser written in a portable subset of C++. Xerces-C++ makes it easy to give your application the ability to read and write XML data. A shared library is provided for parsing, generating, manipulating, and validating XML documents using the DOM, SAX, and SAX2 APIs. Yasm 1.3.0 broadwell, skylake, gpu iris Programming Languages Yasm: Complete rewrite of the NASM assembler with BSD license Zip 3.0 broadwell, skylake, gpu iris Utilities Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality ant 1.10.6 broadwell, skylake iris Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. ant 1.10.7 broadwell, skylake iris Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. archspec 0.1.0 broadwell, skylake iris Utilities A library for detecting, labeling, and reasoning about microarchitectures arpack-ng 3.7.0 broadwell, skylake iris Numerical libraries ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. at-spi2-atk 2.34.1 broadwell, skylake iris Visualisation AT-SPI 2 toolkit bridge at-spi2-core 2.34.0 broadwell, skylake iris Visualisation Assistive Technology Service Provider Interface. binutils 2.32 broadwell, skylake, gpu iris Utilities binutils: GNU binary utilities bzip2 1.0.8 broadwell, skylake, gpu iris Utilities bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.66.0 broadwell, skylake, gpu iris Utilities libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. cairo 1.16.0 broadwell, skylake, gpu iris Visualisation Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB cuDNN 7.6.4.38 gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. double-conversion 3.1.4 broadwell, skylake, gpu iris Libraries Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. expat 2.2.7 broadwell, skylake, gpu iris Utilities Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) flatbuffers 1.12.0 broadwell, skylake, gpu iris Development FlatBuffers: Memory Efficient Serialization Library flex 2.6.4 broadwell, skylake, gpu iris Programming Languages Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. fontconfig 2.13.1 broadwell, skylake, gpu iris Visualisation Fontconfig is a library designed to provide system-wide font configuration, customization and application access. foss 2019b broadwell, skylake iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2019b gpu iris Toolchains (software stacks) GCC based compiler toolchain with CUDA support , and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. freetype 2.10.1 broadwell, skylake, gpu iris Visualisation FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gc 7.6.12 broadwell, skylake iris Libraries The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new. gcccuda 2019b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gettext 0.19.8.1 broadwell, skylake, gpu iris Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation gettext 0.20.1 broadwell, skylake, gpu iris Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation gflags 2.2.2 broadwell, skylake iris Development The gflags package contains a C++ library that implements commandline flags processing. It includes built-in support for standard types such as string and the ability to define flags in the source file in which they are used. giflib 5.2.1 broadwell, skylake, gpu iris Libraries giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. git 2.23.0 broadwell, skylake, gpu iris Utilities Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. glog 0.4.0 broadwell, skylake iris Development A C++ implementation of the Google logging module. gmsh 4.4.0 broadwell, skylake iris CFD/Finite element modelling Salome is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components. gnuplot 5.2.8 broadwell, skylake iris Visualisation Portable interactive, function plotting utility gocryptfs 1.7.1 broadwell, skylake iris Utilities Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. gompi 2019b broadwell, skylake iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2019b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. googletest 1.10.0 broadwell, skylake iris Development Google's framework for writing C++ tests on a variety of platforms gperf 3.1 broadwell, skylake, gpu iris Development GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. gzip 1.10 broadwell, skylake iris Utilities gzip (GNU zip) is a popular data compression program as a replacement for compress h5py 2.10.0 broadwell, skylake, gpu iris Data processing HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. help2man 1.47.4 broadwell, skylake, gpu iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. help2man 1.47.8 broadwell, skylake, gpu iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. hwloc 1.11.12 broadwell, skylake, gpu iris System-level software The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. hypothesis 4.44.2 broadwell, skylake, gpu iris Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. iccifort 2019.5.281 broadwell, skylake, gpu iris Compilers Intel C, C++ & Fortran compilers iccifortcuda 2019b gpu iris Toolchains (software stacks) Intel C, C++ & Fortran compilers with CUDA toolkit iimpi 2019b broadwell, skylake iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2019b gpu iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. imkl 2019.5.281 broadwell, skylake, gpu iris Numerical libraries Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. impi 2018.5.288 broadwell, skylake, gpu iris MPI Intel MPI Library, compatible with MPICH ABI intel 2019b broadwell, skylake iris Toolchains (software stacks) Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2019b gpu iris Toolchains (software stacks) Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI & Intel MKL, with CUDA toolkit intltool 0.51.0 broadwell, skylake, gpu iris Development intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. itac 2019.4.036 broadwell, skylake iris Utilities The Intel Trace Collector is a low-overhead tracing library that performs event-based tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. jemalloc 5.2.1 broadwell, skylake iris Libraries jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. kallisto 0.46.1 broadwell, skylake iris Biology kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. kim-api 2.1.3 broadwell, skylake iris Chemistry Open Knowledgebase of Interatomic Models. KIM is an API and OpenKIM is a collection of interatomic models (potentials) for atomistic simulations. This is a library that can be used by simulation programs to get access to the models in the OpenKIM database. This EasyBuild only installs the API, the models can be installed with the package openkim-models, or the user can install them manually by running kim-api-collections-management install user MODELNAME or kim-api-collections-management install user OpenKIM to install them all. libGLU 9.0.1 broadwell, skylake, gpu iris Visualisation The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. libcerf 1.13 broadwell, skylake iris Mathematics libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. libctl 4.0.0 broadwell, skylake iris Chemistry libctl is a free Guile-based library implementing flexible control files for scientific simulations. libdrm 2.4.99 broadwell, skylake, gpu iris Libraries Direct Rendering Manager runtime library. libepoxy 1.5.4 broadwell, skylake iris Libraries Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.11 broadwell, skylake iris Libraries The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.2.1 broadwell, skylake, gpu iris Libraries The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.2.5 broadwell, skylake iris Libraries GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.5.1 broadwell, skylake, gpu iris Libraries Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.2.0 broadwell, skylake iris Libraries libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 gpu iris Libraries Library to manipulate tensors on the GPU. libiconv 1.16 broadwell, skylake, gpu iris Libraries Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.3 broadwell, skylake, gpu iris Libraries libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libmatheval 1.1.11 broadwell, skylake iris Libraries GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libpciaccess 0.14 broadwell, skylake, gpu iris System-level software Generic PCI access library. libpng 1.6.37 broadwell, skylake, gpu iris Libraries libpng is the official PNG reference library libreadline 8.0 broadwell, skylake, gpu iris Libraries The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 broadwell, skylake, gpu iris Libraries Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtool 2.4.6 broadwell, skylake, gpu iris Libraries GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunistring 0.9.10 broadwell, skylake iris Libraries This library provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard. libunwind 1.3.1 broadwell, skylake, gpu iris Libraries The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libxc 4.3.4 broadwell, skylake iris Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxml2 2.9.9 broadwell, skylake, gpu iris Libraries Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libxslt 1.1.34 broadwell, skylake iris Libraries Libxslt is the XSLT C library developed for the GNOME project (but usable outside of the Gnome platform). libyaml 0.2.2 broadwell, skylake, gpu iris Libraries LibYAML is a YAML parser and emitter written in C. lxml 4.4.2 broadwell, skylake iris Libraries The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. magma 2.5.1 gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. magma 2.5.4 gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. matplotlib 3.1.1 broadwell, skylake iris Visualisation matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. molmod 1.4.5 broadwell, skylake iris Mathematics MolMod is a Python library with many compoments that are useful to write molecular modeling programs. ncurses 6.0 broadwell, skylake, gpu iris Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. ncurses 6.1 broadwell, skylake, gpu iris Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. netCDF-Fortran 4.5.2 broadwell, skylake iris Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.1 broadwell, skylake, gpu iris Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. nettle 3.5.1 broadwell, skylake, gpu iris Libraries Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. nsync 1.24.0 broadwell, skylake, gpu iris Development nsync is a C library that exports various synchronization primitives, such as mutexes numactl 2.0.12 broadwell, skylake, gpu iris Utilities The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. phonopy 2.2.0 broadwell, skylake iris Libraries Phonopy is an open source package of phonon calculations based on the supercell approach. pixman 0.38.4 broadwell, skylake, gpu iris Visualisation Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. pkg-config 0.29.2 broadwell, skylake, gpu iris Development pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c pkg-config --libs --cflags glib-2.0 for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 broadwell, skylake, gpu iris Development pkgconfig is a Python module to interface with the pkg-config command line tool pocl 1.4 gpu iris Libraries Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard protobuf-python 3.10.0 broadwell, skylake, gpu iris Development Python Protocol Buffers runtime library. protobuf 2.5.0 broadwell, skylake iris Development Google Protocol Buffers protobuf 3.10.0 broadwell, skylake iris Development Google Protocol Buffers pybind11 2.4.3 broadwell, skylake, gpu iris Libraries pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. re2c 1.2.1 broadwell, skylake iris Utilities re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. scipy 1.4.1 broadwell, skylake, gpu iris Mathematics SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension for Python. setuptools 41.0.1 broadwell, skylake iris Development Easily download, build, install, upgrade, and uninstall Python packages snappy 1.1.7 broadwell, skylake, gpu iris Libraries Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. sparsehash 2.0.3 broadwell, skylake iris Development An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. tbb 2019_U9 broadwell, skylake iris Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. tbb 2020.2 broadwell, skylake iris Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. texinfo 6.7 broadwell, skylake iris Development Texinfo is the official documentation format of the GNU project. typing-extensions 3.7.4.3 gpu iris Development Typing Extensions \u2013 Backported and Experimental Type Hints for Python util-linux 2.34 broadwell, skylake, gpu iris Utilities Set of Linux utilities x264 20190925 broadwell, skylake, gpu iris Visualisation x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.2 broadwell, skylake, gpu iris Visualisation x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xorg-macros 1.19.2 broadwell, skylake, gpu iris Development X.org macros utilities. xprop 1.2.4 broadwell, skylake iris Visualisation The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information. yaff 1.6.0 broadwell, skylake iris Chemistry Yaff stands for 'Yet another force field'. It is a pythonic force-field code. zlib 1.2.11 broadwell, skylake, gpu iris Libraries zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system.","title":"2019b"},{"location":"software/swsets/2020b/","text":"Alphabetical list of available ULHPC software belonging to the '2020b' software set. To load a software of this set, use: # Eventually: resif-load-swset-[...] module load <category>/<software> [ /<version> ] Software Architectures Clusters Category Description ABAQUS 2021 broadwell, epyc, skylake aion, iris CFD/Finite element modelling Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ABINIT 9.4.1 epyc aion Chemistry ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ABySS 2.2.5 broadwell, epyc, skylake aion, iris Biology Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler ACTC 1.1 broadwell, skylake iris Libraries ACTC converts independent triangles into triangle strips or fans. ANSYS 21.1 broadwell, skylake iris Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. AOCC 3.1.0 epyc aion Compilers AMD Optimized C/C++ & Fortran compilers (AOCC) based on LLVM 12.0 ASE 3.20.1 broadwell, epyc, skylake, gpu aion, iris Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE. ASE uses it automatically when installed. ASE 3.21.1 broadwell, epyc, skylake, gpu aion, iris Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE. ASE uses it automatically when installed. ATK 2.36.0 broadwell, epyc, skylake aion, iris Visualisation ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. Anaconda3 2020.11 broadwell, epyc, skylake aion, iris Programming Languages Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. ArmForge 20.0.3 broadwell, skylake iris Utilities The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 broadwell, skylake iris Utilities Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Armadillo 10.5.3 broadwell, epyc, skylake aion, iris Numerical libraries Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Aspera-CLI 3.9.6 broadwell, epyc, skylake aion, iris Utilities IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. Autoconf 2.69 broadwell, skylake, gpu iris Development Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.2 broadwell, epyc, skylake, gpu aion, iris Development Automake: GNU Standards-compliant Makefile generator Autotools 20200321 broadwell, epyc, skylake, gpu aion, iris Development This bundle collect the standard GNU build tools: Autoconf, Automake and libtool BEDTools 2.30.0 broadwell, epyc, skylake aion, iris Biology BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.11.0 broadwell, epyc, skylake aion, iris Biology Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 broadwell, skylake iris Biology Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 broadwell, skylake iris Biology BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Bazel 3.7.2 broadwell, epyc, skylake, gpu aion, iris Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. BioPerl 1.7.8 broadwell, epyc, skylake aion, iris Biology Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bison 3.5.3 broadwell, epyc, skylake, gpu aion, iris Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Bison 3.7.1 broadwell, epyc, skylake, gpu aion, iris Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Boost.Python 1.74.0 broadwell, epyc, skylake aion, iris Libraries Boost.Python is a C++ library which enables seamless interoperability between C++ and the Python programming language. Boost 1.74.0 broadwell, epyc, skylake aion, iris Development Boost provides free peer-reviewed portable C++ source libraries. Bowtie2 2.4.2 broadwell, epyc, skylake aion, iris Biology Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. CGAL 5.2 broadwell, epyc, skylake aion, iris Numerical libraries The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. CMake 3.18.4 broadwell, epyc, skylake, gpu aion, iris Development CMake, the cross-platform, open-source build system. CMake is a family of tools designed to build, test and package software. CMake 3.20.1 broadwell, epyc, skylake, gpu aion, iris Development CMake, the cross-platform, open-source build system. CMake is a family of tools designed to build, test and package software. CUDA 11.1.1 gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. CUDAcore 11.1.1 gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. Check 0.15.2 gpu iris Libraries Check is a unit testing framework for C. It features a simple interface for defining unit tests, putting little in the way of the developer. Tests are run in a separate address space, so both assertion failures and code errors that cause segmentation faults or other signals can be caught. Test results are reportable in the following: Subunit, TAP, XML, and a generic logging format. Clang 11.0.1 broadwell, epyc, skylake, gpu aion, iris Compilers C, C++, Objective-C compiler, based on LLVM. Does not include C++ standard library -- use libstdc++ from GCC. DB 18.1.40 broadwell, epyc, skylake, gpu aion, iris Utilities Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DB_File 1.855 broadwell, epyc, skylake aion, iris Data processing Perl5 access to Berkeley DB version 1.x. DBus 1.13.18 broadwell, epyc, skylake aion, iris Development D-Bus is a message bus system, a simple way for applications to talk to one another. In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. Dakota 6.15.0 broadwell, skylake iris Mathematics The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" Doxygen 1.8.20 broadwell, epyc, skylake, gpu aion, iris Development Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. ELPA 2020.11.001 broadwell, epyc, skylake aion, iris Mathematics Eigenvalue SoLvers for Petaflop-Applications . EasyBuild 4.4.1 broadwell, epyc, skylake aion, iris Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. EasyBuild 4.4.2 broadwell, epyc, skylake aion, iris Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. EasyBuild 4.5.4 broadwell, epyc, skylake aion, iris Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. Eigen 3.3.8 broadwell, epyc, skylake, gpu aion, iris Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Eigen 3.4.0 broadwell, epyc, skylake, gpu aion, iris Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Elk 7.0.12 broadwell, epyc, skylake aion, iris Physics An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.6 broadwell, epyc, skylake aion, iris Physics Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. FFTW 3.3.8 broadwell, skylake, gpu iris Numerical libraries FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. FFmpeg 4.3.1 broadwell, epyc, skylake, gpu aion, iris Visualisation A complete, cross-platform solution to record, convert and stream audio and video. FLAC 1.3.3 broadwell, epyc, skylake, gpu aion, iris Libraries FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. FLTK 1.3.5 broadwell, skylake iris Visualisation FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FastQC 0.11.9 broadwell, skylake iris Biology FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. Flask 1.1.2 broadwell, epyc, skylake, gpu aion, iris Libraries Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. This module includes the Flask extensions: Flask-Cors Flink 1.11.2 broadwell, epyc, skylake aion, iris Development Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. FreeImage 3.18.0 broadwell, epyc, skylake aion, iris Visualisation FreeImage is an Open Source library project for developers who would like to support popular graphics image formats like PNG, BMP, JPEG, TIFF and others as needed by today's multimedia applications. FreeImage is easy to use, fast, multithreading safe. FriBidi 1.0.10 broadwell, epyc, skylake, gpu aion, iris Programming Languages The Free Implementation of the Unicode Bidirectional Algorithm. GCC 10.2.0 broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 10.2.0 broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GDAL 3.2.1 broadwell, epyc, skylake, gpu aion, iris Data processing GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. GDB 10.1 broadwell, epyc, skylake aion, iris Debugging The GNU Project Debugger GDRCopy 2.1 gpu iris Libraries A low-latency GPU memory copy library based on NVIDIA GPUDirect RDMA technology. GEOS 3.9.1 broadwell, epyc, skylake, gpu aion, iris Mathematics GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GLPK 4.65 broadwell, skylake iris Utilities The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. GLib 2.66.1 broadwell, epyc, skylake, gpu aion, iris Visualisation GLib is one of the base libraries of the GTK+ project GMP 6.2.0 broadwell, epyc, skylake, gpu aion, iris Mathematics GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. GObject-Introspection 1.66.1 broadwell, epyc, skylake aion, iris Development GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. GROMACS 2021 broadwell, epyc, skylake aion, iris Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GROMACS 2021.2 broadwell, epyc, skylake aion, iris Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GSL 2.6 broadwell, skylake, gpu iris Numerical libraries The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. GTK+ 3.24.23 broadwell, epyc, skylake aion, iris Visualisation GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.40.0 broadwell, epyc, skylake aion, iris Visualisation The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. Ghostscript 9.53.3 broadwell, epyc, skylake, gpu aion, iris Utilities Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Go 1.14.1 broadwell, skylake iris Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Go 1.16.6 broadwell, epyc, skylake aion, iris Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Gurobi 9.1.2 broadwell, epyc, skylake aion, iris Mathematics The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. HDF5 1.10.7 broadwell, epyc, skylake, gpu aion, iris Data processing HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF 4.2.15 broadwell, epyc, skylake, gpu aion, iris Data processing HDF (also known as HDF4) is a library and multi-object file format for storing and managing data between machines. HTSlib 1.12 broadwell, epyc, skylake aion, iris Biology A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix Hadoop 2.10.0 broadwell, epyc, skylake aion, iris Utilities Hadoop MapReduce by Cloudera HarfBuzz 2.6.7 broadwell, epyc, skylake aion, iris Visualisation HarfBuzz is an OpenType text shaping engine. Horovod 0.22.0 gpu iris Utilities Horovod is a distributed training framework for TensorFlow. Hypre 2.20.0 broadwell, epyc, skylake aion, iris Numerical libraries Hypre is a library for solving large, sparse linear systems of equations on massively parallel computers. The problems of interest arise in the simulation codes being developed at LLNL and elsewhere to study physical phenomena in the defense, environmental, energy, and biological sciences. ICU 67.1 broadwell, epyc, skylake, gpu aion, iris Libraries ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. ISL 0.23 broadwell, epyc, skylake aion, iris Mathematics isl is a library for manipulating sets and relations of integer points bounded by linear constraints. ImageMagick 7.0.10-35 broadwell, epyc, skylake, gpu aion, iris Visualisation ImageMagick is a software suite to create, edit, compose, or convert bitmap images JasPer 2.0.24 broadwell, epyc, skylake, gpu aion, iris Visualisation The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. Java 1.8.0_241 broadwell, skylake, gpu iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Java 11.0.2 broadwell, epyc, skylake aion, iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Java 13.0.2 broadwell, epyc, skylake aion, iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Java 16.0.1 broadwell, epyc, skylake aion, iris Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. JsonCpp 1.9.4 broadwell, epyc, skylake, gpu aion, iris Libraries JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. Julia 1.6.2 broadwell, epyc, skylake aion, iris Programming Languages Julia is a high-level, high-performance dynamic programming language for numerical computing Keras 2.4.3 broadwell, epyc, skylake, gpu aion, iris Mathematics Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. LAME 3.100 broadwell, skylake, gpu iris Data processing LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. LLVM 10.0.1 broadwell, epyc, skylake, gpu aion, iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LLVM 11.0.0 broadwell, epyc, skylake, gpu aion, iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LMDB 0.9.24 broadwell, skylake, gpu iris Libraries LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.1.0 broadwell, epyc, skylake, gpu aion, iris Libraries tiff: Library and tools for reading and writing TIFF data files LittleCMS 2.11 broadwell, epyc, skylake, gpu aion, iris Visualisation Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Lua 5.4.2 broadwell, epyc, skylake aion, iris Programming Languages Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. M4 1.4.18 broadwell, skylake, gpu iris Development GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. MATLAB 2021a broadwell, epyc, skylake aion, iris Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 broadwell, skylake iris Mathematics METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPC 1.2.1 broadwell, epyc, skylake aion, iris Mathematics Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. It extends the principles of the IEEE-754 standard for fixed precision real floating point numbers to complex numbers, providing well-defined semantics for every operation. At the same time, speed of operation at high precision is a major design goal. MPFR 4.1.0 broadwell, epyc, skylake, gpu aion, iris Mathematics The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. MUMPS 5.3.5 broadwell, epyc, skylake aion, iris Mathematics A parallel sparse direct solver Mako 1.1.3 broadwell, epyc, skylake, gpu aion, iris Development A super-fast templating language that borrows the best ideas from the existing templating languages Mathematica 12.1.0 broadwell, epyc, skylake aion, iris Mathematics Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Maven 3.6.3 broadwell, skylake iris Development Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. Mesa 20.2.1 broadwell, epyc, skylake, gpu aion, iris Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Meson 0.55.3 broadwell, epyc, skylake, gpu aion, iris Utilities Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. NASM 2.15.05 broadwell, epyc, skylake, gpu aion, iris Programming Languages NASM: General-purpose x86 assembler NCCL 2.8.3 gpu iris Libraries The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NLopt 2.6.2 broadwell, epyc, skylake, gpu aion, iris Numerical libraries NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. NSPR 4.29 broadwell, epyc, skylake aion, iris Libraries Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.57 broadwell, epyc, skylake aion, iris Libraries Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. Ninja 1.10.1 broadwell, epyc, skylake, gpu aion, iris Utilities Ninja is a small build system with a focus on speed. OpenBLAS 0.3.12 broadwell, epyc, skylake, gpu aion, iris Numerical libraries OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. OpenCV 4.5.1 broadwell, epyc, skylake aion, iris Visualisation OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenEXR 2.5.5 broadwell, epyc, skylake aion, iris Visualisation OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light & Magic for use in computer imaging applications OpenFOAM 8 epyc aion CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenMPI 4.0.5 broadwell, epyc, skylake, gpu aion, iris MPI The Open MPI Project is an open source MPI-3 implementation. PAPI 6.0.0 broadwell, skylake iris Performance measurements PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PCRE2 10.35 broadwell, epyc, skylake, gpu aion, iris Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.44 broadwell, epyc, skylake, gpu aion, iris Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PETSc 3.14.4 broadwell, epyc, skylake aion, iris Numerical libraries PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. PLUMED 2.7.0 broadwell, epyc, skylake aion, iris Chemistry PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. POV-Ray 3.7.0.8 broadwell, epyc, skylake aion, iris Visualisation The Persistence of Vision Raytracer, or POV-Ray, is a ray tracing program which generates images from a text-based scene description, and is available for a variety of computer platforms. POV-Ray is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports. PROJ 7.2.1 broadwell, epyc, skylake, gpu aion, iris Libraries Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates Pango 1.47.0 broadwell, epyc, skylake aion, iris Visualisation Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParaView 5.8.1 broadwell, epyc, skylake aion, iris Visualisation ParaView is a scientific parallel visualizer. Perl 5.32.0 broadwell, epyc, skylake, gpu aion, iris Programming Languages Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Pillow 8.0.1 broadwell, epyc, skylake, gpu aion, iris Visualisation Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyOpenGL 3.1.5 broadwell, epyc, skylake aion, iris Visualisation PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs. PyQt5 5.15.1 broadwell, epyc, skylake aion, iris Visualisation PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company. This bundle includes PyQtWebEngine, a set of Python bindings for The Qt Company\u2019s Qt WebEngine framework. PyQtGraph 0.11.1 broadwell, epyc, skylake aion, iris Visualisation PyQtGraph is a pure-python graphics and GUI library built on PyQt5/PySide2 and numpy. PyTorch-Geometric 1.6.3 broadwell, epyc, skylake, gpu aion, iris Libraries PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch. PyTorch 1.7.1 gpu iris Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyTorch 1.8.1 broadwell, epyc, skylake, gpu aion, iris Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyTorch 1.9.0 broadwell, epyc, skylake, gpu aion, iris Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyYAML 5.3.1 broadwell, epyc, skylake, gpu aion, iris Libraries PyYAML is a YAML parser and emitter for the Python programming language. Python 2.7.18 broadwell, epyc, skylake, gpu aion, iris Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Python 3.8.6 broadwell, epyc, skylake, gpu aion, iris Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Qt5 5.14.2 broadwell, epyc, skylake aion, iris Development Qt is a comprehensive cross-platform C++ application framework. QuantumESPRESSO 6.7 broadwell iris Chemistry Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). RDFlib 5.0.0 broadwell, epyc, skylake, gpu aion, iris Libraries RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information. R 4.0.5 broadwell, epyc, skylake, gpu aion, iris Programming Languages R is a free software environment for statistical computing and graphics. ReFrame 3.6.3 broadwell, epyc, skylake aion, iris Development ReFrame is a framework for writing regression tests for HPC systems. Ruby 2.7.2 broadwell, epyc, skylake aion, iris Programming Languages Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. SAMtools 1.12 broadwell, epyc, skylake aion, iris Biology SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. SCOTCH 6.1.0 broadwell, epyc, skylake aion, iris Mathematics Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. SDL2 2.0.14 broadwell, epyc, skylake aion, iris Libraries SDL: Simple DirectMedia Layer, a cross-platform multimedia library SLEPc 3.14.2 broadwell, epyc, skylake aion, iris Numerical libraries SLEPc (Scalable Library for Eigenvalue Problem Computations) is a software library for the solution of large scale sparse eigenvalue problems on parallel computers. It is an extension of PETSc and can be used for either standard or generalized eigenproblems, with real or complex arithmetic. It can also be used for computing a partial SVD of a large, sparse, rectangular matrix, and to solve quadratic eigenvalue problems. SQLite 3.33.0 broadwell, epyc, skylake, gpu aion, iris Development SQLite: SQL Database Engine in a C Library SWIG 4.0.2 broadwell, epyc, skylake aion, iris Development SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Salome 9.8.0 broadwell, epyc, skylake aion, iris CFD/Finite element modelling The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. ScaLAPACK 2.1.0 broadwell, epyc, skylake, gpu aion, iris Numerical libraries The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. SciPy-bundle 2020.11 broadwell, epyc, skylake, gpu aion, iris Programming Languages Bundle of Python packages for scientific software Singularity 3.8.1 broadwell, epyc, skylake aion, iris Utilities SingularityCE is an open source container platform designed to be simple, fast, and secure. Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Spack 0.12.1 broadwell, skylake iris Development Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Stata 17 broadwell, epyc, skylake aion, iris Mathematics Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics. SuiteSparse 5.8.1 broadwell, epyc, skylake aion, iris Numerical libraries SuiteSparse is a collection of libraries manipulate sparse matrices. Szip 2.1.1 broadwell, skylake, gpu iris Utilities Szip compression software, providing lossless compression of scientific data Tcl 8.6.10 broadwell, epyc, skylake, gpu aion, iris Programming Languages Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. TensorFlow 2.4.1 broadwell, epyc, skylake, gpu aion, iris Libraries An open-source software library for Machine Intelligence TensorFlow 2.5.0 broadwell, epyc, skylake, gpu aion, iris Libraries An open-source software library for Machine Intelligence Theano 1.1.2 broadwell, epyc, skylake, gpu aion, iris Mathematics Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Tk 8.6.10 broadwell, epyc, skylake, gpu aion, iris Visualisation Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. Tkinter 3.8.6 broadwell, epyc, skylake, gpu aion, iris Programming Languages Tkinter module, built with the Python buildsystem TopHat 2.1.2 broadwell, skylake iris Biology TopHat is a fast splice junction mapper for RNA-Seq reads. UCX 1.9.0 broadwell, epyc, skylake, gpu aion, iris Libraries Unified Communication X An open-source production grade communication framework for data centric and high-performance applications UDUNITS 2.2.26 broadwell, skylake, gpu iris Physics UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. ULHPC-bd 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for BigData Analytics software in use on the UL HPC Facility ULHPC-bio 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2020b epyc aion System-level software Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management & processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2020b gpu iris System-level software Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2020b broadwell, epyc, skylake aion, iris System-level software Misc tools, incl. - perf: Performance tools - tools: General purpose tools UnZip 6.0 broadwell, epyc, skylake, gpu aion, iris Utilities UnZip is an extraction utility for archives compressed in .zip format (also called \"zipfiles\"). Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own Zip program, our primary objectives have been portability and non-MSDOS functionality. VASP 5.4.4 broadwell, skylake iris Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VASP 6.2.1 broadwell, epyc, skylake aion, iris Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VMD 1.9.4a51 broadwell, epyc, skylake aion, iris Visualisation VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. VTK 9.0.1 broadwell, epyc, skylake aion, iris Visualisation The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VTune 2020_update3 broadwell, epyc, skylake aion, iris Utilities Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. Valgrind 3.16.1 broadwell, epyc, skylake aion, iris Debugging Valgrind: Debugging and profiling tools Wannier90 3.1.0 broadwell, epyc, skylake aion, iris Chemistry A tool for obtaining maximally-localised Wannier functions X11 20201008 broadwell, epyc, skylake, gpu aion, iris Visualisation The X Window System (X11) is a windowing system for bitmap displays XML-LibXML 2.0206 broadwell, epyc, skylake aion, iris Data processing Perl binding for libxml2 XZ 5.2.5 broadwell, epyc, skylake, gpu aion, iris Utilities xz: XZ utilities Xvfb 1.20.9 broadwell, epyc, skylake, gpu aion, iris Visualisation Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory. YACS 0.1.8 broadwell, epyc, skylake aion, iris Libraries YACS was created as a lightweight library to define and manage system configurations, such as those commonly found in software designed for scientific experimentation. These \"configurations\" typically cover concepts like hyperparameters used in training a machine learning model or configurable model hyperparameters, such as the depth of a convolutional neural network. Yasm 1.3.0 broadwell, skylake, gpu iris Programming Languages Yasm: Complete rewrite of the NASM assembler with BSD license Z3 4.8.10 broadwell, epyc, skylake, gpu aion, iris Utilities Z3 is a theorem prover from Microsoft Research. Zip 3.0 broadwell, skylake, gpu iris Utilities Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality ant 1.10.9 broadwell, epyc, skylake aion, iris Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. arpack-ng 3.8.0 broadwell, epyc, skylake aion, iris Numerical libraries ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. at-spi2-atk 2.38.0 broadwell, epyc, skylake aion, iris Visualisation AT-SPI 2 toolkit bridge at-spi2-core 2.38.0 broadwell, epyc, skylake aion, iris Visualisation Assistive Technology Service Provider Interface. binutils 2.35 broadwell, epyc, skylake, gpu aion, iris Utilities binutils: GNU binary utilities bokeh 2.2.3 broadwell, epyc, skylake, gpu aion, iris Utilities Statistical and novel interactive HTML plots for Python bzip2 1.0.8 broadwell, skylake, gpu iris Utilities bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.72.0 broadwell, epyc, skylake, gpu aion, iris Utilities libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. cairo 1.16.0 broadwell, skylake, gpu iris Visualisation Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB cuDNN 8.0.4.30 gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN 8.0.5.39 gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. dask 2021.2.0 broadwell, epyc, skylake, gpu aion, iris Data processing Dask natively scales Python. Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. double-conversion 3.1.5 broadwell, epyc, skylake, gpu aion, iris Libraries Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. elfutils 0.183 gpu iris Libraries The elfutils project provides libraries and tools for ELF files and DWARF data. expat 2.2.9 broadwell, epyc, skylake, gpu aion, iris Utilities Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) flatbuffers-python 1.12 broadwell, epyc, skylake, gpu aion, iris Development Python Flatbuffers runtime library. flatbuffers 1.12.0 broadwell, skylake, gpu iris Development FlatBuffers: Memory Efficient Serialization Library flex 2.6.4 broadwell, skylake, gpu iris Programming Languages Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. fontconfig 2.13.92 broadwell, epyc, skylake, gpu aion, iris Visualisation Fontconfig is a library designed to provide system-wide font configuration, customization and application access. foss 2020b broadwell, epyc, skylake aion, iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2020b gpu iris Toolchains (software stacks) GCC based compiler toolchain with CUDA support , and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. freetype 2.10.3 broadwell, epyc, skylake, gpu aion, iris Visualisation FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gcccuda 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gettext 0.21 broadwell, epyc, skylake, gpu aion, iris Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation giflib 5.2.1 broadwell, skylake, gpu iris Libraries giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. git 2.28.0 broadwell, epyc, skylake, gpu aion, iris Utilities Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. gmsh 4.8.4 broadwell, epyc, skylake aion, iris Mathematics Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor. gnuplot 5.4.1 broadwell, epyc, skylake aion, iris Visualisation Portable interactive, function plotting utility gocryptfs 2.0.1 broadwell, epyc, skylake aion, iris Utilities Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. gompi 2020b broadwell, epyc, skylake aion, iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. gperf 3.1 broadwell, skylake, gpu iris Development GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. groff 1.22.4 broadwell, epyc, skylake, gpu aion, iris Utilities Groff (GNU troff) is a typesetting system that reads plain text mixed with formatting commands and produces formatted output. gzip 1.10 broadwell, skylake iris Utilities gzip (GNU zip) is a popular data compression program as a replacement for compress h5py 3.1.0 broadwell, epyc, skylake, gpu aion, iris Data processing HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. help2man 1.47.16 broadwell, epyc, skylake, gpu aion, iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. help2man 1.47.4 broadwell, epyc, skylake, gpu aion, iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. hwloc 2.2.0 broadwell, epyc, skylake, gpu aion, iris System-level software The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. hypothesis 5.41.2 broadwell, epyc, skylake, gpu aion, iris Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. hypothesis 5.41.5 broadwell, epyc, skylake, gpu aion, iris Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. iccifort 2020.4.304 broadwell, epyc, skylake, gpu aion, iris Compilers Intel C, C++ & Fortran compilers iccifortcuda 2020b gpu iris Toolchains (software stacks) Intel C, C++ & Fortran compilers with CUDA toolkit iimpi 2020b broadwell, epyc, skylake, gpu aion, iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2020b gpu iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. imkl 2020.4.304 broadwell, epyc, skylake, gpu aion, iris Numerical libraries Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. impi 2019.9.304 broadwell, epyc, skylake, gpu aion, iris MPI Intel MPI Library, compatible with MPICH ABI intel 2020b broadwell, epyc, skylake, gpu aion, iris Toolchains (software stacks) Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2020b gpu iris Toolchains (software stacks) Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI & Intel MKL, with CUDA toolkit intltool 0.51.0 broadwell, skylake, gpu iris Development intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. libGLU 9.0.1 broadwell, skylake, gpu iris Visualisation The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. libarchive 3.4.3 broadwell, epyc, skylake, gpu aion, iris Utilities Multi-format archive and compression library libcerf 1.14 broadwell, epyc, skylake aion, iris Mathematics libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. libdrm 2.4.102 broadwell, epyc, skylake, gpu aion, iris Libraries Direct Rendering Manager runtime library. libepoxy 1.5.4 broadwell, skylake iris Libraries Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.12 broadwell, epyc, skylake aion, iris Libraries The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.3 broadwell, epyc, skylake, gpu aion, iris Libraries The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.3.0 broadwell, epyc, skylake aion, iris Libraries GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.6.0 broadwell, epyc, skylake, gpu aion, iris Libraries Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.3.2 broadwell, epyc, skylake, gpu aion, iris Libraries libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 gpu iris Libraries Library to manipulate tensors on the GPU. libiconv 1.16 broadwell, skylake, gpu iris Libraries Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.5 broadwell, epyc, skylake, gpu aion, iris Libraries libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libogg 1.3.4 broadwell, epyc, skylake, gpu aion, iris Libraries Ogg is a multimedia container format, and the native file and stream format for the Xiph.org multimedia codecs. libpciaccess 0.16 broadwell, epyc, skylake, gpu aion, iris System-level software Generic PCI access library. libpng 1.6.37 broadwell, skylake, gpu iris Libraries libpng is the official PNG reference library libreadline 8.0 broadwell, skylake, gpu iris Libraries The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 broadwell, skylake, gpu iris Libraries Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtirpc 1.3.1 broadwell, epyc, skylake, gpu aion, iris Libraries Libtirpc is a port of Suns Transport-Independent RPC library to Linux. libtool 2.4.6 broadwell, skylake, gpu iris Libraries GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunwind 1.4.0 broadwell, epyc, skylake, gpu aion, iris Libraries The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libvorbis 1.3.7 broadwell, epyc, skylake, gpu aion, iris Libraries Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressed audio format libwebp 1.1.0 broadwell, epyc, skylake aion, iris Libraries WebP is a modern image format that provides superior lossless and lossy compression for images on the web. Using WebP, webmasters and web developers can create smaller, richer images that make the web faster. libxc 4.3.4 broadwell, skylake iris Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxc 5.1.2 broadwell, epyc, skylake aion, iris Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxml2 2.9.10 broadwell, epyc, skylake, gpu aion, iris Libraries Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libyaml 0.2.5 broadwell, epyc, skylake, gpu aion, iris Libraries LibYAML is a YAML parser and emitter written in C. lz4 1.9.2 broadwell, epyc, skylake, gpu aion, iris Libraries LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core. It features an extremely fast decoder, with speed in multiple GB/s per core. magma 2.5.4 gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. makeinfo 6.7 broadwell, epyc, skylake, gpu aion, iris Development makeinfo is part of the Texinfo project, the official documentation format of the GNU project. matplotlib 3.3.3 broadwell, epyc, skylake, gpu aion, iris Visualisation matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. ncurses 6.2 broadwell, epyc, skylake, gpu aion, iris Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. netCDF-Fortran 4.5.3 epyc aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.4 broadwell, epyc, skylake, gpu aion, iris Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. nettle 3.6 broadwell, epyc, skylake, gpu aion, iris Libraries Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. networkx 2.5 broadwell, epyc, skylake, gpu aion, iris Utilities NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. nodejs 12.19.0 broadwell, epyc, skylake, gpu aion, iris Programming Languages Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. nsync 1.24.0 broadwell, skylake, gpu iris Development nsync is a C library that exports various synchronization primitives, such as mutexes numactl 2.0.13 broadwell, epyc, skylake, gpu aion, iris Utilities The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. numba 0.52.0 broadwell, epyc, skylake, gpu aion, iris Programming Languages Numba is an Open Source NumPy-aware optimizing compiler for Python sponsored by Continuum Analytics, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code. pixman 0.40.0 broadwell, epyc, skylake, gpu aion, iris Visualisation Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. pkg-config 0.29.2 broadwell, skylake, gpu iris Development pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c pkg-config --libs --cflags glib-2.0 for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 broadwell, skylake, gpu iris Development pkgconfig is a Python module to interface with the pkg-config command line tool pocl 1.6 gpu iris Libraries Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard protobuf-python 3.14.0 broadwell, epyc, skylake, gpu aion, iris Development Python Protocol Buffers runtime library. protobuf 2.5.0 broadwell, skylake iris Development Google Protocol Buffers protobuf 3.14.0 broadwell, epyc, skylake aion, iris Development Google Protocol Buffers pybind11 2.6.0 broadwell, epyc, skylake, gpu aion, iris Libraries pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. re2c 2.0.3 broadwell, epyc, skylake aion, iris Utilities re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. scikit-build 0.11.1 broadwell, epyc, skylake, gpu aion, iris Libraries Scikit-Build, or skbuild, is an improved build system generator for CPython C/C++/Fortran/Cython extensions. scikit-image 0.18.1 broadwell, epyc, skylake, gpu aion, iris Visualisation scikit-image is a collection of algorithms for image processing. scikit-learn 0.23.2 broadwell, epyc, skylake, gpu aion, iris Data processing Scikit-learn integrates machine learning algorithms in the tightly-knit scientific Python world, building upon numpy, scipy, and matplotlib. As a machine-learning module, it provides versatile tools for data mining and analysis in any field of science and engineering. It strives to be simple and efficient, accessible to everybody, and reusable in various contexts. snappy 1.1.8 broadwell, epyc, skylake, gpu aion, iris Libraries Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. sparsehash 2.0.4 broadwell, epyc, skylake aion, iris Development An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. spglib-python 1.16.0 broadwell, epyc, skylake, gpu aion, iris Chemistry Spglib for Python. Spglib is a library for finding and handling crystal symmetries written in C. tbb 2020.3 broadwell, epyc, skylake aion, iris Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. tqdm 4.56.2 broadwell, epyc, skylake, gpu aion, iris Libraries A fast, extensible progress bar for Python and CLI typing-extensions 3.7.4.3 gpu iris Development Typing Extensions \u2013 Backported and Experimental Type Hints for Python util-linux 2.36 broadwell, epyc, skylake, gpu aion, iris Utilities Set of Linux utilities x264 20201026 broadwell, epyc, skylake, gpu aion, iris Visualisation x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.3 broadwell, epyc, skylake, gpu aion, iris Visualisation x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xorg-macros 1.19.2 broadwell, skylake, gpu iris Development X.org macros utilities. xprop 1.2.5 broadwell, epyc, skylake aion, iris Visualisation The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information. zlib 1.2.11 broadwell, skylake, gpu iris Libraries zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd 1.4.5 broadwell, epyc, skylake, gpu aion, iris Libraries Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set.","title":"2020a"},{"location":"software/swsets/all_softwares/","text":"Software Versions Swsets Architectures Clusters Category Description ABAQUS 2018, 2021 2019b, 2020b broadwell, skylake, epyc iris, aion CFD/Finite element modelling Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. ABINIT 9.4.1 2020b epyc aion Chemistry ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ABySS 2.2.5 2020b broadwell, epyc, skylake aion, iris Biology Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler ACTC 1.1 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries ACTC converts independent triangles into triangle strips or fans. ANSYS 19.4, 21.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. AOCC 3.1.0 2020b epyc aion Compilers AMD Optimized C/C++ & Fortran compilers (AOCC) based on LLVM 12.0 ASE 3.19.0, 3.20.1, 3.21.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Chemistry ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE. ASE uses it automatically when installed. ATK 2.34.1, 2.36.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. Advisor 2019_update5 2019b broadwell, skylake iris Performance measurements Vectorization Optimization and Thread Prototyping - Vectorize & thread code or performance \u201cdies\u201d - Easy workflow + data + tips = faster code faster - Prioritize, Prototype & Predict performance gain Anaconda3 2020.02, 2020.11 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. ArmForge 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Armadillo 10.5.3, 9.900.1 2020b, 2019b broadwell, epyc, skylake aion, iris Numerical libraries Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Arrow 0.16.0 2019b broadwell, skylake iris Data processing Apache Arrow (incl. PyArrow Python bindings)), a cross-language development platform for in-memory data. Aspera-CLI 3.9.1, 3.9.6 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. Autoconf 2.69 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.1, 1.16.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Automake: GNU Standards-compliant Makefile generator Autotools 20180311, 20200321 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development This bundle collect the standard GNU build tools: Autoconf, Automake and libtool BEDTools 2.29.2, 2.30.0 2019b, 2020b broadwell, skylake, epyc iris, aion Biology BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.11.0, 2.9.0 2020b, 2019b broadwell, epyc, skylake aion, iris Biology Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion Biology BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. Bazel 0.26.1, 0.29.1, 3.7.2 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Development Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. BioPerl 1.7.2, 1.7.8 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bison 3.3.2, 3.5.3, 3.7.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. Boost.Python 1.74.0 2020b broadwell, epyc, skylake aion, iris Libraries Boost.Python is a C++ library which enables seamless interoperability between C++ and the Python programming language. Boost 1.71.0, 1.74.0 2019b, 2020b broadwell, skylake, epyc iris, aion Development Boost provides free peer-reviewed portable C++ source libraries. Bowtie2 2.3.5.1, 2.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Biology Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. CGAL 4.14.1, 5.2 2019b, 2020b broadwell, skylake, epyc iris, aion Numerical libraries The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. CMake 3.15.3, 3.18.4, 3.20.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development CMake, the cross-platform, open-source build system. CMake is a family of tools designed to build, test and package software. CPLEX 12.10 2019b broadwell, skylake iris Mathematics IBM ILOG CPLEX Optimizer's mathematical programming technology enables analytical decision support for improving efficiency, reducing costs, and increasing profitability. CRYSTAL 17 2019b broadwell, skylake iris Chemistry The CRYSTAL package performs ab initio calculations of the ground state energy, energy gradient, electronic wave function and properties of periodic systems. Hartree-Fock or Kohn- Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used. CUDA 10.1.243, 11.1.1 2019b, 2020b gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. CUDAcore 11.1.1 2020b gpu iris System-level software CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. Check 0.15.2 2020b gpu iris Libraries Check is a unit testing framework for C. It features a simple interface for defining unit tests, putting little in the way of the developer. Tests are run in a separate address space, so both assertion failures and code errors that cause segmentation faults or other signals can be caught. Test results are reportable in the following: Subunit, TAP, XML, and a generic logging format. Clang 11.0.1, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers C, C++, Objective-C compiler, based on LLVM. Does not include C++ standard library -- use libstdc++ from GCC. CubeGUI 4.4.4 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube graphical report explorer. CubeLib 4.4.4 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube general purpose C++ library component and command-line tools. CubeWriter 4.4.3 2019b broadwell, skylake iris Performance measurements Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube high-performance C writer library component. DB 18.1.32, 18.1.40 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DB_File 1.855 2020b broadwell, epyc, skylake aion, iris Data processing Perl5 access to Berkeley DB version 1.x. DBus 1.13.12, 1.13.18 2019b, 2020b broadwell, skylake, epyc iris, aion Development D-Bus is a message bus system, a simple way for applications to talk to one another. In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. DMTCP 2.5.2 2019b broadwell, skylake iris Utilities DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. Dakota 6.11.0, 6.15.0 2019b, 2020b broadwell, skylake iris Mathematics The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" Doxygen 1.8.16, 1.8.20 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. ELPA 2019.11.001, 2020.11.001 2019b, 2020b broadwell, epyc, skylake iris, aion Mathematics Eigenvalue SoLvers for Petaflop-Applications . EasyBuild 4.3.0, 4.3.3, 4.4.1, 4.4.2, 4.5.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. Eigen 3.3.7, 3.3.8, 3.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. Elk 6.3.2, 7.0.12 2019b, 2020b broadwell, skylake, epyc iris, aion Physics An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.1, 6.7.6 2019b, 2020b broadwell, skylake, epyc iris, aion Physics Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. FFTW 3.3.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. FFmpeg 4.2.1, 4.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation A complete, cross-platform solution to record, convert and stream audio and video. FLAC 1.3.3 2020b broadwell, epyc, skylake, gpu aion, iris Libraries FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. FLTK 1.3.5 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FastQC 0.11.9 2019b, 2020b broadwell, skylake, epyc iris, aion Biology FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. Flask 1.1.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. This module includes the Flask extensions: Flask-Cors Flink 1.11.2 2020b broadwell, epyc, skylake aion, iris Development Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. FreeImage 3.18.0 2020b broadwell, epyc, skylake aion, iris Visualisation FreeImage is an Open Source library project for developers who would like to support popular graphics image formats like PNG, BMP, JPEG, TIFF and others as needed by today's multimedia applications. FreeImage is easy to use, fast, multithreading safe. FriBidi 1.0.10, 1.0.5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Programming Languages The Free Implementation of the Unicode Bidirectional Algorithm. GCC 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GDAL 3.0.2, 3.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. GDB 10.1, 9.1 2020b, 2019b broadwell, epyc, skylake aion, iris Debugging The GNU Project Debugger GDRCopy 2.1 2020b gpu iris Libraries A low-latency GPU memory copy library based on NVIDIA GPUDirect RDMA technology. GEOS 3.8.0, 3.9.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GLPK 4.65 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. GLib 2.62.0, 2.66.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation GLib is one of the base libraries of the GTK+ project GMP 6.1.2, 6.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. GObject-Introspection 1.63.1, 1.66.1 2019b, 2020b broadwell, skylake, epyc iris, aion Development GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. GPAW-setups 0.9.20000 2019b broadwell, skylake iris Chemistry PAW setup for the GPAW Density Functional Theory package. Users can install setups manually using 'gpaw install-data' or use setups from this package. The versions of GPAW and GPAW-setups can be intermixed. GPAW 20.1.0 2019b broadwell, skylake iris Chemistry GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). It uses real-space uniform grids and multigrid methods or atom-centered basis-functions. GROMACS 2019.4, 2019.6, 2020, 2021, 2021.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Biology GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. GSL 2.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. GTK+ 3.24.13, 3.24.23 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.38.2, 2.40.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. Ghostscript 9.50, 9.53.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Go 1.14.1, 1.16.6 2019b, 2020b broadwell, skylake, epyc iris, aion Compilers Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. Guile 1.8.8, 2.2.4 2019b broadwell, skylake iris Programming Languages Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Gurobi 9.0.0, 9.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. HDF5 1.10.5, 1.10.7 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF 4.2.15 2020b broadwell, epyc, skylake, gpu aion, iris Data processing HDF (also known as HDF4) is a library and multi-object file format for storing and managing data between machines. HTSlib 1.10.2, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Biology A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix Hadoop 2.10.0 2020b broadwell, epyc, skylake aion, iris Utilities Hadoop MapReduce by Cloudera HarfBuzz 2.6.4, 2.6.7 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation HarfBuzz is an OpenType text shaping engine. Harminv 1.4.1 2019b broadwell, skylake iris Mathematics Harminv is a free program (and accompanying library) to solve the problem of harmonic inversion - given a discrete-time, finite-length signal that consists of a sum of finitely-many sinusoids (possibly exponentially decaying) in a given bandwidth, it determines the frequencies, decay constants, amplitudes, and phases of those sinusoids. Horovod 0.19.1, 0.22.0 2019b, 2020b broadwell, skylake, gpu iris Utilities Horovod is a distributed training framework for TensorFlow. Hypre 2.20.0 2020b broadwell, epyc, skylake aion, iris Numerical libraries Hypre is a library for solving large, sparse linear systems of equations on massively parallel computers. The problems of interest arise in the simulation codes being developed at LLNL and elsewhere to study physical phenomena in the defense, environmental, energy, and biological sciences. ICU 64.2, 67.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. ISL 0.23 2020b broadwell, epyc, skylake aion, iris Mathematics isl is a library for manipulating sets and relations of integer points bounded by linear constraints. ImageMagick 7.0.10-35, 7.0.9-5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation ImageMagick is a software suite to create, edit, compose, or convert bitmap images Inspector 2019_update5 2019b broadwell, skylake iris Utilities Intel Inspector XE is an easy to use memory error checker and thread checker for serial and parallel applications JasPer 2.0.14, 2.0.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. Java 1.8.0_241, 11.0.2, 13.0.2, 16.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Jellyfish 2.3.0 2019b broadwell, skylake iris Biology Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA. JsonCpp 1.9.3, 1.9.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. Julia 1.4.1, 1.6.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Julia is a high-level, high-performance dynamic programming language for numerical computing Keras 2.3.1, 2.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Mathematics Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. LAME 3.100 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. LLVM 10.0.1, 11.0.0, 9.0.0, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Compilers The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. LMDB 0.9.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.0.10, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries tiff: Library and tools for reading and writing TIFF data files LittleCMS 2.11, 2.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Lua 5.1.5, 5.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. M4 1.4.18 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. MATLAB 2019b, 2020a, 2021a 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPC 1.2.1 2020b broadwell, epyc, skylake aion, iris Mathematics Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. It extends the principles of the IEEE-754 standard for fixed precision real floating point numbers to complex numbers, providing well-defined semantics for every operation. At the same time, speed of operation at high precision is a major design goal. MPFR 4.0.2, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mathematics The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. MUMPS 5.3.5 2020b broadwell, epyc, skylake aion, iris Mathematics A parallel sparse direct solver Mako 1.1.0, 1.1.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development A super-fast templating language that borrows the best ideas from the existing templating languages Mathematica 12.0.0, 12.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Maven 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Development Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. Meep 1.4.3 2019b broadwell, skylake iris Physics Meep (or MEEP) is a free finite-difference time-domain (FDTD) simulation software package developed at MIT to model electromagnetic systems. Mesa 19.1.7, 19.2.1, 20.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. Meson 0.51.2, 0.55.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. Mesquite 2.3.0 2019b broadwell, skylake iris Mathematics Mesh-Quality Improvement Library NAMD 2.13 2019b broadwell, skylake iris Chemistry NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NASM 2.14.02, 2.15.05 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages NASM: General-purpose x86 assembler NCCL 2.4.8, 2.8.3 2019b, 2020b gpu iris Libraries The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NLopt 2.6.1, 2.6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. NSPR 4.21, 4.29 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.45, 3.57 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. Ninja 1.10.1, 1.9.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Utilities Ninja is a small build system with a focus on speed. OPARI2 2.0.5 2019b broadwell, skylake iris Performance measurements OPARI2, the successor of Forschungszentrum Juelich's OPARI, is a source-to-source instrumentation tool for OpenMP and hybrid codes. It surrounds OpenMP directives and runtime library calls with calls to the POMP2 measurement interface. OTF2 2.2 2019b broadwell, skylake iris Performance measurements The Open Trace Format 2 is a highly scalable, memory efficient event trace data format plus support library. It is the new standard trace format for Scalasca, Vampir, and TAU and is open for other tools. OpenBLAS 0.3.12, 0.3.7 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Numerical libraries OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. OpenCV 4.2.0, 4.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenEXR 2.5.5 2020b broadwell, epyc, skylake aion, iris Visualisation OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light & Magic for use in computer imaging applications OpenFOAM-Extend 4.1-20200408 2019b broadwell, skylake iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenFOAM 8, v1912 2020b, 2019b epyc, broadwell, skylake aion, iris CFD/Finite element modelling OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenMPI 3.1.4, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion MPI The Open MPI Project is an open source MPI-3 implementation. PAPI 6.0.0 2019b, 2020b broadwell, skylake, epyc iris, aion Performance measurements PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PCRE2 10.33, 10.35 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.43, 8.44 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PDT 3.25 2019b broadwell, skylake iris Performance measurements Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program knowledge accessible to developers of static and dynamic analysis tools. PDT implements a standard program representation, the program database (PDB), that can be accessed in a uniform way through a class library supporting common PDB operations. PETSc 3.14.4 2020b broadwell, epyc, skylake aion, iris Numerical libraries PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. PGI 19.10 2019b broadwell, skylake iris Compilers C, C++ and Fortran compilers from The Portland Group - PGI PLUMED 2.5.3, 2.7.0 2019b, 2020b broadwell, skylake, epyc iris, aion Chemistry PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. POV-Ray 3.7.0.8 2020b broadwell, epyc, skylake aion, iris Visualisation The Persistence of Vision Raytracer, or POV-Ray, is a ray tracing program which generates images from a text-based scene description, and is available for a variety of computer platforms. POV-Ray is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports. PROJ 6.2.1, 7.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates Pango 1.44.7, 1.47.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParMETIS 4.0.3 2019b broadwell, skylake iris Mathematics ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes. ParMGridGen 1.0 2019b broadwell, skylake iris Mathematics ParMGridGen is an MPI-based parallel library that is based on the serial package MGridGen, that implements (serial) algorithms for obtaining a sequence of successive coarse grids that are well-suited for geometric multigrid methods. ParaView 5.6.2, 5.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation ParaView is a scientific parallel visualizer. Perl 5.30.0, 5.32.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Pillow 6.2.1, 8.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyOpenGL 3.1.5 2020b broadwell, epyc, skylake aion, iris Visualisation PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs. PyQt5 5.15.1 2020b broadwell, epyc, skylake aion, iris Visualisation PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company. This bundle includes PyQtWebEngine, a set of Python bindings for The Qt Company\u2019s Qt WebEngine framework. PyQtGraph 0.11.1 2020b broadwell, epyc, skylake aion, iris Visualisation PyQtGraph is a pure-python graphics and GUI library built on PyQt5/PySide2 and numpy. PyTorch-Geometric 1.6.3 2020b broadwell, epyc, skylake, gpu aion, iris Libraries PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch. PyTorch 1.4.0, 1.7.1, 1.8.1, 1.9.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. PyYAML 5.1.2, 5.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries PyYAML is a YAML parser and emitter for the Python programming language. Python 2.7.16, 2.7.18, 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Python is a programming language that lets you work more quickly and integrate your systems more effectively. Qt5 5.13.1, 5.14.2 2019b, 2020b broadwell, skylake, epyc iris, aion Development Qt is a comprehensive cross-platform C++ application framework. QuantumESPRESSO 6.7 2019b, 2020b broadwell, epyc, skylake iris, aion Chemistry Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). RDFlib 5.0.0 2020b broadwell, epyc, skylake, gpu aion, iris Libraries RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information. R 3.6.2, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages R is a free software environment for statistical computing and graphics. ReFrame 2.21, 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Development ReFrame is a framework for writing regression tests for HPC systems. Ruby 2.7.1, 2.7.2 2019b, 2020b broadwell, skylake, epyc iris, aion Programming Languages Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. Rust 1.37.0 2019b broadwell, skylake iris Programming Languages Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. SAMtools 1.10, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Biology SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. SCOTCH 6.0.9, 6.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. SDL2 2.0.14 2020b broadwell, epyc, skylake aion, iris Libraries SDL: Simple DirectMedia Layer, a cross-platform multimedia library SIONlib 1.7.6 2019b broadwell, skylake iris Libraries SIONlib is a scalable I/O library for parallel access to task-local files. The library not only supports writing and reading binary data to or from several thousands of processors into a single or a small number of physical files, but also provides global open and close functions to access SIONlib files in parallel. This package provides a stripped-down installation of SIONlib for use with performance tools (e.g., Score-P), with renamed symbols to avoid conflicts when an application using SIONlib itself is linked against a tool requiring a different SIONlib version. SLEPc 3.14.2 2020b broadwell, epyc, skylake aion, iris Numerical libraries SLEPc (Scalable Library for Eigenvalue Problem Computations) is a software library for the solution of large scale sparse eigenvalue problems on parallel computers. It is an extension of PETSc and can be used for either standard or generalized eigenproblems, with real or complex arithmetic. It can also be used for computing a partial SVD of a large, sparse, rectangular matrix, and to solve quadratic eigenvalue problems. SQLite 3.29.0, 3.33.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development SQLite: SQL Database Engine in a C Library SWIG 4.0.1, 4.0.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Salmon 1.1.0 2019b broadwell, skylake iris Biology Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data. Salome 8.5.0, 9.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion CFD/Finite element modelling The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. ScaLAPACK 2.0.2, 2.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. Scalasca 2.5 2019b broadwell, skylake iris Performance measurements Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks -- in particular those concerning communication and synchronization -- and offers guidance in exploring their causes. SciPy-bundle 2019.10, 2020.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Bundle of Python packages for scientific software Score-P 6.0 2019b broadwell, skylake iris Performance measurements The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Singularity 3.6.0, 3.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities SingularityCE is an open source container platform designed to be simple, fast, and secure. Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Spack 0.12.1 2019b, 2020b broadwell, skylake, epyc iris, aion Development Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Spark 2.4.3 2019b broadwell, skylake iris Development Spark is Hadoop MapReduce done in memory Stata 17 2020b broadwell, epyc, skylake aion, iris Mathematics Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics. SuiteSparse 5.8.1 2020b broadwell, epyc, skylake aion, iris Numerical libraries SuiteSparse is a collection of libraries manipulate sparse matrices. Sumo 1.3.1 2019b broadwell, skylake iris Utilities Sumo is an open source, highly portable, microscopic and continuous traffic simulation package designed to handle large road networks. Szip 2.1.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Szip compression software, providing lossless compression of scientific data Tcl 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Programming Languages Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. TensorFlow 1.15.5, 2.1.0, 2.4.1, 2.5.0 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Libraries An open-source software library for Machine Intelligence Theano 1.0.4, 1.1.2 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Mathematics Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Tk 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Visualisation Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. Tkinter 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Programming Languages Tkinter module, built with the Python buildsystem TopHat 2.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Biology TopHat is a fast splice junction mapper for RNA-Seq reads. Trinity 2.10.0 2019b broadwell, skylake iris Biology Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-Seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-Seq reads. UCX 1.9.0 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Unified Communication X An open-source production grade communication framework for data centric and high-performance applications UDUNITS 2.2.26 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Physics UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. ULHPC-bd 2020b 2020b broadwell, epyc, skylake aion, iris System-level software Generic Module bundle for BigData Analytics software in use on the UL HPC Facility ULHPC-bio 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management & processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2019b, 2020b 2019b, 2020b gpu iris System-level software Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle for High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion System-level software Misc tools, incl. - perf: Performance tools - tools: General purpose tools UnZip 6.0 2020b broadwell, epyc, skylake, gpu aion, iris Utilities UnZip is an extraction utility for archives compressed in .zip format (also called \"zipfiles\"). Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own Zip program, our primary objectives have been portability and non-MSDOS functionality. VASP 5.4.4, 6.2.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Physics The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VMD 1.9.4a51 2020b broadwell, epyc, skylake aion, iris Visualisation VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. VTK 8.2.0, 9.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VTune 2019_update8, 2020_update3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. Valgrind 3.15.0, 3.16.1 2019b, 2020b broadwell, skylake, epyc iris, aion Debugging Valgrind: Debugging and profiling tools VirtualGL 2.6.2 2019b broadwell, skylake iris Visualisation VirtualGL is an open source toolkit that gives any Linux or Unix remote display software the ability to run OpenGL applications with full hardware acceleration. Voro++ 0.4.6 2019b broadwell, skylake iris Mathematics Voro++ is a software library for carrying out three-dimensional computations of the Voronoi tessellation. A distinguishing feature of the Voro++ library is that it carries out cell-based calculations, computing the Voronoi cell for each particle individually. It is particularly well-suited for applications that rely on cell-based statistics, where features of Voronoi cells (eg. volume, centroid, number of faces) can be used to analyze a system of particles. Wannier90 3.1.0 2020b broadwell, epyc, skylake aion, iris Chemistry A tool for obtaining maximally-localised Wannier functions X11 20190717, 20201008 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The X Window System (X11) is a windowing system for bitmap displays XML-LibXML 2.0201, 2.0206 2019b, 2020b broadwell, skylake, epyc iris, aion Data processing Perl binding for libxml2 XZ 5.2.4, 5.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities xz: XZ utilities Xerces-C++ 3.2.2 2019b broadwell, skylake iris Libraries Xerces-C++ is a validating XML parser written in a portable subset of C++. Xerces-C++ makes it easy to give your application the ability to read and write XML data. A shared library is provided for parsing, generating, manipulating, and validating XML documents using the DOM, SAX, and SAX2 APIs. Xvfb 1.20.9 2020b broadwell, epyc, skylake, gpu aion, iris Visualisation Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory. YACS 0.1.8 2020b broadwell, epyc, skylake aion, iris Libraries YACS was created as a lightweight library to define and manage system configurations, such as those commonly found in software designed for scientific experimentation. These \"configurations\" typically cover concepts like hyperparameters used in training a machine learning model or configurable model hyperparameters, such as the depth of a convolutional neural network. Yasm 1.3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Yasm: Complete rewrite of the NASM assembler with BSD license Z3 4.8.10 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Z3 is a theorem prover from Microsoft Research. Zip 3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality ant 1.10.6, 1.10.7, 1.10.9 2019b, 2020b broadwell, skylake, epyc iris, aion Development Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. archspec 0.1.0 2019b broadwell, skylake iris Utilities A library for detecting, labeling, and reasoning about microarchitectures arpack-ng 3.7.0, 3.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion Numerical libraries ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. at-spi2-atk 2.34.1, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation AT-SPI 2 toolkit bridge at-spi2-core 2.34.0, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Assistive Technology Service Provider Interface. binutils 2.32, 2.35 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities binutils: GNU binary utilities bokeh 2.2.3 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Statistical and novel interactive HTML plots for Python bzip2 1.0.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.66.0, 7.72.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. cairo 1.16.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB cuDNN 7.6.4.38, 8.0.4.30, 8.0.5.39 2019b, 2020b gpu iris Numerical libraries The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. dask 2021.2.0 2020b broadwell, epyc, skylake, gpu aion, iris Data processing Dask natively scales Python. Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. double-conversion 3.1.4, 3.1.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. elfutils 0.183 2020b gpu iris Libraries The elfutils project provides libraries and tools for ELF files and DWARF data. expat 2.2.7, 2.2.9 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) flatbuffers-python 1.12 2020b broadwell, epyc, skylake, gpu aion, iris Development Python Flatbuffers runtime library. flatbuffers 1.12.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development FlatBuffers: Memory Efficient Serialization Library flex 2.6.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Programming Languages Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. fontconfig 2.13.1, 2.13.92 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Fontconfig is a library designed to provide system-wide font configuration, customization and application access. foss 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GCC based compiler toolchain with CUDA support , and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. freetype 2.10.1, 2.10.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gc 7.6.12 2019b broadwell, skylake iris Libraries The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new. gcccuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gettext 0.19.8.1, 0.20.1, 0.21 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation gflags 2.2.2 2019b broadwell, skylake iris Development The gflags package contains a C++ library that implements commandline flags processing. It includes built-in support for standard types such as string and the ability to define flags in the source file in which they are used. giflib 5.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. git 2.23.0, 2.28.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. glog 0.4.0 2019b broadwell, skylake iris Development A C++ implementation of the Google logging module. gmsh 4.4.0 2019b broadwell, skylake iris CFD/Finite element modelling Salome is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components. gmsh 4.8.4 2020b broadwell, epyc, skylake aion, iris Mathematics Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor. gnuplot 5.2.8, 5.4.1 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation Portable interactive, function plotting utility gocryptfs 1.7.1, 2.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. gompi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. googletest 1.10.0 2019b broadwell, skylake iris Development Google's framework for writing C++ tests on a variety of platforms gperf 3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. groff 1.22.4 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Groff (GNU troff) is a typesetting system that reads plain text mixed with formatting commands and produces formatted output. gzip 1.10 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Utilities gzip (GNU zip) is a popular data compression program as a replacement for compress h5py 2.10.0, 3.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. help2man 1.47.16, 1.47.4, 1.47.8 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Utilities help2man produces simple manual pages from the '--help' and '--version' output of other commands. hwloc 1.11.12, 2.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion System-level software The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. hypothesis 4.44.2, 5.41.2, 5.41.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. iccifort 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Compilers Intel C, C++ & Fortran compilers iccifortcuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel C, C++ & Fortran compilers with CUDA toolkit iimpi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. imkl 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Numerical libraries Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. impi 2018.5.288, 2019.9.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion MPI Intel MPI Library, compatible with MPICH ABI intel 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Toolchains (software stacks) Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2019b, 2020b 2019b, 2020b gpu iris Toolchains (software stacks) Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI & Intel MKL, with CUDA toolkit intltool 0.51.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. itac 2019.4.036 2019b broadwell, skylake iris Utilities The Intel Trace Collector is a low-overhead tracing library that performs event-based tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. jemalloc 5.2.1 2019b broadwell, skylake iris Libraries jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. kallisto 0.46.1 2019b broadwell, skylake iris Biology kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. kim-api 2.1.3 2019b broadwell, skylake iris Chemistry Open Knowledgebase of Interatomic Models. KIM is an API and OpenKIM is a collection of interatomic models (potentials) for atomistic simulations. This is a library that can be used by simulation programs to get access to the models in the OpenKIM database. This EasyBuild only installs the API, the models can be installed with the package openkim-models, or the user can install them manually by running kim-api-collections-management install user MODELNAME or kim-api-collections-management install user OpenKIM to install them all. libGLU 9.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. libarchive 3.4.3 2020b broadwell, epyc, skylake, gpu aion, iris Utilities Multi-format archive and compression library libcerf 1.13, 1.14 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematics libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. libctl 4.0.0 2019b broadwell, skylake iris Chemistry libctl is a free Guile-based library implementing flexible control files for scientific simulations. libdrm 2.4.102, 2.4.99 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libraries Direct Rendering Manager runtime library. libepoxy 1.5.4 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.11, 2.1.12 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.2.1, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.2.5, 2.3.0 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.5.1, 1.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.2.0, 1.3.2 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Libraries libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 2019b, 2020b gpu iris Libraries Library to manipulate tensors on the GPU. libiconv 1.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.3, 2.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libmatheval 1.1.11 2019b broadwell, skylake iris Libraries GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libogg 1.3.4 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Ogg is a multimedia container format, and the native file and stream format for the Xiph.org multimedia codecs. libpciaccess 0.14, 0.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion System-level software Generic PCI access library. libpng 1.6.37 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries libpng is the official PNG reference library libreadline 8.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtirpc 1.3.1 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Libtirpc is a port of Suns Transport-Independent RPC library to Linux. libtool 2.4.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunistring 0.9.10 2019b broadwell, skylake iris Libraries This library provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard. libunwind 1.3.1, 1.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libvorbis 1.3.7 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressed audio format libwebp 1.1.0 2020b broadwell, epyc, skylake aion, iris Libraries WebP is a modern image format that provides superior lossless and lossy compression for images on the web. Using WebP, webmasters and web developers can create smaller, richer images that make the web faster. libxc 4.3.4, 5.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Chemistry Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. libxml2 2.9.10, 2.9.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libraries Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libxslt 1.1.34 2019b broadwell, skylake iris Libraries Libxslt is the XSLT C library developed for the GNOME project (but usable outside of the Gnome platform). libyaml 0.2.2, 0.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries LibYAML is a YAML parser and emitter written in C. lxml 4.4.2 2019b broadwell, skylake iris Libraries The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. lz4 1.9.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core. It features an extremely fast decoder, with speed in multiple GB/s per core. magma 2.5.1, 2.5.4 2019b, 2020b gpu iris Mathematics The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. makeinfo 6.7 2020b broadwell, epyc, skylake, gpu aion, iris Development makeinfo is part of the Texinfo project, the official documentation format of the GNU project. matplotlib 3.1.1, 3.3.3 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Visualisation matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. molmod 1.4.5 2019b broadwell, skylake iris Mathematics MolMod is a Python library with many compoments that are useful to write molecular modeling programs. ncurses 6.0, 6.1, 6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. netCDF-Fortran 4.5.2, 4.5.3 2019b, 2020b broadwell, skylake, epyc iris, aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.1, 4.7.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Data processing NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. nettle 3.5.1, 3.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. networkx 2.5 2020b broadwell, epyc, skylake, gpu aion, iris Utilities NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. nodejs 12.19.0 2020b broadwell, epyc, skylake, gpu aion, iris Programming Languages Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. nsync 1.24.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development nsync is a C library that exports various synchronization primitives, such as mutexes numactl 2.0.12, 2.0.13 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. numba 0.52.0 2020b broadwell, epyc, skylake, gpu aion, iris Programming Languages Numba is an Open Source NumPy-aware optimizing compiler for Python sponsored by Continuum Analytics, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code. phonopy 2.2.0 2019b broadwell, skylake iris Libraries Phonopy is an open source package of phonon calculations based on the supercell approach. pixman 0.38.4, 0.40.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. pkg-config 0.29.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c pkg-config --libs --cflags glib-2.0 for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development pkgconfig is a Python module to interface with the pkg-config command line tool pocl 1.4, 1.6 2019b, 2020b gpu iris Libraries Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard protobuf-python 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development Python Protocol Buffers runtime library. protobuf 2.5.0, 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Development Google Protocol Buffers pybind11 2.4.3, 2.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. re2c 1.2.1, 2.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Utilities re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. scikit-build 0.11.1 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Scikit-Build, or skbuild, is an improved build system generator for CPython C/C++/Fortran/Cython extensions. scikit-image 0.18.1 2020b broadwell, epyc, skylake, gpu aion, iris Visualisation scikit-image is a collection of algorithms for image processing. scikit-learn 0.23.2 2020b broadwell, epyc, skylake, gpu aion, iris Data processing Scikit-learn integrates machine learning algorithms in the tightly-knit scientific Python world, building upon numpy, scipy, and matplotlib. As a machine-learning module, it provides versatile tools for data mining and analysis in any field of science and engineering. It strives to be simple and efficient, accessible to everybody, and reusable in various contexts. scipy 1.4.1 2019b broadwell, skylake, gpu iris Mathematics SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension for Python. setuptools 41.0.1 2019b broadwell, skylake iris Development Easily download, build, install, upgrade, and uninstall Python packages snappy 1.1.7, 1.1.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. sparsehash 2.0.3, 2.0.4 2019b, 2020b broadwell, skylake, epyc iris, aion Development An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. spglib-python 1.16.0 2020b broadwell, epyc, skylake, gpu aion, iris Chemistry Spglib for Python. Spglib is a library for finding and handling crystal symmetries written in C. tbb 2019_U9, 2020.2, 2020.3 2019b, 2020b broadwell, skylake, epyc iris, aion Libraries Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. texinfo 6.7 2019b broadwell, skylake iris Development Texinfo is the official documentation format of the GNU project. tqdm 4.56.2 2020b broadwell, epyc, skylake, gpu aion, iris Libraries A fast, extensible progress bar for Python and CLI typing-extensions 3.7.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Development Typing Extensions \u2013 Backported and Experimental Type Hints for Python util-linux 2.34, 2.36 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Utilities Set of Linux utilities x264 20190925, 20201026 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.2, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Visualisation x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xorg-macros 1.19.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Development X.org macros utilities. xprop 1.2.4, 1.2.5 2019b, 2020b broadwell, skylake, epyc iris, aion Visualisation The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information. yaff 1.6.0 2019b broadwell, skylake iris Chemistry Yaff stands for 'Yet another force field'. It is a pythonic force-field code. zlib 1.2.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libraries zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd 1.4.5 2020b broadwell, epyc, skylake, gpu aion, iris Libraries Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set.","title":"Full List (alphabetical order)"},{"location":"software/swsets/bio/","text":"Alphabetical list of available ULHPC software belonging to the 'bio' category. To load a software of this category, use: module load bio/<software>[/<version>] Software Versions Swsets Architectures Clusters Description ABySS 2.2.5 2020b broadwell, epyc, skylake aion, iris Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler BEDTools 2.29.2, 2.30.0 2019b, 2020b broadwell, skylake, epyc iris, aion BEDTools: a powerful toolset for genome arithmetic. The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps and computing coverage. The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM. BLAST+ 2.11.0, 2.9.0 2020b, 2019b broadwell, epyc, skylake aion, iris Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. BWA 0.7.17 2019b, 2020b broadwell, skylake, epyc iris, aion Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome. BamTools 2.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. BioPerl 1.7.2, 1.7.8 2019b, 2020b broadwell, skylake, epyc iris, aion Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects. Bowtie2 2.3.5.1, 2.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. FastQC 0.11.9 2019b, 2020b broadwell, skylake, epyc iris, aion FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline. GROMACS 2019.4, 2019.6, 2020, 2021, 2021.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a CPU only build, containing both MPI and threadMPI builds for both single and double precision. It also contains the gmxapi extension for the single precision MPI build. HTSlib 1.10.2, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix Jellyfish 2.3.0 2019b broadwell, skylake iris Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA. SAMtools 1.10, 1.12 2019b, 2020b broadwell, skylake, epyc iris, aion SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format. Salmon 1.1.0 2019b broadwell, skylake iris Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data. TopHat 2.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion TopHat is a fast splice junction mapper for RNA-Seq reads. Trinity 2.10.0 2019b broadwell, skylake iris Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-Seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-Seq reads. kallisto 0.46.1 2019b broadwell, skylake iris kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads.","title":"Biology"},{"location":"software/swsets/cae/","text":"Alphabetical list of available ULHPC software belonging to the 'cae' category. To load a software of this category, use: module load cae/<software>[/<version>] Software Versions Swsets Architectures Clusters Description ABAQUS 2018, 2021 2019b, 2020b broadwell, skylake, epyc iris, aion Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA. OpenFOAM-Extend 4.1-20200408 2019b broadwell, skylake iris OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. OpenFOAM 8, v1912 2020b, 2019b epyc, broadwell, skylake aion, iris OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics. Salome 8.5.0, 9.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion The SALOME platform is an open source software framework for pre- and post-processing and integration of numerical solvers from various scientific fields. CEA and EDF use SALOME to perform a large number of simulations, typically related to power plant equipment and alternative energy. To address these challenges, SALOME includes a CAD/CAE modelling tool, mesh generators, an advanced 3D visualization tool, etc. gmsh 4.4.0 2019b broadwell, skylake iris Salome is an open-source software that provides a generic Pre- and Post-Processing platform for numerical simulation. It is based on an open and flexible architecture made of reusable components.","title":"CFD/Finite element modelling"},{"location":"software/swsets/chem/","text":"Alphabetical list of available ULHPC software belonging to the 'chem' category. To load a software of this category, use: module load chem/<software>[/<version>] Software Versions Swsets Architectures Clusters Description ABINIT 9.4.1 2020b epyc aion ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ASE 3.19.0, 3.20.1, 3.21.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language. From version 3.20.1 we also include the ase-ext package, it contains optional reimplementations in C of functions in ASE. ASE uses it automatically when installed. CRYSTAL 17 2019b broadwell, skylake iris The CRYSTAL package performs ab initio calculations of the ground state energy, energy gradient, electronic wave function and properties of periodic systems. Hartree-Fock or Kohn- Sham Hamiltonians (that adopt an Exchange-Correlation potential following the postulates of Density-Functional Theory) can be used. GPAW-setups 0.9.20000 2019b broadwell, skylake iris PAW setup for the GPAW Density Functional Theory package. Users can install setups manually using 'gpaw install-data' or use setups from this package. The versions of GPAW and GPAW-setups can be intermixed. GPAW 20.1.0 2019b broadwell, skylake iris GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). It uses real-space uniform grids and multigrid methods or atom-centered basis-functions. NAMD 2.13 2019b broadwell, skylake iris NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. PLUMED 2.5.3, 2.7.0 2019b, 2020b broadwell, skylake, epyc iris, aion PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes. QuantumESPRESSO 6.7 2019b, 2020b broadwell, epyc, skylake iris, aion Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft). Wannier90 3.1.0 2020b broadwell, epyc, skylake aion, iris A tool for obtaining maximally-localised Wannier functions kim-api 2.1.3 2019b broadwell, skylake iris Open Knowledgebase of Interatomic Models. KIM is an API and OpenKIM is a collection of interatomic models (potentials) for atomistic simulations. This is a library that can be used by simulation programs to get access to the models in the OpenKIM database. This EasyBuild only installs the API, the models can be installed with the package openkim-models, or the user can install them manually by running kim-api-collections-management install user MODELNAME or kim-api-collections-management install user OpenKIM to install them all. libctl 4.0.0 2019b broadwell, skylake iris libctl is a free Guile-based library implementing flexible control files for scientific simulations. libxc 4.3.4, 5.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals. spglib-python 1.16.0 2020b broadwell, epyc, skylake, gpu aion, iris Spglib for Python. Spglib is a library for finding and handling crystal symmetries written in C. yaff 1.6.0 2019b broadwell, skylake iris Yaff stands for 'Yet another force field'. It is a pythonic force-field code.","title":"Chemistry"},{"location":"software/swsets/compiler/","text":"Alphabetical list of available ULHPC software belonging to the 'compiler' category. To load a software of this category, use: module load compiler/<software>[/<version>] Software Versions Swsets Architectures Clusters Description AOCC 3.1.0 2020b epyc aion AMD Optimized C/C++ & Fortran compilers (AOCC) based on LLVM 12.0 Clang 11.0.1, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris C, C++, Objective-C compiler, based on LLVM. Does not include C++ standard library -- use libstdc++ from GCC. GCC 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). GCCcore 10.2.0, 8.3.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). Go 1.14.1, 1.16.6 2019b, 2020b broadwell, skylake, epyc iris, aion Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. LLVM 10.0.1, 11.0.0, 9.0.0, 9.0.1 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator. PGI 19.10 2019b broadwell, skylake iris C, C++ and Fortran compilers from The Portland Group - PGI iccifort 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Intel C, C++ & Fortran compilers","title":"Compilers"},{"location":"software/swsets/data/","text":"Alphabetical list of available ULHPC software belonging to the 'data' category. To load a software of this category, use: module load data/<software>[/<version>] Software Versions Swsets Architectures Clusters Description Arrow 0.16.0 2019b broadwell, skylake iris Apache Arrow (incl. PyArrow Python bindings)), a cross-language development platform for in-memory data. DB_File 1.855 2020b broadwell, epyc, skylake aion, iris Perl5 access to Berkeley DB version 1.x. GDAL 3.0.2, 3.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing. HDF5 1.10.5, 1.10.7 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF 4.2.15 2020b broadwell, epyc, skylake, gpu aion, iris HDF (also known as HDF4) is a library and multi-object file format for storing and managing data between machines. LAME 3.100 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL. XML-LibXML 2.0201, 2.0206 2019b, 2020b broadwell, skylake, epyc iris, aion Perl binding for libxml2 dask 2021.2.0 2020b broadwell, epyc, skylake, gpu aion, iris Dask natively scales Python. Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. h5py 2.10.0, 3.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data. netCDF-Fortran 4.5.2, 4.5.3 2019b, 2020b broadwell, skylake, epyc iris, aion NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. netCDF 4.7.1, 4.7.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. scikit-learn 0.23.2 2020b broadwell, epyc, skylake, gpu aion, iris Scikit-learn integrates machine learning algorithms in the tightly-knit scientific Python world, building upon numpy, scipy, and matplotlib. As a machine-learning module, it provides versatile tools for data mining and analysis in any field of science and engineering. It strives to be simple and efficient, accessible to everybody, and reusable in various contexts.","title":"Data processing"},{"location":"software/swsets/debugger/","text":"Alphabetical list of available ULHPC software belonging to the 'debugger' category. To load a software of this category, use: module load debugger/<software>[/<version>] Software Versions Swsets Architectures Clusters Description GDB 10.1, 9.1 2020b, 2019b broadwell, epyc, skylake aion, iris The GNU Project Debugger Valgrind 3.15.0, 3.16.1 2019b, 2020b broadwell, skylake, epyc iris, aion Valgrind: Debugging and profiling tools","title":"Debugging"},{"location":"software/swsets/devel/","text":"Alphabetical list of available ULHPC software belonging to the 'devel' category. To load a software of this category, use: module load devel/<software>[/<version>] Software Versions Swsets Architectures Clusters Description Autoconf 2.69 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls. Automake 1.16.1, 1.16.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Automake: GNU Standards-compliant Makefile generator Autotools 20180311, 20200321 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion This bundle collect the standard GNU build tools: Autoconf, Automake and libtool Bazel 0.26.1, 0.29.1, 3.7.2 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion Bazel is a build tool that builds code quickly and reliably. It is used to build the majority of Google's software. Boost 1.71.0, 1.74.0 2019b, 2020b broadwell, skylake, epyc iris, aion Boost provides free peer-reviewed portable C++ source libraries. CMake 3.15.3, 3.18.4, 3.20.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion CMake, the cross-platform, open-source build system. CMake is a family of tools designed to build, test and package software. DBus 1.13.12, 1.13.18 2019b, 2020b broadwell, skylake, epyc iris, aion D-Bus is a message bus system, a simple way for applications to talk to one another. In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed. Doxygen 1.8.16, 1.8.20 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D. Flink 1.11.2 2020b broadwell, epyc, skylake aion, iris Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. GObject-Introspection 1.63.1, 1.66.1 2019b, 2020b broadwell, skylake, epyc iris, aion GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library. M4 1.4.18 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc. Mako 1.1.0, 1.1.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion A super-fast templating language that borrows the best ideas from the existing templating languages Maven 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion Binary maven install, Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. PCRE2 10.33, 10.35 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PCRE 8.43, 8.44 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5. PyTorch 1.4.0, 1.7.1, 1.8.1, 1.9.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework that puts Python first. Qt5 5.13.1, 5.14.2 2019b, 2020b broadwell, skylake, epyc iris, aion Qt is a comprehensive cross-platform C++ application framework. ReFrame 2.21, 3.6.3 2019b, 2020b broadwell, skylake, epyc iris, aion ReFrame is a framework for writing regression tests for HPC systems. SQLite 3.29.0, 3.33.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion SQLite: SQL Database Engine in a C Library SWIG 4.0.1, 4.0.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages. Spack 0.12.1 2019b, 2020b broadwell, skylake, epyc iris, aion Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine. Spark 2.4.3 2019b broadwell, skylake iris Spark is Hadoop MapReduce done in memory ant 1.10.6, 1.10.7, 1.10.9 2019b, 2020b broadwell, skylake, epyc iris, aion Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. flatbuffers-python 1.12 2020b broadwell, epyc, skylake, gpu aion, iris Python Flatbuffers runtime library. flatbuffers 1.12.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion FlatBuffers: Memory Efficient Serialization Library gflags 2.2.2 2019b broadwell, skylake iris The gflags package contains a C++ library that implements commandline flags processing. It includes built-in support for standard types such as string and the ability to define flags in the source file in which they are used. glog 0.4.0 2019b broadwell, skylake iris A C++ implementation of the Google logging module. googletest 1.10.0 2019b broadwell, skylake iris Google's framework for writing C++ tests on a variety of platforms gperf 3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only. intltool 0.51.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. makeinfo 6.7 2020b broadwell, epyc, skylake, gpu aion, iris makeinfo is part of the Texinfo project, the official documentation format of the GNU project. ncurses 6.0, 6.1, 6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses. nsync 1.24.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion nsync is a C library that exports various synchronization primitives, such as mutexes pkg-config 0.29.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c pkg-config --libs --cflags glib-2.0 for instance, rather than hard-coding values on where to find glib (or other libraries). pkgconfig 1.5.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion pkgconfig is a Python module to interface with the pkg-config command line tool protobuf-python 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Python Protocol Buffers runtime library. protobuf 2.5.0, 3.10.0, 3.14.0 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Google Protocol Buffers setuptools 41.0.1 2019b broadwell, skylake iris Easily download, build, install, upgrade, and uninstall Python packages sparsehash 2.0.3, 2.0.4 2019b, 2020b broadwell, skylake, epyc iris, aion An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed. texinfo 6.7 2019b broadwell, skylake iris Texinfo is the official documentation format of the GNU project. typing-extensions 3.7.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Typing Extensions \u2013 Backported and Experimental Type Hints for Python xorg-macros 1.19.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion X.org macros utilities.","title":"Development"},{"location":"software/swsets/lang/","text":"Alphabetical list of available ULHPC software belonging to the 'lang' category. To load a software of this category, use: module load lang/<software>[/<version>] Software Versions Swsets Architectures Clusters Description Anaconda3 2020.02, 2020.11 2019b, 2020b broadwell, skylake, epyc iris, aion Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture. Bison 3.3.2, 3.5.3, 3.7.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables. FriBidi 1.0.10, 1.0.5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris The Free Implementation of the Unicode Bidirectional Algorithm. Guile 1.8.8, 2.2.4 2019b broadwell, skylake iris Guile is a programming language, designed to help programmers create flexible applications that can be extended by users or other programmers with plug-ins, modules, or scripts. Java 1.8.0_241, 11.0.2, 13.0.2, 16.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Java Platform, Standard Edition (Java SE) lets you develop and deploy Java applications on desktops and servers. Julia 1.4.1, 1.6.2 2019b, 2020b broadwell, skylake, epyc iris, aion Julia is a high-level, high-performance dynamic programming language for numerical computing Lua 5.1.5, 5.4.2 2019b, 2020b broadwell, skylake, epyc iris, aion Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. NASM 2.14.02, 2.15.05 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion NASM: General-purpose x86 assembler Perl 5.30.0, 5.32.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Larry Wall's Practical Extraction and Report Language This is a minimal build without any modules. Should only be used for build dependencies. Python 2.7.16, 2.7.18, 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Python is a programming language that lets you work more quickly and integrate your systems more effectively. R 3.6.2, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion R is a free software environment for statistical computing and graphics. Ruby 2.7.1, 2.7.2 2019b, 2020b broadwell, skylake, epyc iris, aion Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write. Rust 1.37.0 2019b broadwell, skylake iris Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. SciPy-bundle 2019.10, 2020.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Bundle of Python packages for scientific software Tcl 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. Tkinter 3.7.4, 3.8.6 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Tkinter module, built with the Python buildsystem Yasm 1.3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Yasm: Complete rewrite of the NASM assembler with BSD license flex 2.6.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text. nodejs 12.19.0 2020b broadwell, epyc, skylake, gpu aion, iris Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. numba 0.52.0 2020b broadwell, epyc, skylake, gpu aion, iris Numba is an Open Source NumPy-aware optimizing compiler for Python sponsored by Continuum Analytics, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code.","title":"Programming Languages"},{"location":"software/swsets/lib/","text":"Alphabetical list of available ULHPC software belonging to the 'lib' category. To load a software of this category, use: module load lib/<software>[/<version>] Software Versions Swsets Architectures Clusters Description ACTC 1.1 2019b, 2020b broadwell, skylake, epyc iris, aion ACTC converts independent triangles into triangle strips or fans. Boost.Python 1.74.0 2020b broadwell, epyc, skylake aion, iris Boost.Python is a C++ library which enables seamless interoperability between C++ and the Python programming language. Check 0.15.2 2020b gpu iris Check is a unit testing framework for C. It features a simple interface for defining unit tests, putting little in the way of the developer. Tests are run in a separate address space, so both assertion failures and code errors that cause segmentation faults or other signals can be caught. Test results are reportable in the following: Subunit, TAP, XML, and a generic logging format. FLAC 1.3.3 2020b broadwell, epyc, skylake, gpu aion, iris FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. Flask 1.1.2 2020b broadwell, epyc, skylake, gpu aion, iris Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. This module includes the Flask extensions: Flask-Cors GDRCopy 2.1 2020b gpu iris A low-latency GPU memory copy library based on NVIDIA GPUDirect RDMA technology. ICU 64.2, 67.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. JsonCpp 1.9.3, 1.9.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files. LMDB 0.9.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases. LibTIFF 4.0.10, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion tiff: Library and tools for reading and writing TIFF data files NCCL 2.4.8, 2.8.3 2019b, 2020b gpu iris The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance optimized for NVIDIA GPUs. NSPR 4.21, 4.29 2019b, 2020b broadwell, skylake, epyc iris, aion Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions. NSS 3.45, 3.57 2019b, 2020b broadwell, skylake, epyc iris, aion Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications. PROJ 6.2.1, 7.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates PyTorch-Geometric 1.6.3 2020b broadwell, epyc, skylake, gpu aion, iris PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch. PyYAML 5.1.2, 5.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion PyYAML is a YAML parser and emitter for the Python programming language. RDFlib 5.0.0 2020b broadwell, epyc, skylake, gpu aion, iris RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information. SDL2 2.0.14 2020b broadwell, epyc, skylake aion, iris SDL: Simple DirectMedia Layer, a cross-platform multimedia library SIONlib 1.7.6 2019b broadwell, skylake iris SIONlib is a scalable I/O library for parallel access to task-local files. The library not only supports writing and reading binary data to or from several thousands of processors into a single or a small number of physical files, but also provides global open and close functions to access SIONlib files in parallel. This package provides a stripped-down installation of SIONlib for use with performance tools (e.g., Score-P), with renamed symbols to avoid conflicts when an application using SIONlib itself is linked against a tool requiring a different SIONlib version. TensorFlow 1.15.5, 2.1.0, 2.4.1, 2.5.0 2019b, 2020b gpu, broadwell, skylake, epyc iris, aion An open-source software library for Machine Intelligence UCX 1.9.0 2020b broadwell, epyc, skylake, gpu aion, iris Unified Communication X An open-source production grade communication framework for data centric and high-performance applications Xerces-C++ 3.2.2 2019b broadwell, skylake iris Xerces-C++ is a validating XML parser written in a portable subset of C++. Xerces-C++ makes it easy to give your application the ability to read and write XML data. A shared library is provided for parsing, generating, manipulating, and validating XML documents using the DOM, SAX, and SAX2 APIs. YACS 0.1.8 2020b broadwell, epyc, skylake aion, iris YACS was created as a lightweight library to define and manage system configurations, such as those commonly found in software designed for scientific experimentation. These \"configurations\" typically cover concepts like hyperparameters used in training a machine learning model or configurable model hyperparameters, such as the depth of a convolutional neural network. double-conversion 3.1.4, 3.1.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles. elfutils 0.183 2020b gpu iris The elfutils project provides libraries and tools for ELF files and DWARF data. gc 7.6.12 2019b broadwell, skylake iris The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new. giflib 5.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion giflib is a library for reading and writing gif images. It is API and ABI compatible with libungif which was in wide use while the LZW compression algorithm was patented. jemalloc 5.2.1 2019b broadwell, skylake iris jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. libdrm 2.4.102, 2.4.99 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Direct Rendering Manager runtime library. libepoxy 1.5.4 2019b, 2020b broadwell, skylake, epyc iris, aion Epoxy is a library for handling OpenGL function pointer management for you libevent 2.1.11, 2.1.12 2019b, 2020b broadwell, skylake, epyc iris, aion The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Furthermore, libevent also support callbacks due to signals or regular timeouts. libffi 3.2.1, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time. libgd 2.2.5, 2.3.0 2019b, 2020b broadwell, skylake, epyc iris, aion GD is an open source code library for the dynamic creation of images by programmers. libgeotiff 1.5.1, 1.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Library for reading and writing coordinate system information from/to GeoTIFF files libglvnd 1.2.0, 1.3.2 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors. libgpuarray 0.7.6 2019b, 2020b gpu iris Library to manipulate tensors on the GPU. libiconv 1.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libiconv converts from one character encoding to another through Unicode conversion libjpeg-turbo 2.0.3, 2.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding. libmatheval 1.1.11 2019b broadwell, skylake iris GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. libogg 1.3.4 2020b broadwell, epyc, skylake, gpu aion, iris Ogg is a multimedia container format, and the native file and stream format for the Xiph.org multimedia codecs. libpng 1.6.37 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion libpng is the official PNG reference library libreadline 8.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands. libsndfile 1.0.28 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface. libtirpc 1.3.1 2020b broadwell, epyc, skylake, gpu aion, iris Libtirpc is a port of Suns Transport-Independent RPC library to Linux. libtool 2.4.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. libunistring 0.9.10 2019b broadwell, skylake iris This library provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard. libunwind 1.3.1, 1.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications libvorbis 1.3.7 2020b broadwell, epyc, skylake, gpu aion, iris Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressed audio format libwebp 1.1.0 2020b broadwell, epyc, skylake aion, iris WebP is a modern image format that provides superior lossless and lossy compression for images on the web. Using WebP, webmasters and web developers can create smaller, richer images that make the web faster. libxml2 2.9.10, 2.9.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform). libxslt 1.1.34 2019b broadwell, skylake iris Libxslt is the XSLT C library developed for the GNOME project (but usable outside of the Gnome platform). libyaml 0.2.2, 0.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion LibYAML is a YAML parser and emitter written in C. lxml 4.4.2 2019b broadwell, skylake iris The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. lz4 1.9.2 2020b broadwell, epyc, skylake, gpu aion, iris LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core. It features an extremely fast decoder, with speed in multiple GB/s per core. nettle 3.5.1, 3.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space. phonopy 2.2.0 2019b broadwell, skylake iris Phonopy is an open source package of phonon calculations based on the supercell approach. pocl 1.4, 1.6 2019b, 2020b gpu iris Pocl is a portable open source (MIT-licensed) implementation of the OpenCL standard pybind11 2.4.3, 2.6.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. scikit-build 0.11.1 2020b broadwell, epyc, skylake, gpu aion, iris Scikit-Build, or skbuild, is an improved build system generator for CPython C/C++/Fortran/Cython extensions. snappy 1.1.7, 1.1.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. tbb 2019_U9, 2020.2, 2020.3 2019b, 2020b broadwell, skylake, epyc iris, aion Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability. tqdm 4.56.2 2020b broadwell, epyc, skylake, gpu aion, iris A fast, extensible progress bar for Python and CLI zlib 1.2.11 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. zstd 1.4.5 2020b broadwell, epyc, skylake, gpu aion, iris Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set.","title":"Libraries"},{"location":"software/swsets/math/","text":"Alphabetical list of available ULHPC software belonging to the 'math' category. To load a software of this category, use: module load math/<software>[/<version>] Software Versions Swsets Architectures Clusters Description CPLEX 12.10 2019b broadwell, skylake iris IBM ILOG CPLEX Optimizer's mathematical programming technology enables analytical decision support for improving efficiency, reducing costs, and increasing profitability. Dakota 6.11.0, 6.15.0 2019b, 2020b broadwell, skylake iris The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models.\" ELPA 2019.11.001, 2020.11.001 2019b, 2020b broadwell, epyc, skylake iris, aion Eigenvalue SoLvers for Petaflop-Applications . Eigen 3.3.7, 3.3.8, 3.4.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. GEOS 3.8.0, 3.9.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) GMP 6.1.2, 6.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. Gurobi 9.0.0, 9.1.2 2019b, 2020b broadwell, skylake, epyc iris, aion The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multi-core processors, using the most advanced implementations of the latest algorithms. Harminv 1.4.1 2019b broadwell, skylake iris Harminv is a free program (and accompanying library) to solve the problem of harmonic inversion - given a discrete-time, finite-length signal that consists of a sum of finitely-many sinusoids (possibly exponentially decaying) in a given bandwidth, it determines the frequencies, decay constants, amplitudes, and phases of those sinusoids. ISL 0.23 2020b broadwell, epyc, skylake aion, iris isl is a library for manipulating sets and relations of integer points bounded by linear constraints. Keras 2.3.1, 2.4.3 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. MATLAB 2019b, 2020a, 2021a 2019b, 2020b broadwell, skylake, epyc iris, aion MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran. METIS 5.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes. MPC 1.2.1 2020b broadwell, epyc, skylake aion, iris Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. It extends the principles of the IEEE-754 standard for fixed precision real floating point numbers to complex numbers, providing well-defined semantics for every operation. At the same time, speed of operation at high precision is a major design goal. MPFR 4.0.2, 4.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The MPFR library is a C library for multiple-precision floating-point computations with correct rounding. MUMPS 5.3.5 2020b broadwell, epyc, skylake aion, iris A parallel sparse direct solver Mathematica 12.0.0, 12.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields. Mesquite 2.3.0 2019b broadwell, skylake iris Mesh-Quality Improvement Library ParMETIS 4.0.3 2019b broadwell, skylake iris ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes. ParMGridGen 1.0 2019b broadwell, skylake iris ParMGridGen is an MPI-based parallel library that is based on the serial package MGridGen, that implements (serial) algorithms for obtaining a sequence of successive coarse grids that are well-suited for geometric multigrid methods. SCOTCH 6.0.9, 6.1.0 2019b, 2020b broadwell, skylake, epyc iris, aion Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning. Stata 17 2020b broadwell, epyc, skylake aion, iris Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics. Theano 1.0.4, 1.1.2 2019b, 2020b gpu, broadwell, epyc, skylake iris, aion Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Voro++ 0.4.6 2019b broadwell, skylake iris Voro++ is a software library for carrying out three-dimensional computations of the Voronoi tessellation. A distinguishing feature of the Voro++ library is that it carries out cell-based calculations, computing the Voronoi cell for each particle individually. It is particularly well-suited for applications that rely on cell-based statistics, where features of Voronoi cells (eg. volume, centroid, number of faces) can be used to analyze a system of particles. gmsh 4.8.4 2020b broadwell, epyc, skylake aion, iris Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor. libcerf 1.13, 1.14 2019b, 2020b broadwell, skylake, epyc iris, aion libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions. magma 2.5.1, 2.5.4 2019b, 2020b gpu iris The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems. molmod 1.4.5 2019b broadwell, skylake iris MolMod is a Python library with many compoments that are useful to write molecular modeling programs. scipy 1.4.1 2019b broadwell, skylake, gpu iris SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension for Python.","title":"Mathematics"},{"location":"software/swsets/mpi/","text":"Alphabetical list of available ULHPC software belonging to the 'mpi' category. To load a software of this category, use: module load mpi/<software>[/<version>] Software Versions Swsets Architectures Clusters Description OpenMPI 3.1.4, 4.0.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The Open MPI Project is an open source MPI-3 implementation. impi 2018.5.288, 2019.9.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Intel MPI Library, compatible with MPICH ABI","title":"MPI"},{"location":"software/swsets/numlib/","text":"Alphabetical list of available ULHPC software belonging to the 'numlib' category. To load a software of this category, use: module load numlib/<software>[/<version>] Software Versions Swsets Architectures Clusters Description Armadillo 10.5.3, 9.900.1 2020b, 2019b broadwell, epyc, skylake aion, iris Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. CGAL 4.14.1, 5.2 2019b, 2020b broadwell, skylake, epyc iris, aion The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library. FFTW 3.3.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data. GSL 2.6 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. Hypre 2.20.0 2020b broadwell, epyc, skylake aion, iris Hypre is a library for solving large, sparse linear systems of equations on massively parallel computers. The problems of interest arise in the simulation codes being developed at LLNL and elsewhere to study physical phenomena in the defense, environmental, energy, and biological sciences. NLopt 2.6.1, 2.6.2 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. OpenBLAS 0.3.12, 0.3.7 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. PETSc 3.14.4 2020b broadwell, epyc, skylake aion, iris PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. SLEPc 3.14.2 2020b broadwell, epyc, skylake aion, iris SLEPc (Scalable Library for Eigenvalue Problem Computations) is a software library for the solution of large scale sparse eigenvalue problems on parallel computers. It is an extension of PETSc and can be used for either standard or generalized eigenproblems, with real or complex arithmetic. It can also be used for computing a partial SVD of a large, sparse, rectangular matrix, and to solve quadratic eigenvalue problems. ScaLAPACK 2.0.2, 2.1.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. SuiteSparse 5.8.1 2020b broadwell, epyc, skylake aion, iris SuiteSparse is a collection of libraries manipulate sparse matrices. arpack-ng 3.7.0, 3.8.0 2019b, 2020b broadwell, skylake, epyc iris, aion ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. cuDNN 7.6.4.38, 8.0.4.30, 8.0.5.39 2019b, 2020b gpu iris The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. imkl 2019.5.281, 2020.4.304 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Intel Math Kernel Library is a library of highly optimized, extensively threaded math routines for science, engineering, and financial applications that require maximum performance. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more.","title":"Numerical libraries"},{"location":"software/swsets/perf/","text":"Alphabetical list of available ULHPC software belonging to the 'perf' category. To load a software of this category, use: module load perf/<software>[/<version>] Software Versions Swsets Architectures Clusters Description Advisor 2019_update5 2019b broadwell, skylake iris Vectorization Optimization and Thread Prototyping - Vectorize & thread code or performance \u201cdies\u201d - Easy workflow + data + tips = faster code faster - Prioritize, Prototype & Predict performance gain CubeGUI 4.4.4 2019b broadwell, skylake iris Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube graphical report explorer. CubeLib 4.4.4 2019b broadwell, skylake iris Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube general purpose C++ library component and command-line tools. CubeWriter 4.4.3 2019b broadwell, skylake iris Cube, which is used as performance report explorer for Scalasca and Score-P, is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. Each dimension can be represented as a tree, where non-leaf nodes of the tree can be collapsed or expanded to achieve the desired level of granularity. This module provides the Cube high-performance C writer library component. OPARI2 2.0.5 2019b broadwell, skylake iris OPARI2, the successor of Forschungszentrum Juelich's OPARI, is a source-to-source instrumentation tool for OpenMP and hybrid codes. It surrounds OpenMP directives and runtime library calls with calls to the POMP2 measurement interface. OTF2 2.2 2019b broadwell, skylake iris The Open Trace Format 2 is a highly scalable, memory efficient event trace data format plus support library. It is the new standard trace format for Scalasca, Vampir, and TAU and is open for other tools. PAPI 6.0.0 2019b, 2020b broadwell, skylake, epyc iris, aion PAPI provides the tool designer and application engineer with a consistent interface and methodology for use of the performance counter hardware found in most major microprocessors. PAPI enables software engineers to see, in near real time, the relation between software performance and processor events. In addition Component PAPI provides access to a collection of components that expose performance measurement opportunites across the hardware and software stack. PDT 3.25 2019b broadwell, skylake iris Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program knowledge accessible to developers of static and dynamic analysis tools. PDT implements a standard program representation, the program database (PDB), that can be accessed in a uniform way through a class library supporting common PDB operations. Scalasca 2.5 2019b broadwell, skylake iris Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks -- in particular those concerning communication and synchronization -- and offers guidance in exploring their causes. Score-P 6.0 2019b broadwell, skylake iris The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications.","title":"Performance measurements"},{"location":"software/swsets/phys/","text":"Alphabetical list of available ULHPC software belonging to the 'phys' category. To load a software of this category, use: module load phys/<software>[/<version>] Software Versions Swsets Architectures Clusters Description Elk 6.3.2, 7.0.12 2019b, 2020b broadwell, skylake, epyc iris, aion An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universit\u00e4t Graz as a milestone of the EXCITING EU Research and Training Network, the code is designed to be as simple as possible so that new developments in the field of density functional theory (DFT) can be added quickly and reliably. FDS 6.7.1, 6.7.6 2019b, 2020b broadwell, skylake, epyc iris, aion Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows, with an emphasis on smoke and heat transport from fires. Meep 1.4.3 2019b broadwell, skylake iris Meep (or MEEP) is a free finite-difference time-domain (FDTD) simulation software package developed at MIT to model electromagnetic systems. UDUNITS 2.2.26 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement. VASP 5.4.4, 6.2.1 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.","title":"Physics"},{"location":"software/swsets/system/","text":"Alphabetical list of available ULHPC software belonging to the 'system' category. To load a software of this category, use: module load system/<software>[/<version>] Software Versions Swsets Architectures Clusters Description CUDA 10.1.243, 11.1.1 2019b, 2020b gpu iris CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. CUDAcore 11.1.1 2020b gpu iris CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs. ULHPC-bd 2020b 2020b broadwell, epyc, skylake aion, iris Generic Module bundle for BigData Analytics software in use on the UL HPC Facility ULHPC-bio 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Generic Module bundle for Bioinformatics, biology and biomedical software in use on the UL HPC Facility, especially at LCSB ULHPC-cs 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Generic Module bundle for Computational science software in use on the UL HPC Facility, including: - Computer Aided Engineering, incl. CFD - Chemistry, Computational Chemistry and Quantum Chemistry - Data management & processing tools - Earth Sciences - Quantum Computing - Physics and physical systems simulations ULHPC-dl 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Generic Module bundle for (CPU-version) of AI / Deep Learning / Machine Learning software in use on the UL HPC Facility ULHPC-gpu 2019b, 2020b 2019b, 2020b gpu iris Generic Module bundle for GPU accelerated User Software in use on the UL HPC Facility ULHPC-math 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Generic Module bundle for High-level mathematical software and Linear Algrebra libraries in use on the UL HPC Facility ULHPC-toolchains 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Generic Module bundle that contains all the dependencies required to enable toolchains and building tools/programming language in use on the UL HPC Facility ULHPC-tools 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion Misc tools, incl. - perf: Performance tools - tools: General purpose tools hwloc 1.11.12, 2.2.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently. libpciaccess 0.14, 0.16 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Generic PCI access library.","title":"System-level software"},{"location":"software/swsets/toolchain/","text":"Alphabetical list of available ULHPC software belonging to the 'toolchain' category. To load a software of this category, use: module load toolchain/<software>[/<version>] Software Versions Swsets Architectures Clusters Description foss 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. fosscuda 2019b, 2020b 2019b, 2020b gpu iris GCC based compiler toolchain with CUDA support , and including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK. gcccuda 2019b, 2020b 2019b, 2020b gpu iris GNU Compiler Collection (GCC) based compiler toolchain, along with CUDA toolkit. gompi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc iris, aion GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support. gompic 2019b, 2020b 2019b, 2020b gpu iris GNU Compiler Collection (GCC) based compiler toolchain along with CUDA toolkit, including OpenMPI for MPI support with CUDA features enabled. iccifortcuda 2019b, 2020b 2019b, 2020b gpu iris Intel C, C++ & Fortran compilers with CUDA toolkit iimpi 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Intel C/C++ and Fortran compilers, alongside Intel MPI. iimpic 2019b, 2020b 2019b, 2020b gpu iris Intel C/C++ and Fortran compilers, alongside Intel MPI and CUDA. intel 2019b, 2020b 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL). intelcuda 2019b, 2020b 2019b, 2020b gpu iris Intel Cluster Toolkit Compiler Edition provides Intel C/C++ and Fortran compilers, Intel MPI & Intel MKL, with CUDA toolkit","title":"Toolchains (software stacks)"},{"location":"software/swsets/tools/","text":"Alphabetical list of available ULHPC software belonging to the 'tools' category. To load a software of this category, use: module load tools/<software>[/<version>] Software Versions Swsets Architectures Clusters Description ANSYS 19.4, 21.1 2019b, 2020b broadwell, skylake, epyc iris, aion ANSYS simulation software enables organizations to confidently predict how their products will operate in the real world. We believe that every product is a promise of something greater. ArmForge 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion The industry standard development package for C, C++ and Fortran high performance code on Linux. Forge is designed to handle the complex software projects - including parallel, multiprocess and multithreaded code. Arm Forge combines an industry-leading debugger, Arm DDT, and an out-of-the-box-ready profiler, Arm MAP. ArmReports 20.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion Arm Performance Reports - a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. Arm Performance Reports runs transparently on optimized production-ready codes by adding a single command to your scripts, and provides the most effective way to characterize and understand the performance of HPC application runs. Aspera-CLI 3.9.1, 3.9.6 2019b, 2020b broadwell, skylake, epyc iris, aion IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line. The Aspera CLI is for users and organizations who want to automate their transfer workflows. DB 18.1.32, 18.1.40 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects. DMTCP 2.5.2 2019b broadwell, skylake iris DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. EasyBuild 4.3.0, 4.3.3, 4.4.1, 4.4.2, 4.5.4 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way. GLPK 4.65 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library. Ghostscript 9.50, 9.53.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that. Hadoop 2.10.0 2020b broadwell, epyc, skylake aion, iris Hadoop MapReduce by Cloudera Horovod 0.19.1, 0.22.0 2019b, 2020b broadwell, skylake, gpu iris Horovod is a distributed training framework for TensorFlow. Inspector 2019_update5 2019b broadwell, skylake iris Intel Inspector XE is an easy to use memory error checker and thread checker for serial and parallel applications Meson 0.51.2, 0.55.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Meson is a cross-platform build system designed to be both as fast and as user friendly as possible. Ninja 1.10.1, 1.9.0 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Ninja is a small build system with a focus on speed. Singularity 3.6.0, 3.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion SingularityCE is an open source container platform designed to be simple, fast, and secure. Singularity is optimized for EPC and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way. Sumo 1.3.1 2019b broadwell, skylake iris Sumo is an open source, highly portable, microscopic and continuous traffic simulation package designed to handle large road networks. Szip 2.1.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Szip compression software, providing lossless compression of scientific data UnZip 6.0 2020b broadwell, epyc, skylake, gpu aion, iris UnZip is an extraction utility for archives compressed in .zip format (also called \"zipfiles\"). Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own Zip program, our primary objectives have been portability and non-MSDOS functionality. VTune 2019_update8, 2020_update3 2019b, 2020b broadwell, skylake, epyc iris, aion Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran, Assembly and Java. XZ 5.2.4, 5.2.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion xz: XZ utilities Z3 4.8.10 2020b broadwell, epyc, skylake, gpu aion, iris Z3 is a theorem prover from Microsoft Research. Zip 3.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Zip is a compression and file packaging/archive utility. Although highly compatible both with PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectives have been portability and other-than-MSDOS functionality archspec 0.1.0 2019b broadwell, skylake iris A library for detecting, labeling, and reasoning about microarchitectures binutils 2.32, 2.35 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion binutils: GNU binary utilities bokeh 2.2.3 2020b broadwell, epyc, skylake, gpu aion, iris Statistical and novel interactive HTML plots for Python bzip2 1.0.8 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. cURL 7.66.0, 7.72.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more. expat 2.2.7, 2.2.9 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Expat is an XML parser library written in C. It is a stream-oriented parser in which an application registers handlers for things the parser might find in the XML document (like start tags) gettext 0.19.8.1, 0.20.1, 0.21 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation git 2.23.0, 2.28.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. gocryptfs 1.7.1, 2.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion Encrypted overlay filesystem written in Go. gocryptfs uses file-based encryption that is implemented as a mountable FUSE filesystem. Each file in gocryptfs is stored as one corresponding encrypted file on the hard disk. groff 1.22.4 2020b broadwell, epyc, skylake, gpu aion, iris Groff (GNU troff) is a typesetting system that reads plain text mixed with formatting commands and produces formatted output. gzip 1.10 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion gzip (GNU zip) is a popular data compression program as a replacement for compress help2man 1.47.16, 1.47.4, 1.47.8 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris help2man produces simple manual pages from the '--help' and '--version' output of other commands. hypothesis 4.44.2, 5.41.2, 5.41.5 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work. itac 2019.4.036 2019b broadwell, skylake iris The Intel Trace Collector is a low-overhead tracing library that performs event-based tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. libarchive 3.4.3 2020b broadwell, epyc, skylake, gpu aion, iris Multi-format archive and compression library networkx 2.5 2020b broadwell, epyc, skylake, gpu aion, iris NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. numactl 2.0.12, 2.0.13 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program. re2c 1.2.1, 2.0.3 2019b, 2020b broadwell, skylake, epyc iris, aion re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. util-linux 2.34, 2.36 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Set of Linux utilities","title":"Utilities"},{"location":"software/swsets/vis/","text":"Alphabetical list of available ULHPC software belonging to the 'vis' category. To load a software of this category, use: module load vis/<software>[/<version>] Software Versions Swsets Architectures Clusters Description ATK 2.34.1, 2.36.0 2019b, 2020b broadwell, skylake, epyc iris, aion ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications. FFmpeg 4.2.1, 4.3.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion A complete, cross-platform solution to record, convert and stream audio and video. FLTK 1.3.5 2019b, 2020b broadwell, skylake, epyc iris, aion FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation. FreeImage 3.18.0 2020b broadwell, epyc, skylake aion, iris FreeImage is an Open Source library project for developers who would like to support popular graphics image formats like PNG, BMP, JPEG, TIFF and others as needed by today's multimedia applications. FreeImage is easy to use, fast, multithreading safe. GLib 2.62.0, 2.66.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion GLib is one of the base libraries of the GTK+ project GTK+ 3.24.13, 3.24.23 2019b, 2020b broadwell, skylake, epyc iris, aion GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction. Gdk-Pixbuf 2.38.2, 2.40.0 2019b, 2020b broadwell, skylake, epyc iris, aion The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3. HarfBuzz 2.6.4, 2.6.7 2019b, 2020b broadwell, skylake, epyc iris, aion HarfBuzz is an OpenType text shaping engine. ImageMagick 7.0.10-35, 7.0.9-5 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris ImageMagick is a software suite to create, edit, compose, or convert bitmap images JasPer 2.0.14, 2.0.24 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard. LittleCMS 2.11, 2.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance. Mesa 19.1.7, 19.2.1, 20.2.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. OpenCV 4.2.0, 4.5.1 2019b, 2020b broadwell, skylake, epyc iris, aion OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Includes extra modules for OpenCV from the contrib repository. OpenEXR 2.5.5 2020b broadwell, epyc, skylake aion, iris OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light & Magic for use in computer imaging applications POV-Ray 3.7.0.8 2020b broadwell, epyc, skylake aion, iris The Persistence of Vision Raytracer, or POV-Ray, is a ray tracing program which generates images from a text-based scene description, and is available for a variety of computer platforms. POV-Ray is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports. Pango 1.44.7, 1.47.0 2019b, 2020b broadwell, skylake, epyc iris, aion Pango is a library for laying out and rendering of text, with an emphasis on internationalization. Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in the context of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x. ParaView 5.6.2, 5.8.1 2019b, 2020b broadwell, skylake, epyc iris, aion ParaView is a scientific parallel visualizer. Pillow 6.2.1, 8.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. PyOpenGL 3.1.5 2020b broadwell, epyc, skylake aion, iris PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs. PyQt5 5.15.1 2020b broadwell, epyc, skylake aion, iris PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company. This bundle includes PyQtWebEngine, a set of Python bindings for The Qt Company\u2019s Qt WebEngine framework. PyQtGraph 0.11.1 2020b broadwell, epyc, skylake aion, iris PyQtGraph is a pure-python graphics and GUI library built on PyQt5/PySide2 and numpy. Tk 8.6.10, 8.6.9 2020b, 2019b broadwell, epyc, skylake, gpu aion, iris Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages. VMD 1.9.4a51 2020b broadwell, epyc, skylake aion, iris VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. VTK 8.2.0, 9.0.1 2019b, 2020b broadwell, skylake, epyc iris, aion The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VirtualGL 2.6.2 2019b broadwell, skylake iris VirtualGL is an open source toolkit that gives any Linux or Unix remote display software the ability to run OpenGL applications with full hardware acceleration. X11 20190717, 20201008 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The X Window System (X11) is a windowing system for bitmap displays Xvfb 1.20.9 2020b broadwell, epyc, skylake, gpu aion, iris Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory. at-spi2-atk 2.34.1, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion AT-SPI 2 toolkit bridge at-spi2-core 2.34.0, 2.38.0 2019b, 2020b broadwell, skylake, epyc iris, aion Assistive Technology Service Provider Interface. cairo 1.16.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB fontconfig 2.13.1, 2.13.92 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Fontconfig is a library designed to provide system-wide font configuration, customization and application access. freetype 2.10.1, 2.10.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well. gnuplot 5.2.8, 5.4.1 2019b, 2020b broadwell, skylake, epyc iris, aion Portable interactive, function plotting utility libGLU 9.0.1 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL. matplotlib 3.1.1, 3.3.3 2019b, 2020b broadwell, skylake, epyc, gpu iris, aion matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits. pixman 0.38.4, 0.40.0 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server. scikit-image 0.18.1 2020b broadwell, epyc, skylake, gpu aion, iris scikit-image is a collection of algorithms for image processing. x264 20190925, 20201026 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL. x265 3.2, 3.3 2019b, 2020b broadwell, skylake, gpu, epyc iris, aion x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL. xprop 1.2.4, 1.2.5 2019b, 2020b broadwell, skylake, epyc iris, aion The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information.","title":"Visualisation"},{"location":"software/visu/paraview/","text":"ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView\u2019s batch processing capabilities. ParaView was developed to analyse extremely large datasets using distributed memory computing resources. It can be run on supercomputers to analyse datasets of petascale size as well as on laptops for smaller data, has become an integral tool in many national laboratories, universities and industry, and has won several awards related to high performance computation. ParaView ia an open-source, interactive, scalable, data analysis and scientific visualization tools. It can be used to visualize the simulation data or processing the data by using GUI or non-interactive mode by using the Python scripting . Using non-interacting mode, that is using the python scripting is much faster than using the interactive mode, when the data set is larger in both ParaView and VisIt. Available versions of ParaView in ULHPC \u00b6 To check available versions of ParaView at ULHPC type module spider paraview . The following list shows the available versions of ParaView in ULHPC. vis/ParaView/5.5.0-intel-2018a-mpi vis/ParaView/5.6.2-foss-2019a-mpi vis/ParaView/5.6.2-intel-2019a-mpi Interactive mode \u00b6 To open an ParaView in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module abinit and needed environment $ module purge $ module load swenv/default-env/latest $ module load vis/ParaView/5.6.2-intel-2019a-mpi $ paraview & Batch mode \u00b6 #!/bin/bash -l #SBATCH -J ParaView ###SBATCH -A <project name> #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module Paraview and needed environment module purge module load swenv/default-env/latest module load vis/ParaView/5.6.2-intel-2019a-mpi srun -n ${ SLURM_NTASKS } pvbatch python-script.py Additional information \u00b6 ParaView's User Manual has a detail instructions about visualization and processing data in ParaView. There are two ways of getting or writing the python script for the ParaView: Reading the ParaView's python scripting wiki and ParaView's Python Scripting Manual . Record the commands that we do in ParaView GUI. Later this commands put into python script and it can be run as python scripting in ParaView. Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"ParaView"},{"location":"software/visu/paraview/#available-versions-of-paraview-in-ulhpc","text":"To check available versions of ParaView at ULHPC type module spider paraview . The following list shows the available versions of ParaView in ULHPC. vis/ParaView/5.5.0-intel-2018a-mpi vis/ParaView/5.6.2-foss-2019a-mpi vis/ParaView/5.6.2-intel-2019a-mpi","title":"Available versions of ParaView in ULHPC"},{"location":"software/visu/paraview/#interactive-mode","text":"To open an ParaView in the interactive mode, please follow the following steps: # From your local computer $ ssh -X iris-cluster # Reserve the node for interactive computation $ salloc -p interactive --time = 00 :30:00 --ntasks 1 -c 4 --x11 # OR si --x11 [...] # Load the module abinit and needed environment $ module purge $ module load swenv/default-env/latest $ module load vis/ParaView/5.6.2-intel-2019a-mpi $ paraview &","title":"Interactive mode"},{"location":"software/visu/paraview/#batch-mode","text":"#!/bin/bash -l #SBATCH -J ParaView ###SBATCH -A <project name> #SBATCH -N 2 #SBATCH --ntasks-per-node=28 #SBATCH --time=00:30:00 #SBATCH -p batch # Load the module Paraview and needed environment module purge module load swenv/default-env/latest module load vis/ParaView/5.6.2-intel-2019a-mpi srun -n ${ SLURM_NTASKS } pvbatch python-script.py","title":"Batch mode"},{"location":"software/visu/paraview/#additional-information","text":"ParaView's User Manual has a detail instructions about visualization and processing data in ParaView. There are two ways of getting or writing the python script for the ParaView: Reading the ParaView's python scripting wiki and ParaView's Python Scripting Manual . Record the commands that we do in ParaView GUI. Later this commands put into python script and it can be run as python scripting in ParaView. Tip If you find some issues with the instructions above, please report it to us using support ticket .","title":"Additional information"},{"location":"support/","text":"Support \u00b6 ULHPC strives to support in a user friendly way your [super]computing needs. Note however that we are not here to make your PhD at your place ;) Service Now HPC Support Portal FAQ/Troubleshooting \u00b6 Password reset Connection issues File Permissions Access rights to project directory Quotas and Purging Read the Friendly Manual \u00b6 We have always maintained an extensive documentation and tutorials available online, which aims at being the most up-to-date and comprehensive. So please, read the documentation first if you have a question of problem -- we probably provide detailed instructions here Help Desk \u00b6 The online help desk Service is the preferred method for contacting ULHPC. Tips Before reporting a problem or and issue, kindly remember that: Your issue is probably documented here on the ULHPC Technical documentation An event may be on-going: check the ULHPC Live status page Planned maintenance are announced at least 2 weeks in advance - -- see Maintenance and Downtime Policy The proper SSH banner is displayed during planned downtime check the state of your nodes and jobs Joining/monitoring running jobs Monitoring post-mortem Job status and efficiency Service Now HPC Support Portal You can make code snippets, shell outputs, etc in your ticket much more readable by inserting a line with: [code]<pre> before the snippet, and another line with: </pre>[/code] after it. For a full list of formatting options, see this ServiceNow article . Be as precise and complete as possible ULHPC team handle thousands of support requests per year. In order to ensure efficient timely resolution of issues, ensure that: you select the appropriate category (left menu) you include as much of the following as possible when making a request: Who? - Name and user id (login), eventually project name When? - When did the problem occur? Where? - Which cluster ? Which node ? Which job ? Really include Job IDs Location of relevant files input/output, job launcher scripts, source code, executables etc. What? - What happened? What exactly were you doing or trying to do ? include Error messages - kindly report system or software messages literally and exactly . output of module list any steps you have tried Steps to reproduce Any part of this technical documentation you checked before opening the ticket Access to the online help system requires logging in with your Uni.lu username, password, and eventually one-time password. If you are an existing user unable to log in, you can send us an email . Availability and Response Time HPC support is provided on a volunteer basis by UL HPC staff and associated UL experts working at normal business hours. We offer no guarantee on response time except with paid support contracts. Email support \u00b6 You can contact us by mail to the ULHPC Team Email ( ONLY if you cannot login/access the HPC Support helpdesk portal : hpc-team@uni.lu You may also ask the help of other ULHPC users using the HPC User community mailing list: (moderated): hpc-users@uni.lu","title":"Overview"},{"location":"support/#support","text":"ULHPC strives to support in a user friendly way your [super]computing needs. Note however that we are not here to make your PhD at your place ;) Service Now HPC Support Portal","title":"Support"},{"location":"support/#faqtroubleshooting","text":"Password reset Connection issues File Permissions Access rights to project directory Quotas and Purging","title":"FAQ/Troubleshooting"},{"location":"support/#read-the-friendly-manual","text":"We have always maintained an extensive documentation and tutorials available online, which aims at being the most up-to-date and comprehensive. So please, read the documentation first if you have a question of problem -- we probably provide detailed instructions here","title":"Read the Friendly Manual"},{"location":"support/#help-desk","text":"The online help desk Service is the preferred method for contacting ULHPC. Tips Before reporting a problem or and issue, kindly remember that: Your issue is probably documented here on the ULHPC Technical documentation An event may be on-going: check the ULHPC Live status page Planned maintenance are announced at least 2 weeks in advance - -- see Maintenance and Downtime Policy The proper SSH banner is displayed during planned downtime check the state of your nodes and jobs Joining/monitoring running jobs Monitoring post-mortem Job status and efficiency Service Now HPC Support Portal You can make code snippets, shell outputs, etc in your ticket much more readable by inserting a line with: [code]<pre> before the snippet, and another line with: </pre>[/code] after it. For a full list of formatting options, see this ServiceNow article . Be as precise and complete as possible ULHPC team handle thousands of support requests per year. In order to ensure efficient timely resolution of issues, ensure that: you select the appropriate category (left menu) you include as much of the following as possible when making a request: Who? - Name and user id (login), eventually project name When? - When did the problem occur? Where? - Which cluster ? Which node ? Which job ? Really include Job IDs Location of relevant files input/output, job launcher scripts, source code, executables etc. What? - What happened? What exactly were you doing or trying to do ? include Error messages - kindly report system or software messages literally and exactly . output of module list any steps you have tried Steps to reproduce Any part of this technical documentation you checked before opening the ticket Access to the online help system requires logging in with your Uni.lu username, password, and eventually one-time password. If you are an existing user unable to log in, you can send us an email . Availability and Response Time HPC support is provided on a volunteer basis by UL HPC staff and associated UL experts working at normal business hours. We offer no guarantee on response time except with paid support contracts.","title":"Help Desk"},{"location":"support/#email-support","text":"You can contact us by mail to the ULHPC Team Email ( ONLY if you cannot login/access the HPC Support helpdesk portal : hpc-team@uni.lu You may also ask the help of other ULHPC users using the HPC User community mailing list: (moderated): hpc-users@uni.lu","title":"Email support"},{"location":"support/professional-services/","text":"Consulting and Professional HPC Support Services \u00b6 Expert-level service and support can be provided by the University HPC staff in the form of pools of 4-hour specialized help, at 480\u20ac VAT excluded (equivalent to 120\u20ac per hour expert rate EA). Service and support activities include but are not limited to HPC training on facilities utilization, software installation, HPC workflow development. Such support service is typically handled by an HPC service agreement signed between the ULHPC and the company requesting consultant services. Note that such expert-level support is natively embedded on all HPC Resource allocation Service Contract for external and private partners . Contact us for more details.","title":"ULHPC Consulting Services"},{"location":"support/professional-services/#consulting-and-professional-hpc-support-services","text":"Expert-level service and support can be provided by the University HPC staff in the form of pools of 4-hour specialized help, at 480\u20ac VAT excluded (equivalent to 120\u20ac per hour expert rate EA). Service and support activities include but are not limited to HPC training on facilities utilization, software installation, HPC workflow development. Such support service is typically handled by an HPC service agreement signed between the ULHPC and the company requesting consultant services. Note that such expert-level support is natively embedded on all HPC Resource allocation Service Contract for external and private partners . Contact us for more details.","title":"Consulting and Professional HPC Support Services"},{"location":"systems/","text":"HPC @ Uni.lu \u00b6 For more details, see the reference ULHPC Article : ACM Reference Format | ORBilu entry | ULHPC blog post | slides : Sebastien Varrette, Hyacinthe Cartiaux, Sarah Peter, Emmanuel Kieffer, Teddy Valette, and Abatcha Olloh. 2022. Management of an Academic HPC & Research Computing Facility: The ULHPC Experience 2.0. In 6 th High Performance Computing and Cluster Technologies Conference (HPCCT 2022), July 08-10, 2022, Fuzhou, China. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3560442.3560445 Chronological Evolution \u00b6 With the advent of the technological revolution and the digital transformation that made all scientific disciplines becoming computational nowadays, High-Performance Computing (HPC) is increasingly identified as a strategic asset and enabler to accelerate the research performed in all areas requiring intensive computing and large-scale Big Data analytic capabilities. The University of Luxembourg (UL) operates since 2007 a large academic HPC facility that remained the reference HPC implementation within the country until 2021, offering a cutting-edge research infrastructure to Luxembourg public research while serving as edge access to the Euro-HPC Luxembourg supercomputer operated by LuxProvide and more focused at serving the private sector. Special focus was laid for the ULHPC facility on the development of large computing power combined with huge data storage capacity to accelerate the research performed in intensive computing and large-scale data analytic (Big Data). This was made possible through an ambitious funding strategy enabled from the early stage of the HPC developments, which was supported at the rectorate level to establish the HPC strategy as transversal to all research domains. For more details 1 : hpc.uni.lu Capacity evolution \u00b6 The historically first production system installed in 2007 has been Chaos with a final theoretical peak performance of 14.5 TFlop/s. Gaia was then launched in 2011 as a replacement to reach a theoretical peak performance of 145.5 TFlops. It was the first computing cluster introducing GPU accelerators to our users. Both systems were kept running until their decommissioning in 2019. Info Currently, Iris ( R_\\text{peak} R_\\text{peak} =1071 TFlops) and Aion ( R_\\text{peak} R_\\text{peak} =1693 TFlops) are our production systems sharing the same High Performance Storage solutions, when Aion is our flagship supercomputer until 2024. The below figures illustrates the evolution of the computing and storage capacity of the ULHPC facility over the last years. Experimental systems \u00b6 We maintain (or used to maintain) several experimental systems in parallel ( nyx , a testing cluster, pyro , on OpenStack-based cluster, viridis , a low-power ARM-based cluster). As of now, only our experimental Grid'5000 clusters are still maintained. Usage \u00b6 The below figure outline the cumulative usage (in CPU Years) of the production clusters within the ULHPC facility for the time period 2015-2019. During their lifetime, Gaia and Chaos processed respectively 4.5 million and 1.7 million jobs , cumulating 13835 Years of CPU Time usage . Naming conventions \u00b6 Our clusters and supercomputers are named from Greek primordial deities or Greek mythology while keeping a name as short as possible. chaos was, according to Hesiod's Theogony, the first thing to exist and thus looked as appropriate. \" Hesiod's Chaos has been interpreted as either \"the gaping void above the Earth created when Earth and Sky are separated from their primordial unity \" gaia is the personification of the Earth and the ancestral mother of all life. It sounded pertinent for our first system installed in Belval to serve the growing life-science community and the newly created LCSB system bio-medicine Interdiciplinary center. iris is the personification and goddess of the rainbow and messenger of the gods. aion is a Hellenistic deity associated with time, the orb or circle encompassing the universe, and the zodiac. The facility is managed and operated by an expert team led by Dr. S. Varrette . Contacts: hpc-team@uni.lu \u21a9","title":"Overview"},{"location":"systems/#hpc-unilu","text":"For more details, see the reference ULHPC Article : ACM Reference Format | ORBilu entry | ULHPC blog post | slides : Sebastien Varrette, Hyacinthe Cartiaux, Sarah Peter, Emmanuel Kieffer, Teddy Valette, and Abatcha Olloh. 2022. Management of an Academic HPC & Research Computing Facility: The ULHPC Experience 2.0. In 6 th High Performance Computing and Cluster Technologies Conference (HPCCT 2022), July 08-10, 2022, Fuzhou, China. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3560442.3560445","title":"HPC @ Uni.lu"},{"location":"systems/#chronological-evolution","text":"With the advent of the technological revolution and the digital transformation that made all scientific disciplines becoming computational nowadays, High-Performance Computing (HPC) is increasingly identified as a strategic asset and enabler to accelerate the research performed in all areas requiring intensive computing and large-scale Big Data analytic capabilities. The University of Luxembourg (UL) operates since 2007 a large academic HPC facility that remained the reference HPC implementation within the country until 2021, offering a cutting-edge research infrastructure to Luxembourg public research while serving as edge access to the Euro-HPC Luxembourg supercomputer operated by LuxProvide and more focused at serving the private sector. Special focus was laid for the ULHPC facility on the development of large computing power combined with huge data storage capacity to accelerate the research performed in intensive computing and large-scale data analytic (Big Data). This was made possible through an ambitious funding strategy enabled from the early stage of the HPC developments, which was supported at the rectorate level to establish the HPC strategy as transversal to all research domains. For more details 1 : hpc.uni.lu","title":"Chronological Evolution"},{"location":"systems/#capacity-evolution","text":"The historically first production system installed in 2007 has been Chaos with a final theoretical peak performance of 14.5 TFlop/s. Gaia was then launched in 2011 as a replacement to reach a theoretical peak performance of 145.5 TFlops. It was the first computing cluster introducing GPU accelerators to our users. Both systems were kept running until their decommissioning in 2019. Info Currently, Iris ( R_\\text{peak} R_\\text{peak} =1071 TFlops) and Aion ( R_\\text{peak} R_\\text{peak} =1693 TFlops) are our production systems sharing the same High Performance Storage solutions, when Aion is our flagship supercomputer until 2024. The below figures illustrates the evolution of the computing and storage capacity of the ULHPC facility over the last years.","title":"Capacity evolution"},{"location":"systems/#experimental-systems","text":"We maintain (or used to maintain) several experimental systems in parallel ( nyx , a testing cluster, pyro , on OpenStack-based cluster, viridis , a low-power ARM-based cluster). As of now, only our experimental Grid'5000 clusters are still maintained.","title":"Experimental systems"},{"location":"systems/#usage","text":"The below figure outline the cumulative usage (in CPU Years) of the production clusters within the ULHPC facility for the time period 2015-2019. During their lifetime, Gaia and Chaos processed respectively 4.5 million and 1.7 million jobs , cumulating 13835 Years of CPU Time usage .","title":"Usage"},{"location":"systems/#naming-conventions","text":"Our clusters and supercomputers are named from Greek primordial deities or Greek mythology while keeping a name as short as possible. chaos was, according to Hesiod's Theogony, the first thing to exist and thus looked as appropriate. \" Hesiod's Chaos has been interpreted as either \"the gaping void above the Earth created when Earth and Sky are separated from their primordial unity \" gaia is the personification of the Earth and the ancestral mother of all life. It sounded pertinent for our first system installed in Belval to serve the growing life-science community and the newly created LCSB system bio-medicine Interdiciplinary center. iris is the personification and goddess of the rainbow and messenger of the gods. aion is a Hellenistic deity associated with time, the orb or circle encompassing the universe, and the zodiac. The facility is managed and operated by an expert team led by Dr. S. Varrette . Contacts: hpc-team@uni.lu \u21a9","title":"Naming conventions"},{"location":"systems/aion/","text":"Aion Overview \u00b6 Aion is a Atos/Bull /AMD supercomputer which consists of 318 compute nodes, totaling 40704 compute cores and 81408 GB RAM, with a peak performance of about 1,70 PetaFLOP/s . All nodes are interconnected through a Fast InfiniBand (IB) HDR100 network 1 , configured over a Fat-Tree Topology (blocking factor 1:2). Aion nodes are equipped with AMD Epyc ROME 7H12 processors. Two global high -performance clustered file systems are available on all ULHPC computational systems: one based on GPFS/SpectrumScale, one on Lustre. Aion Compute Aion Interconnect Global Storage The cluster runs a Red Hat Linux operating system. The ULHPC Team supplies on all clusters a large variety of HPC utilities, scientific applications and programming libraries to its user community. The user software environment is generated using Easybuild (EB) and is made available as environment modules from the compute nodes only. Slurm is the Resource and Job Management Systems (RJMS) which provides computing resources allocations and job execution. For more information: see ULHPC slurm docs . Cluster Organization \u00b6 Data Center Configuration \u00b6 The Aion cluster is based on a cell made of 4 BullSequana XH2000 adjacent racks installed in the CDC ( Centre de Calcul ) data center of the University within one of the DLC-enabled server room (CDC S-02-004) adjacent to the room hosting the Iris cluster and the global storage . Each rack has the following dimensions: HxWxD (mm) = 2030x750x1270 (Depth is 1350mm with aesthetic doors). The full solution with 4 racks (total dimension: dimensions: HxWxD (mm) = 2030x3000x1270) with the following characteristics: Rack 1 Rack 2 Rack 3 Rack 4 TOTAL Weight [kg] 1872,4 1830,2 1830,2 1824,2 7357 kg #X2410 Rome Blade 28 26 26 26 106 #Compute Nodes 84 78 78 78 318 #Compute Cores 10752 9984 9984 9984 40704 R_\\text{peak} R_\\text{peak} [TFlops] 447,28 TF 415,33 TF 415,33 TF 415,33 TF 1693.29 TF For more details: BullSequana XH2000 SpecSheet (PDF) Cooling \u00b6 The BullSequana XH2000 is a fan less innovative cooling solution which is ultra-energy-efficient (targeting a PUE very close to 1) using an enhanced version of the Bull Direct Liquid Cooling (DLC) technology. A separate hot -water circuit minimizes the total energy consumption of a system. For more information: see [Direct] Liquid Cooling . The illustration on the right shows an exploded view of a compute blade with the cold plate and heat spreaders. The DLC 1 components in the rack are: Compute nodes (CPU, Memory, Drives, GPU) High Speed Interconnect: HDR Management network: Ethernet management switches Power Supply Unit: DLC shelves The cooling area in the rack is composed of: 3 Hydraulic chassis (HYCs) for 2+1 redundancy at the bottom of the cabinet, 10.5U height. Each HYCs dissipates at a maximum of 240W in the air. A primary manifold system connects the University hot-water loop to the HYCs primary water inlets A secondary manifold system connects HYCs outlets to each blade in the compute cabinet Login/Access servers \u00b6 Aion has 2 access servers (256 GB of memory each, general access) access[1-2] Each login node has two sockets, each socket is populated with an AMD EPYC 7452 processor (2.2 GHz, 32 cores) Access servers are not meant for compute! The module command is not available on the access servers, only on the compute nodes you MUST NOT run any computing process on the access servers . Rack Cabinets \u00b6 The Aion cluster (management compute and interconnect) is installed across the two adjacent server rooms in the premises of the Centre de Calcul (CDC), in the CDC-S02-005 server room. Server Room Rack ID Purpose Type Description CDC-S02-005 D02 Network Interconnect equipment CDC-S02-005 A04 Management Management servers, Interconnect CDC-S02-004 A01 Compute regular aion-[0001-0084] , interconnect CDC-S02-004 A02 Compute regular aion-[0085-0162] , interconnect CDC-S02-004 A03 Compute regular aion-[0163-0240] , interconnect CDC-S02-004 A04 Compute regular aion-[0241-0318] , interconnect In addition, the global storage equipment ( GPFS/SpectrumScale and Lustre , common to both Iris and Aion clusters) is installed in another row of cabinets of the same server room. All DLC components are built on a cold plate which cools all components by direct contact, except DIMMS for which custom heat spreaders evacuate the heat to the cold plate. \u21a9 \u21a9","title":"Aion System"},{"location":"systems/aion/#aion-overview","text":"Aion is a Atos/Bull /AMD supercomputer which consists of 318 compute nodes, totaling 40704 compute cores and 81408 GB RAM, with a peak performance of about 1,70 PetaFLOP/s . All nodes are interconnected through a Fast InfiniBand (IB) HDR100 network 1 , configured over a Fat-Tree Topology (blocking factor 1:2). Aion nodes are equipped with AMD Epyc ROME 7H12 processors. Two global high -performance clustered file systems are available on all ULHPC computational systems: one based on GPFS/SpectrumScale, one on Lustre. Aion Compute Aion Interconnect Global Storage The cluster runs a Red Hat Linux operating system. The ULHPC Team supplies on all clusters a large variety of HPC utilities, scientific applications and programming libraries to its user community. The user software environment is generated using Easybuild (EB) and is made available as environment modules from the compute nodes only. Slurm is the Resource and Job Management Systems (RJMS) which provides computing resources allocations and job execution. For more information: see ULHPC slurm docs .","title":"Aion Overview"},{"location":"systems/aion/#cluster-organization","text":"","title":"Cluster Organization"},{"location":"systems/aion/#data-center-configuration","text":"The Aion cluster is based on a cell made of 4 BullSequana XH2000 adjacent racks installed in the CDC ( Centre de Calcul ) data center of the University within one of the DLC-enabled server room (CDC S-02-004) adjacent to the room hosting the Iris cluster and the global storage . Each rack has the following dimensions: HxWxD (mm) = 2030x750x1270 (Depth is 1350mm with aesthetic doors). The full solution with 4 racks (total dimension: dimensions: HxWxD (mm) = 2030x3000x1270) with the following characteristics: Rack 1 Rack 2 Rack 3 Rack 4 TOTAL Weight [kg] 1872,4 1830,2 1830,2 1824,2 7357 kg #X2410 Rome Blade 28 26 26 26 106 #Compute Nodes 84 78 78 78 318 #Compute Cores 10752 9984 9984 9984 40704 R_\\text{peak} R_\\text{peak} [TFlops] 447,28 TF 415,33 TF 415,33 TF 415,33 TF 1693.29 TF For more details: BullSequana XH2000 SpecSheet (PDF)","title":"Data Center Configuration"},{"location":"systems/aion/#cooling","text":"The BullSequana XH2000 is a fan less innovative cooling solution which is ultra-energy-efficient (targeting a PUE very close to 1) using an enhanced version of the Bull Direct Liquid Cooling (DLC) technology. A separate hot -water circuit minimizes the total energy consumption of a system. For more information: see [Direct] Liquid Cooling . The illustration on the right shows an exploded view of a compute blade with the cold plate and heat spreaders. The DLC 1 components in the rack are: Compute nodes (CPU, Memory, Drives, GPU) High Speed Interconnect: HDR Management network: Ethernet management switches Power Supply Unit: DLC shelves The cooling area in the rack is composed of: 3 Hydraulic chassis (HYCs) for 2+1 redundancy at the bottom of the cabinet, 10.5U height. Each HYCs dissipates at a maximum of 240W in the air. A primary manifold system connects the University hot-water loop to the HYCs primary water inlets A secondary manifold system connects HYCs outlets to each blade in the compute cabinet","title":"Cooling"},{"location":"systems/aion/#loginaccess-servers","text":"Aion has 2 access servers (256 GB of memory each, general access) access[1-2] Each login node has two sockets, each socket is populated with an AMD EPYC 7452 processor (2.2 GHz, 32 cores) Access servers are not meant for compute! The module command is not available on the access servers, only on the compute nodes you MUST NOT run any computing process on the access servers .","title":"Login/Access servers"},{"location":"systems/aion/#rack-cabinets","text":"The Aion cluster (management compute and interconnect) is installed across the two adjacent server rooms in the premises of the Centre de Calcul (CDC), in the CDC-S02-005 server room. Server Room Rack ID Purpose Type Description CDC-S02-005 D02 Network Interconnect equipment CDC-S02-005 A04 Management Management servers, Interconnect CDC-S02-004 A01 Compute regular aion-[0001-0084] , interconnect CDC-S02-004 A02 Compute regular aion-[0085-0162] , interconnect CDC-S02-004 A03 Compute regular aion-[0163-0240] , interconnect CDC-S02-004 A04 Compute regular aion-[0241-0318] , interconnect In addition, the global storage equipment ( GPFS/SpectrumScale and Lustre , common to both Iris and Aion clusters) is installed in another row of cabinets of the same server room. All DLC components are built on a cold plate which cools all components by direct contact, except DIMMS for which custom heat spreaders evacuate the heat to the cold plate. \u21a9 \u21a9","title":"Rack Cabinets"},{"location":"systems/aion/compute/","text":"Aion Compute Nodes \u00b6 Aion is a cluster of x86-64 AMD-based compute nodes. More precisely, Aion consists of 318 \" regular \" computational nodes named aion-[0001-0318] as follows: Hostname (#Nodes) #cores type Processors RAM R_\\text{peak} R_\\text{peak} [TFlops] aion-[0001-0318] (318) 40704 Regular Epyc 2 AMD Epyc ROME 7H12 @ 2.6 GHz [64c/280W] 256 GB 5.32 TF Aion compute nodes compute nodes MUST be seen as 8 (virtual) processors of 16 cores each , even if physically the nodes are hosting 2 physical sockets of AMD Epyc ROME 7H12 processors having 64 cores each (total: 128 cores per node). As will be highlighted in the slurm resource allocation , that means that targetting a full node utilization assumes that you use the following format attributes to your jobs: {sbatch|srun|si|salloc} [-N <N>] --ntasks-per-node <8n> --ntasks-per-socket <n> -c <thread> where you want to ensure that <n> \\times \\times <thread> = 16 on aion this will bring a total of <N> \\times 8\\times \\times 8\\times <n> tasks, each on <thread> threads Ex: -N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4 ( Total : 64 tasks) Processor Performance \u00b6 Each Aion node rely on an AMD Epyc Rome processor architecture which is binary compatible with the x86-64 architecture. Each processor has the following performance: Processor Model #cores TDP(*) CPU Freq. R_\\text{peak} R_\\text{peak} [TFlops] R_\\text{max} R_\\text{max} [TFlops] AMD Epyc ROME 7H12 64 280W 2.6 GHz 2.66 TF 2.13 TF (*) The Thermal Design Power (TDP) represents the average power, in watts, the processor dissipates when operating at Base Frequency with all cores active under an Intel-defined, high-complexity workload. Theoretical R_\\text{peak} R_\\text{peak} vs. Maximum R_\\text{max} R_\\text{max} Performance for AMD Epyc The AMD Epyc processors carry on 16 Double Precision (DP) ops/cycle . Thus the reported R_\\text{peak} R_\\text{peak} performance is computed as follows: R_\\text{peak} = ops/cycle \\times Freq. \\times \\#Cores R_\\text{peak} = ops/cycle \\times Freq. \\times \\#Cores With regards the estimation of the Maximum Performance R_\\text{max} R_\\text{max} , an efficiency factor of 80% is applied. It is computed from the expected performance runs during the HPL benchmark workload. Regular Dual-CPU Nodes \u00b6 These nodes are packaged within BullSequana X2410 AMD compute blades . Each blade contains 3 dual-socket AMD Rome nodes side-by-side, connected to the BullSequana XH2000 local interconnect network through HDR100 ports which is done through a mezzanine board. The BullSequana AMD blade is built upon a cold plate which cools all components by direct contact, except DIMMS for which custom heat spreaders evacuate the heat to the cold plate -- see exploded view Characteristics of each blade and associated compute nodes are depicted in the below table BullSequana X2410 AMD blade Form Factor 1U blade comprising 3 compute nodes side-by-side #Nodes per blade 3 Processors per node 2x AMD Epyc ROME 7H12 @ 2.6 GHz [64c/280W] Architecture AMD SP3 Platform: 3x1 motherboard Memory per node 256 GB DDR4 3200MT/s (8x16 GB DIMMs per socket , 16 DIMMs per node) Network (per node) InfiniBand HDR100 single port mezzanine Storage (per node) 1x 480 GB SSD Power supply PSU shelves on top of XH2000 cabinet Cooling Cooling by direct contact Physical specs. (HxWxD) 44.45 x 600 x 540 mm The four compute racks of Aion (one XH2000 Cell) holds a total of 106 blades i.e., 318 AMD Epyc compute nodes, totalling 40704 computing core -- see Aion configuration .","title":"Compute Nodes"},{"location":"systems/aion/compute/#aion-compute-nodes","text":"Aion is a cluster of x86-64 AMD-based compute nodes. More precisely, Aion consists of 318 \" regular \" computational nodes named aion-[0001-0318] as follows: Hostname (#Nodes) #cores type Processors RAM R_\\text{peak} R_\\text{peak} [TFlops] aion-[0001-0318] (318) 40704 Regular Epyc 2 AMD Epyc ROME 7H12 @ 2.6 GHz [64c/280W] 256 GB 5.32 TF Aion compute nodes compute nodes MUST be seen as 8 (virtual) processors of 16 cores each , even if physically the nodes are hosting 2 physical sockets of AMD Epyc ROME 7H12 processors having 64 cores each (total: 128 cores per node). As will be highlighted in the slurm resource allocation , that means that targetting a full node utilization assumes that you use the following format attributes to your jobs: {sbatch|srun|si|salloc} [-N <N>] --ntasks-per-node <8n> --ntasks-per-socket <n> -c <thread> where you want to ensure that <n> \\times \\times <thread> = 16 on aion this will bring a total of <N> \\times 8\\times \\times 8\\times <n> tasks, each on <thread> threads Ex: -N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4 ( Total : 64 tasks)","title":"Aion Compute Nodes"},{"location":"systems/aion/compute/#processor-performance","text":"Each Aion node rely on an AMD Epyc Rome processor architecture which is binary compatible with the x86-64 architecture. Each processor has the following performance: Processor Model #cores TDP(*) CPU Freq. R_\\text{peak} R_\\text{peak} [TFlops] R_\\text{max} R_\\text{max} [TFlops] AMD Epyc ROME 7H12 64 280W 2.6 GHz 2.66 TF 2.13 TF (*) The Thermal Design Power (TDP) represents the average power, in watts, the processor dissipates when operating at Base Frequency with all cores active under an Intel-defined, high-complexity workload. Theoretical R_\\text{peak} R_\\text{peak} vs. Maximum R_\\text{max} R_\\text{max} Performance for AMD Epyc The AMD Epyc processors carry on 16 Double Precision (DP) ops/cycle . Thus the reported R_\\text{peak} R_\\text{peak} performance is computed as follows: R_\\text{peak} = ops/cycle \\times Freq. \\times \\#Cores R_\\text{peak} = ops/cycle \\times Freq. \\times \\#Cores With regards the estimation of the Maximum Performance R_\\text{max} R_\\text{max} , an efficiency factor of 80% is applied. It is computed from the expected performance runs during the HPL benchmark workload.","title":"Processor Performance"},{"location":"systems/aion/compute/#regular-dual-cpu-nodes","text":"These nodes are packaged within BullSequana X2410 AMD compute blades . Each blade contains 3 dual-socket AMD Rome nodes side-by-side, connected to the BullSequana XH2000 local interconnect network through HDR100 ports which is done through a mezzanine board. The BullSequana AMD blade is built upon a cold plate which cools all components by direct contact, except DIMMS for which custom heat spreaders evacuate the heat to the cold plate -- see exploded view Characteristics of each blade and associated compute nodes are depicted in the below table BullSequana X2410 AMD blade Form Factor 1U blade comprising 3 compute nodes side-by-side #Nodes per blade 3 Processors per node 2x AMD Epyc ROME 7H12 @ 2.6 GHz [64c/280W] Architecture AMD SP3 Platform: 3x1 motherboard Memory per node 256 GB DDR4 3200MT/s (8x16 GB DIMMs per socket , 16 DIMMs per node) Network (per node) InfiniBand HDR100 single port mezzanine Storage (per node) 1x 480 GB SSD Power supply PSU shelves on top of XH2000 cabinet Cooling Cooling by direct contact Physical specs. (HxWxD) 44.45 x 600 x 540 mm The four compute racks of Aion (one XH2000 Cell) holds a total of 106 blades i.e., 318 AMD Epyc compute nodes, totalling 40704 computing core -- see Aion configuration .","title":"Regular Dual-CPU Nodes"},{"location":"systems/aion/interconnect/","text":"Fast Local Interconnect Network \u00b6 The Fast local interconnect network implemented within Aion relies on the Mellanox Infiniband (IB) HDR 1 technology. For more details, see Introduction to High-Speed InfiniBand Interconnect . IB Network Topology \u00b6 One of the most significant differentiators between HPC systems and lesser performing systems is, apart from the interconnect technology deployed, the supporting topology. There are several topologies commonly used in large-scale HPC deployments ( Fat-Tree , 3D-Torus , Dragonfly+ etc.). Aion (like Iris ) is part of an Island which employs a \" Fat-Tree \" Topology 2 which remains the widely used topology in HPC clusters due to its versatility, high bisection bandwidth and well understood routing. Aion IB HDR switched fabric relies on Mellanox WH40 DLC Quantum Switches located at the back of each BullSequana XH2000 racks. Each DLC cooled (see splitted view on the right) HDR switch has the following characteristics: 80 X HDR100 100Gb/s ports in a 1U switch (40 X HDR 200Gb/s ports if used in full HDR mode) 16Tb/s aggregate switch throughput Up to 15.8 billion messages-per-second 90ns switch latency Aion 2-Level 1:2 Fat-Tree is composed of: 12x Infiniband HDR 1 switches (40 HDR ports / 80 HDR100 ports) 8x Leaf IB (LIB) switches (L1), each with 12 HDR L1-L2 interlinks (2 on each rack) 4x Spine IB (SIB) switches (L2), with up to 16 HDR100 uplinks (12 used, total: 48 links) used for the interconnexion with the Iris Cluster Up to 48 compute nodes HDR100 connection per L1 switch using 24 HDR ports with Y-cable 4 available HDR connections for Service, Access or Gateway node per L1 switch The following illustration show HDRtopology within the BullSequana XH2000 cell schematically: For more details: ULHPC Fast IB Interconnect Routing Algorithm \u00b6 The IB Subnet Managers in Aion are configured with the Up/Down InfiniBand Routing Algorithm Up-Down is a super-set of Fat-Tree with a tracker mode that allow each node to have dedicated route. This is well adapted to IO traffic patterns, and would be used within Aion for Gateway nodes, Lustre OSS, and GPFS/SpectrumScale NSD servers. For more details: Understanding Up/Down InfiniBand Routing Algorithm High Data Rate (HDR) \u2013 200 Gb/s throughput with a very low latency, typically below 0,6 \\mu \\mu s. The HDR100 technology allows one 200Gbps HDR port (aggregation 4x 50Gbps) to be divided into 2 HDR100 ports with 100Gbps (2x 50Gbps) bandwidth using an [optical] \" splitter \" cable . \u21a9 \u21a9 with blocking factor 1:2. \u21a9","title":"Fast Local Interconnect"},{"location":"systems/aion/interconnect/#fast-local-interconnect-network","text":"The Fast local interconnect network implemented within Aion relies on the Mellanox Infiniband (IB) HDR 1 technology. For more details, see Introduction to High-Speed InfiniBand Interconnect .","title":"Fast Local Interconnect Network"},{"location":"systems/aion/interconnect/#ib-network-topology","text":"One of the most significant differentiators between HPC systems and lesser performing systems is, apart from the interconnect technology deployed, the supporting topology. There are several topologies commonly used in large-scale HPC deployments ( Fat-Tree , 3D-Torus , Dragonfly+ etc.). Aion (like Iris ) is part of an Island which employs a \" Fat-Tree \" Topology 2 which remains the widely used topology in HPC clusters due to its versatility, high bisection bandwidth and well understood routing. Aion IB HDR switched fabric relies on Mellanox WH40 DLC Quantum Switches located at the back of each BullSequana XH2000 racks. Each DLC cooled (see splitted view on the right) HDR switch has the following characteristics: 80 X HDR100 100Gb/s ports in a 1U switch (40 X HDR 200Gb/s ports if used in full HDR mode) 16Tb/s aggregate switch throughput Up to 15.8 billion messages-per-second 90ns switch latency Aion 2-Level 1:2 Fat-Tree is composed of: 12x Infiniband HDR 1 switches (40 HDR ports / 80 HDR100 ports) 8x Leaf IB (LIB) switches (L1), each with 12 HDR L1-L2 interlinks (2 on each rack) 4x Spine IB (SIB) switches (L2), with up to 16 HDR100 uplinks (12 used, total: 48 links) used for the interconnexion with the Iris Cluster Up to 48 compute nodes HDR100 connection per L1 switch using 24 HDR ports with Y-cable 4 available HDR connections for Service, Access or Gateway node per L1 switch The following illustration show HDRtopology within the BullSequana XH2000 cell schematically: For more details: ULHPC Fast IB Interconnect","title":"IB Network Topology"},{"location":"systems/aion/interconnect/#routing-algorithm","text":"The IB Subnet Managers in Aion are configured with the Up/Down InfiniBand Routing Algorithm Up-Down is a super-set of Fat-Tree with a tracker mode that allow each node to have dedicated route. This is well adapted to IO traffic patterns, and would be used within Aion for Gateway nodes, Lustre OSS, and GPFS/SpectrumScale NSD servers. For more details: Understanding Up/Down InfiniBand Routing Algorithm High Data Rate (HDR) \u2013 200 Gb/s throughput with a very low latency, typically below 0,6 \\mu \\mu s. The HDR100 technology allows one 200Gbps HDR port (aggregation 4x 50Gbps) to be divided into 2 HDR100 ports with 100Gbps (2x 50Gbps) bandwidth using an [optical] \" splitter \" cable . \u21a9 \u21a9 with blocking factor 1:2. \u21a9","title":"Routing Algorithm"},{"location":"systems/aion/timeline/","text":"Aion Timeline \u00b6 This page records a brief timeline of significant events and user environment changes on Aion. Details are provided below. 2019 \u00b6 September 2019 \u00b6 Official Public release of Aion cluster tenders on TED European tender and PMP Portal (Portail des March\u00e9s Publics) on Sept 11, 2019 RFP 190027 : Tender for the acquisition of a Complementary High Performance Computing (HPC) cluster 'Aion' for the University of Luxembourg. TED Reference: TED72/2019-608787 PMP Reference: 1901442 The RFP is composed of the following Lots: Lot 1 : DLC Computing cluster aion . includes cabinets, cooling units (together with any and all required piping and cooling fluids), compute node enclosures and compute nodes blades (dual- socket, relying on x86_64 processor architecture), interconnect elements (Ethernet and InfiniBand) and management servers for the HPC services (including but not limited to operational lifecycle management, Operating System and services deployment, monitoring) tied to this new cluster implementation within one of the University Computer Centre (CDC) server rooms (CDC S-02-004), adjacent to the server room hosting the Iris cluster and its associated storage system), specialized for hosting compute equipment supporting direct liquid cooling through a separate high temperature water circuit, thus guaranteeing unprecedented energy efficiency and equipment density. In the first phase of operation, the system will be connected to the existing cold-water circuit and must be able to operate under these conditions [...] Lot 2 : Adaptation and extension of the existing High-Performance Storage systems includes extension of the existing primary high-performance storage solution featuring a SpectrumScale/GPFS filesystem (based on a DDN GridScaler solution installed as per attribution of the RFP 160019) hosting the user home and project directories, to enable the utilisation of the GPFS filesystem from both existing Iris and new Aion clusters while enabling access to the Lustre-based SCRATCH filesystem (based on a DDN ExaScaler solution installed as per attribution of the RFP 170035) from the new compute cluster is considered a plus. Enhancing and adapting the InfiniBand interconnection to guarantee current performance characteristics while under load from all clients (existing and new compute clusters) is considered a plus. [...] Lot 3 : Adaptation of the network (Ethernet and IB) integration of the new cluster within the existing Ethernet-based data and management networks, which involves the extension and consolidation of the actual Ethernet topology adaptation and extension of the existing InfiniBand (IB) topology to allow for bridging the two networks (Iris \"island\" and Aion \"island\") [...] October-November 2019 \u00b6 Bids Opening for both RFPs on October 29, 2019. Starting offers analysis by the ULHPC team, together with the procurement and legal department of the University December 2019 \u00b6 Awarding notification to the vendors RFP 190027 attributed to the Atos to provide: Lot 1 : the new DLC aion supercomputer, composed by 318 AMD compute nodes hosted within a compute cell made of 4 BullSequana XH2000 adjacent racks Fast Interconnect: HDR Infiniband Fabric in a Fat tree topology (2:1 blocking) Associated servers and management stack Lot 2 : Adaptation and extension of the existing High-Performance Storage systems. In particular, the usable storage capacity of the existing primary high-performance storage solution (SpectrumScale/GPFS filesystem) will be extended by 1720TB/1560TiB to reach a total of 4.41 PB Lot 3 :Adaptation of the network (Ethernet and IB) See also Atos Press Release Aion Supercomputer Overview 2020 \u00b6 January 2020 \u00b6 Kickoff meeting -- see UL Newsletter planning for a production release of the new cluster in May 2020 February-March 2020: Start of global COVID-19 crisis \u00b6 COVID-19 Impact on HPC Activities all operations tied to the preparation and installation of the new aion cluster are postponed ULHPC systems remain operational, technical and non-technical staff are working remotely from home Global worldwide delays on hardware equipment production and shipment July 2020 \u00b6 Start Phase 3 of the deconfinement as per UL policy Preparation work within the CDC server room by the UL external partners slowly restarted target finalization of the CDC-S02-004 server room by end of September Assembly and factory Burn tests completed * Lot 1 : DLC ready for shipment to University Target date: Sept 14, 2020 in practice postponed above Oct 19, 2020 to allow for the CDC preparation work to be completed by the University and its patners. ULHPC maintenance with physical intervention of external expert support team by DDN preparation work for iris storage (HW upgrade, GPFS/SpectrumScale Metadata pool extension, Lustre upgrade) Start and complete the first Statement of Work for DDN Lot 2 installation Aug 2020 \u00b6 Consolidated work by ULHPC team on Slurm configuration Updated model for Fairshare, Account Hierarchy and limits Pre-shipment of [Part of] Ethernet network equipment (Lot 3) Sept - Oct 2020 \u00b6 Delivery Lot 1 (Aion DLC) and Lot 3 (Ethernet) equipment Ethernet network installation done by ULHPC between Sept 3 - 24, 2020 CDC S02-004 preparation work (hydraulic part) supposed to be completed by Sept 15, 2020 has been delayed and was finally completed on Oct 19, 2020 Nov 2020 \u00b6 Partial Delivery of equipment (servers, core switches) Service servers and remaining network equipments were racked by ULHPC team Dec 2020 \u00b6 Confinement restriction lifted in France, allowing for a french team from Atos to come onsite Delivery of remaining equipment (incl. Lot 1 sequana racks and compute nodes) Compute rack (Lot 1 DLC) installation start 2021 \u00b6 Jan - Feb 2021 \u00b6 The 4 DDN expansion enclosure shipped with the lifting tools and pressure tools Lot 1 : Sequana racks and compute nodes finally postionned and internal Infiniband cabling done Lot 2 : DDN disk enclosure racked the rack was adapted to be able to close the rear door Lot 3 : Ethernet and IB Network ULHPC cables were used to cable service servers to make progress on the software configuration Service servers and compute nodes deployment start remotely Mar - Apr 2021 \u00b6 Start GS7990 and NAS server installation (Lot 2) Start installtion of Lot 3 (ethernet side) May - June 2021 \u00b6 IB EDR cables delivered and installed Merge of the Iris/Aion Infiniband island Jul - Aug - Sept 2021 \u00b6 Slurm Federation between both clusters Iris and Aion Benchmark performance results submitted (HPL, HPCG, Green500, Graph500, IOR, IO500) Pre-Acceptance validated and release of the Aion supercomputer for beta testers Oct - Nov 2021 \u00b6 Official production release of Aion supercomputer Inauguration of Aion Supercomputer 11 th ULHPC School 2021 relies in Aion for its practical sessions","title":"Timeline"},{"location":"systems/aion/timeline/#aion-timeline","text":"This page records a brief timeline of significant events and user environment changes on Aion. Details are provided below.","title":"Aion Timeline"},{"location":"systems/aion/timeline/#2019","text":"","title":"2019"},{"location":"systems/aion/timeline/#september-2019","text":"Official Public release of Aion cluster tenders on TED European tender and PMP Portal (Portail des March\u00e9s Publics) on Sept 11, 2019 RFP 190027 : Tender for the acquisition of a Complementary High Performance Computing (HPC) cluster 'Aion' for the University of Luxembourg. TED Reference: TED72/2019-608787 PMP Reference: 1901442 The RFP is composed of the following Lots: Lot 1 : DLC Computing cluster aion . includes cabinets, cooling units (together with any and all required piping and cooling fluids), compute node enclosures and compute nodes blades (dual- socket, relying on x86_64 processor architecture), interconnect elements (Ethernet and InfiniBand) and management servers for the HPC services (including but not limited to operational lifecycle management, Operating System and services deployment, monitoring) tied to this new cluster implementation within one of the University Computer Centre (CDC) server rooms (CDC S-02-004), adjacent to the server room hosting the Iris cluster and its associated storage system), specialized for hosting compute equipment supporting direct liquid cooling through a separate high temperature water circuit, thus guaranteeing unprecedented energy efficiency and equipment density. In the first phase of operation, the system will be connected to the existing cold-water circuit and must be able to operate under these conditions [...] Lot 2 : Adaptation and extension of the existing High-Performance Storage systems includes extension of the existing primary high-performance storage solution featuring a SpectrumScale/GPFS filesystem (based on a DDN GridScaler solution installed as per attribution of the RFP 160019) hosting the user home and project directories, to enable the utilisation of the GPFS filesystem from both existing Iris and new Aion clusters while enabling access to the Lustre-based SCRATCH filesystem (based on a DDN ExaScaler solution installed as per attribution of the RFP 170035) from the new compute cluster is considered a plus. Enhancing and adapting the InfiniBand interconnection to guarantee current performance characteristics while under load from all clients (existing and new compute clusters) is considered a plus. [...] Lot 3 : Adaptation of the network (Ethernet and IB) integration of the new cluster within the existing Ethernet-based data and management networks, which involves the extension and consolidation of the actual Ethernet topology adaptation and extension of the existing InfiniBand (IB) topology to allow for bridging the two networks (Iris \"island\" and Aion \"island\") [...]","title":"September 2019"},{"location":"systems/aion/timeline/#october-november-2019","text":"Bids Opening for both RFPs on October 29, 2019. Starting offers analysis by the ULHPC team, together with the procurement and legal department of the University","title":"October-November 2019"},{"location":"systems/aion/timeline/#december-2019","text":"Awarding notification to the vendors RFP 190027 attributed to the Atos to provide: Lot 1 : the new DLC aion supercomputer, composed by 318 AMD compute nodes hosted within a compute cell made of 4 BullSequana XH2000 adjacent racks Fast Interconnect: HDR Infiniband Fabric in a Fat tree topology (2:1 blocking) Associated servers and management stack Lot 2 : Adaptation and extension of the existing High-Performance Storage systems. In particular, the usable storage capacity of the existing primary high-performance storage solution (SpectrumScale/GPFS filesystem) will be extended by 1720TB/1560TiB to reach a total of 4.41 PB Lot 3 :Adaptation of the network (Ethernet and IB) See also Atos Press Release Aion Supercomputer Overview","title":"December 2019"},{"location":"systems/aion/timeline/#2020","text":"","title":"2020"},{"location":"systems/aion/timeline/#january-2020","text":"Kickoff meeting -- see UL Newsletter planning for a production release of the new cluster in May 2020","title":"January 2020"},{"location":"systems/aion/timeline/#february-march-2020-start-of-global-covid-19-crisis","text":"COVID-19 Impact on HPC Activities all operations tied to the preparation and installation of the new aion cluster are postponed ULHPC systems remain operational, technical and non-technical staff are working remotely from home Global worldwide delays on hardware equipment production and shipment","title":"February-March 2020: Start of global COVID-19 crisis"},{"location":"systems/aion/timeline/#july-2020","text":"Start Phase 3 of the deconfinement as per UL policy Preparation work within the CDC server room by the UL external partners slowly restarted target finalization of the CDC-S02-004 server room by end of September Assembly and factory Burn tests completed * Lot 1 : DLC ready for shipment to University Target date: Sept 14, 2020 in practice postponed above Oct 19, 2020 to allow for the CDC preparation work to be completed by the University and its patners. ULHPC maintenance with physical intervention of external expert support team by DDN preparation work for iris storage (HW upgrade, GPFS/SpectrumScale Metadata pool extension, Lustre upgrade) Start and complete the first Statement of Work for DDN Lot 2 installation","title":"July 2020"},{"location":"systems/aion/timeline/#aug-2020","text":"Consolidated work by ULHPC team on Slurm configuration Updated model for Fairshare, Account Hierarchy and limits Pre-shipment of [Part of] Ethernet network equipment (Lot 3)","title":"Aug 2020"},{"location":"systems/aion/timeline/#sept-oct-2020","text":"Delivery Lot 1 (Aion DLC) and Lot 3 (Ethernet) equipment Ethernet network installation done by ULHPC between Sept 3 - 24, 2020 CDC S02-004 preparation work (hydraulic part) supposed to be completed by Sept 15, 2020 has been delayed and was finally completed on Oct 19, 2020","title":"Sept - Oct 2020"},{"location":"systems/aion/timeline/#nov-2020","text":"Partial Delivery of equipment (servers, core switches) Service servers and remaining network equipments were racked by ULHPC team","title":"Nov 2020"},{"location":"systems/aion/timeline/#dec-2020","text":"Confinement restriction lifted in France, allowing for a french team from Atos to come onsite Delivery of remaining equipment (incl. Lot 1 sequana racks and compute nodes) Compute rack (Lot 1 DLC) installation start","title":"Dec 2020"},{"location":"systems/aion/timeline/#2021","text":"","title":"2021"},{"location":"systems/aion/timeline/#jan-feb-2021","text":"The 4 DDN expansion enclosure shipped with the lifting tools and pressure tools Lot 1 : Sequana racks and compute nodes finally postionned and internal Infiniband cabling done Lot 2 : DDN disk enclosure racked the rack was adapted to be able to close the rear door Lot 3 : Ethernet and IB Network ULHPC cables were used to cable service servers to make progress on the software configuration Service servers and compute nodes deployment start remotely","title":"Jan - Feb 2021"},{"location":"systems/aion/timeline/#mar-apr-2021","text":"Start GS7990 and NAS server installation (Lot 2) Start installtion of Lot 3 (ethernet side)","title":"Mar - Apr 2021"},{"location":"systems/aion/timeline/#may-june-2021","text":"IB EDR cables delivered and installed Merge of the Iris/Aion Infiniband island","title":"May - June 2021"},{"location":"systems/aion/timeline/#jul-aug-sept-2021","text":"Slurm Federation between both clusters Iris and Aion Benchmark performance results submitted (HPL, HPCG, Green500, Graph500, IOR, IO500) Pre-Acceptance validated and release of the Aion supercomputer for beta testers","title":"Jul - Aug - Sept 2021"},{"location":"systems/aion/timeline/#oct-nov-2021","text":"Official production release of Aion supercomputer Inauguration of Aion Supercomputer 11 th ULHPC School 2021 relies in Aion for its practical sessions","title":"Oct - Nov 2021"},{"location":"systems/iris/","text":"Iris Overview \u00b6 Iris is a Dell /Intel supercomputer which consists of 196 compute nodes, totaling 5824 compute cores and 52224 GB RAM, with a peak performance of about 1,072 PetaFLOP/s . All nodes are interconnected through a Fast InfiniBand (IB) EDR network 1 , configured over a Fat-Tree Topology (blocking factor 1:1.5). Iris nodes are equipped with Intel Broadwell or Skylake processors. Several nodes are equipped with 4 Nvidia Tesla V100 SXM2 GPU accelerators. In total, Iris features 96 Nvidia V100 GPU-AI accelerators allowing for high speedup of GPU-enabled applications and AI/Deep Learning-oriented workflows. Finally, a few large-memory (fat) computing nodes offer multiple high-core density CPUs and a large live memory capacity of 3 TB RAM/node, meant for in-memory processing of huge data sets. Two global high -performance clustered file systems are available on all ULHPC computational systems: one based on GPFS/SpectrumScale, one on Lustre. Iris Compute Iris Interconnect Global Storage The cluster runs a Red Hat Linux Family operating system. The ULHPC Team supplies on all clusters a large variety of HPC utilities, scientific applications and programming libraries to its user community. The user software environment is generated using Easybuild (EB) and is made available as environment modules from the compute nodes only. Slurm is the Resource and Job Management Systems (RJMS) which provides computing resources allocations and job execution. For more information: see ULHPC slurm docs . Cluster Organization \u00b6 Login/Access servers \u00b6 Iris has 2 access servers (128 GB of memory each, general access) access[1-2] Each login node has two sockets, each socket is populated with an Intel Xeon E5-2697A v4 processor (2.6 GHz, 16 core) Access servers are not meant for compute! The module command is not available on the access servers, only on the compute nodes you MUST NOT run any computing process on the access servers . Rack Cabinets \u00b6 The Iris cluster (management, compute and interconnect) is installed across 7 racks within a row of cabinets in the premises of the Centre de Calcul (CDC), in the CDC-S02-005 server room. Server Room Rack ID Purpose Type Description CDC-S02-005 D02 Network Interconnect equipment CDC-S02-005 D04 Management Management servers, Interconnect CDC-S02-005 D05 Compute regular iris-[001-056] , interconnect CDC-S02-005 D07 Compute regular iris-[057-112] , interconnect CDC-S02-005 D09 Compute regular iris-[113-168] , interconnect CDC-S02-005 D11 Compute gpu, bigmem iris-[169-177,191-193] (gpu), iris-[187-188] (bigmem) CDC-S02-005 D12 Compute gpu, bigmem iris-[178-186,194-196] (gpu), iris-[189-190] (bigmem) In addition, the global storage equipment ( GPFS/SpectrumScale and Lustre , common to both Iris and Aion clusters) is installed in another row of cabinets of the same server room. Infiniband (IB) EDR networks offer a 100 Gb/s throughput with a very low latency (0,6 \\mu \\mu s). \u21a9","title":"Iris System"},{"location":"systems/iris/#iris-overview","text":"Iris is a Dell /Intel supercomputer which consists of 196 compute nodes, totaling 5824 compute cores and 52224 GB RAM, with a peak performance of about 1,072 PetaFLOP/s . All nodes are interconnected through a Fast InfiniBand (IB) EDR network 1 , configured over a Fat-Tree Topology (blocking factor 1:1.5). Iris nodes are equipped with Intel Broadwell or Skylake processors. Several nodes are equipped with 4 Nvidia Tesla V100 SXM2 GPU accelerators. In total, Iris features 96 Nvidia V100 GPU-AI accelerators allowing for high speedup of GPU-enabled applications and AI/Deep Learning-oriented workflows. Finally, a few large-memory (fat) computing nodes offer multiple high-core density CPUs and a large live memory capacity of 3 TB RAM/node, meant for in-memory processing of huge data sets. Two global high -performance clustered file systems are available on all ULHPC computational systems: one based on GPFS/SpectrumScale, one on Lustre. Iris Compute Iris Interconnect Global Storage The cluster runs a Red Hat Linux Family operating system. The ULHPC Team supplies on all clusters a large variety of HPC utilities, scientific applications and programming libraries to its user community. The user software environment is generated using Easybuild (EB) and is made available as environment modules from the compute nodes only. Slurm is the Resource and Job Management Systems (RJMS) which provides computing resources allocations and job execution. For more information: see ULHPC slurm docs .","title":"Iris Overview"},{"location":"systems/iris/#cluster-organization","text":"","title":"Cluster Organization"},{"location":"systems/iris/#loginaccess-servers","text":"Iris has 2 access servers (128 GB of memory each, general access) access[1-2] Each login node has two sockets, each socket is populated with an Intel Xeon E5-2697A v4 processor (2.6 GHz, 16 core) Access servers are not meant for compute! The module command is not available on the access servers, only on the compute nodes you MUST NOT run any computing process on the access servers .","title":"Login/Access servers"},{"location":"systems/iris/#rack-cabinets","text":"The Iris cluster (management, compute and interconnect) is installed across 7 racks within a row of cabinets in the premises of the Centre de Calcul (CDC), in the CDC-S02-005 server room. Server Room Rack ID Purpose Type Description CDC-S02-005 D02 Network Interconnect equipment CDC-S02-005 D04 Management Management servers, Interconnect CDC-S02-005 D05 Compute regular iris-[001-056] , interconnect CDC-S02-005 D07 Compute regular iris-[057-112] , interconnect CDC-S02-005 D09 Compute regular iris-[113-168] , interconnect CDC-S02-005 D11 Compute gpu, bigmem iris-[169-177,191-193] (gpu), iris-[187-188] (bigmem) CDC-S02-005 D12 Compute gpu, bigmem iris-[178-186,194-196] (gpu), iris-[189-190] (bigmem) In addition, the global storage equipment ( GPFS/SpectrumScale and Lustre , common to both Iris and Aion clusters) is installed in another row of cabinets of the same server room. Infiniband (IB) EDR networks offer a 100 Gb/s throughput with a very low latency (0,6 \\mu \\mu s). \u21a9","title":"Rack Cabinets"},{"location":"systems/iris/compute/","text":"Iris Compute Nodes \u00b6 Iris is a cluster of x86-64 Intel-based compute nodes. More precisely, Iris consists of 196 computational nodes named iris-[001-196] and features 3 types of computing resources: 168 \" regular \" nodes, Dual Intel Xeon Broadwell or Skylake CPU (28 cores), 128 GB of RAM 24 \" gpu \" nodes, Dual Intel Xeon Skylake CPU (28 cores), 4 Nvidia Tesla V100 SXM2 GPU accelerators (16 or 32 GB), 768 GB RAM 4 \" bigmem \" nodes: Quad-Intel Xeon Skylake CPU (112 cores), 3072 GB RAM Hostname (#Nodes) Node type Processor RAM iris-[001-108] (108) Regular Broadwell 2 Xeon E5-2680v4 @ 2.4GHz [14c/120W] 128 GB iris-[109-168] (60) Regular Skylake 2 Xeon Gold 6132 @ 2.6GHz [14c/140W] 128 GB iris-[169-186] (18) Multi-GPU Skylake 2 Xeon Gold 6132 @ 2.6GHz [14c/140W] 4x Tesla V100 SXM2 16G 768 GB iris-[191-196] (6) Multi-GPU Skylake 2 Xeon Gold 6132 @ 2.6GHz [14c/140W] 4x Tesla V100 SXM2 32G 768 GB iris-[187-190] (4) Large Memory Skylake 4 Xeon Platinum 8180M @ 2.5GHz [28c/205W] 3072 GB Processors Performance \u00b6 Each Iris node rely on an Intel x86_64 processor architecture with the following performance: Processor Model #core TDP(*) CPU Freq. ( AVX-512 T.Freq. ) R_\\text{peak} R_\\text{peak} [TFlops] R_\\text{max} R_\\text{max} [TFlops] Xeon E5-2680v4 (Broadwell) 14 120W 2.4GHz ( n/a) 0.538 TF 0.46 TF Xeon Gold 6132 (Skylake) 14 140W 2.6GHz ( 2.3GHz ) 1.03 TF 0.88 TF Xeon Platinum 8180M (Skylake) 28 205W 2.5GHz ( 2.3GHz ) 2.06 TF 1.75 TF (*) The Thermal Design Power (TDP) represents the average power, in watts, the processor dissipates when operating at Base Frequency with all cores active under an Intel-defined, high-complexity workload. Theoretical R_\\text{peak} R_\\text{peak} vs. Maximum R_\\text{max} R_\\text{max} Performance for Intel Broadwell/Skylake The reported R_\\text{peak} R_\\text{peak} performance is computed as follows for the above processors: The Broadwell processors carry on 16 Double Precision (DP) ops/cycle and support AVX2/FMA3. The selected Skylake Gold processors have two AVX512 units, thus they are capable of performing 32 DP ops/cycle YET only upon AVX-512 Turbo Frequency ( i.e. , the maximum all-core frequency in turbo mode) in place of the base non-AVX core frequency. The reported values are extracted from the Reference Intel Specification documentation . Then R_\\text{peak} = ops/cycle \\times Freq. \\times \\#Cores R_\\text{peak} = ops/cycle \\times Freq. \\times \\#Cores with the appropriate frequency (2.3 GHz instead of 2.6 for our Skylake processors). With regards the estimation of the Maximum Performance R_\\text{max} R_\\text{max} , an efficiency factor of 85% is applied. It is computed from the expected performance runs during the HPL benchmark workload. Accelerators Performance \u00b6 Iris is equipped with 96 NVIDIA Tesla V100-SXM2 GPU Accelerators with 16 or 32 GB of GPU memory, interconnected within each node through NVLink which provides higher bandwidth and improved scalability for multi-GPU system configurations. NVidia GPU Model #CUDA core #Tensor core Power Interconnect Bandwidth GPU Memory R_\\text{peak} R_\\text{peak} [TFlops] V100-SXM2 5120 640 300W 300 GB/s 16GB 7.8 TF V100-SXM2 5120 640 300W 300 GB/s 32GB 7.8 TF Regular Dual-CPU Nodes \u00b6 These nodes are packaged within Dell PowerEdge C6300 chassis, each hosting 4 PowerEdge C6320 blade servers. Broadwell Compute Nodes \u00b6 Iris comprises 108 Dell C6320 \"regular\" compute nodes iris-001-108 relying on Broadwell Xeon processor generation, totalling 3024 computing cores. Each node is configured as follows: 2 Intel Xeon E5-2680v4 @ 2.4GHz [14c/120W] RAM: 128 GB DDR4 2400MT/s (4x16 GB DIMMs per socket , 8 DIMMs per node) SSD 120GB InfiniBand (IB) EDR ConnectX-4 Single Port Theoretical Peak Performance per Node: R_\\text{peak} R_\\text{peak} 1.075 TF (see processor performance ) Reserving a Broadwell node If you want to specifically reserve a broadwell node ( iris-[001-108] ), you should use the feature -C broadwell on the batch partition: {sbatch|srun|salloc} -p batch -C broadwell [...] Skylake Compute Nodes \u00b6 Iris also features 60 Dell C6320 \"regular\" compute nodes iris-109-168 relying on Skylake Xeon processor generation, totalling 1680 computing cores. Each node is configured as follows: 2 Intel Xeon Gold 6132 @ 2.6GHz [14c/140W] RAM: 128 GB DDR4 2400MT/s (4x16 GB DIMMs per socket , 8 DIMMs per node) SSD 120GB InfiniBand (IB) EDR ConnectX-4 Single Port Theoretical Peak Performance per Node: R_\\text{peak} R_\\text{peak} 2.061 TF (see processor performance ) Reserving a Regular Skylake node If you want to specifically reserve a regular skylake node ( iris-[109-168] ), you should use the feature -C skylake on the batch partition: {sbatch|srun|salloc} -p batch -C skylake [...] Multi-GPU Compute Nodes \u00b6 Iris includes 24 Dell PowerEdge C4140 \"gpu\" compute nodes embedding on total 96 NVIDIA Tesla V100-SXM2 GPU Accelerators. Each node is configured as follows: 2 Intel Xeon Gold 6132 @ 2.6GHz [14c/140W] RAM: 768 GB DDR4 2666MT/s (12x 32 GB DIMMs per socket , 24 DIMMs per node) 1 Dell NVMe 1.6TB InfiniBand (IB) EDR ConnectX-4 Dual Port 4x NVIDIA Tesla V100-SXM2 GPU Accelerators over NVLink iris-[169-186] feature 16G GPU memory - use -C volta as slurm feature iris-[191-196] feature 32G GPU memory - use -C volta32 as slurm feature Theoretical Peak Performance per Node: R_\\text{peak} R_\\text{peak} 33.26 TF (see processor performance and accelerators performance ) Reserving a GPU node Multi-GPU Compute Nodes can be reserved using the gpu partition. Use the -G [<type>:]<number> to specify the total number of GPUs required for the job # Interactive job on 1 GPU nodes with 1 GPU si-gpu -G 1 nvidia-smi # Check allocated GPU # Interactive job with 4 GPUs on the same node, one task per gpu, 7 cores per task si-gpu -N 1 -G 4 --ntasks-per-node 4 --ntasks-per-socket 2 -c 7 # Job submission on 2 nodes, 4 GPUs/node and 4 tasks/node: sbatch -p gpu -N 2 -G 4 --ntasks-per-node 4 --ntasks-per-socket 2 -c 7 launcher.sh Do NOT reserve a GPU node if you don't need a GPU! Multi-GPU nodes are scarce (and very expansive ) resources and should be dedicated to GPU-enabled workflows. 16 GB vs. 32 GB Onboard GPU Memory Compute nodes with Nvidia V100-SMX2 16GB accelerators are registrered with the -C volta feature. it corresponds to the 18 Multi-GPU compute nodes iris-[169-186] If you want to reserve GPUs with more memory ( i.e. 32GB on-board HBM2), you should use -C volta32 you would then end on one of the 6 Multi-GPU compute nodes iris-[191-196] Large-Memory Compute Nodes \u00b6 Iris holds 4 Dell PowerEdge R840 Large-Memory (\" bibmem \") compute nodes iris-[187-190] , totalling 448 computing cores. Each node is configured as follows: 4 Xeon Platinum 8180M @ 2.5GHz [28c/205W] RAM: 3072 GB DDR4 2666MT/s (12x64 GB DIMMs per socket , 48 DIMMs per node) 1 Dell NVMe 1.6TB InfiniBand (IB) EDR ConnectX-4 Dual Port Theoretical Peak Performance per Node: R_\\text{peak} R_\\text{peak} 8.24 TF (see processor performance ) Reserving a Large-Memory node These nodes can be reserved using the bigmem partition: {sbatch|srun|salloc} -p bigmem [...] DO NOT use bigmem nodes... ... Unless you know what you are doing. We have too few large-memory compute nodes so kindly keep them for workloads that truly need these kind of expansive resources. In short : carefully check your workflow and memory usage before considering using these node! use seff <jobid> or sacct -j <jobid> [...] for instance","title":"Compute Nodes"},{"location":"systems/iris/compute/#iris-compute-nodes","text":"Iris is a cluster of x86-64 Intel-based compute nodes. More precisely, Iris consists of 196 computational nodes named iris-[001-196] and features 3 types of computing resources: 168 \" regular \" nodes, Dual Intel Xeon Broadwell or Skylake CPU (28 cores), 128 GB of RAM 24 \" gpu \" nodes, Dual Intel Xeon Skylake CPU (28 cores), 4 Nvidia Tesla V100 SXM2 GPU accelerators (16 or 32 GB), 768 GB RAM 4 \" bigmem \" nodes: Quad-Intel Xeon Skylake CPU (112 cores), 3072 GB RAM Hostname (#Nodes) Node type Processor RAM iris-[001-108] (108) Regular Broadwell 2 Xeon E5-2680v4 @ 2.4GHz [14c/120W] 128 GB iris-[109-168] (60) Regular Skylake 2 Xeon Gold 6132 @ 2.6GHz [14c/140W] 128 GB iris-[169-186] (18) Multi-GPU Skylake 2 Xeon Gold 6132 @ 2.6GHz [14c/140W] 4x Tesla V100 SXM2 16G 768 GB iris-[191-196] (6) Multi-GPU Skylake 2 Xeon Gold 6132 @ 2.6GHz [14c/140W] 4x Tesla V100 SXM2 32G 768 GB iris-[187-190] (4) Large Memory Skylake 4 Xeon Platinum 8180M @ 2.5GHz [28c/205W] 3072 GB","title":"Iris Compute Nodes"},{"location":"systems/iris/compute/#processors-performance","text":"Each Iris node rely on an Intel x86_64 processor architecture with the following performance: Processor Model #core TDP(*) CPU Freq. ( AVX-512 T.Freq. ) R_\\text{peak} R_\\text{peak} [TFlops] R_\\text{max} R_\\text{max} [TFlops] Xeon E5-2680v4 (Broadwell) 14 120W 2.4GHz ( n/a) 0.538 TF 0.46 TF Xeon Gold 6132 (Skylake) 14 140W 2.6GHz ( 2.3GHz ) 1.03 TF 0.88 TF Xeon Platinum 8180M (Skylake) 28 205W 2.5GHz ( 2.3GHz ) 2.06 TF 1.75 TF (*) The Thermal Design Power (TDP) represents the average power, in watts, the processor dissipates when operating at Base Frequency with all cores active under an Intel-defined, high-complexity workload. Theoretical R_\\text{peak} R_\\text{peak} vs. Maximum R_\\text{max} R_\\text{max} Performance for Intel Broadwell/Skylake The reported R_\\text{peak} R_\\text{peak} performance is computed as follows for the above processors: The Broadwell processors carry on 16 Double Precision (DP) ops/cycle and support AVX2/FMA3. The selected Skylake Gold processors have two AVX512 units, thus they are capable of performing 32 DP ops/cycle YET only upon AVX-512 Turbo Frequency ( i.e. , the maximum all-core frequency in turbo mode) in place of the base non-AVX core frequency. The reported values are extracted from the Reference Intel Specification documentation . Then R_\\text{peak} = ops/cycle \\times Freq. \\times \\#Cores R_\\text{peak} = ops/cycle \\times Freq. \\times \\#Cores with the appropriate frequency (2.3 GHz instead of 2.6 for our Skylake processors). With regards the estimation of the Maximum Performance R_\\text{max} R_\\text{max} , an efficiency factor of 85% is applied. It is computed from the expected performance runs during the HPL benchmark workload.","title":"Processors Performance"},{"location":"systems/iris/compute/#accelerators-performance","text":"Iris is equipped with 96 NVIDIA Tesla V100-SXM2 GPU Accelerators with 16 or 32 GB of GPU memory, interconnected within each node through NVLink which provides higher bandwidth and improved scalability for multi-GPU system configurations. NVidia GPU Model #CUDA core #Tensor core Power Interconnect Bandwidth GPU Memory R_\\text{peak} R_\\text{peak} [TFlops] V100-SXM2 5120 640 300W 300 GB/s 16GB 7.8 TF V100-SXM2 5120 640 300W 300 GB/s 32GB 7.8 TF","title":"Accelerators Performance"},{"location":"systems/iris/compute/#regular-dual-cpu-nodes","text":"These nodes are packaged within Dell PowerEdge C6300 chassis, each hosting 4 PowerEdge C6320 blade servers.","title":"Regular Dual-CPU Nodes"},{"location":"systems/iris/compute/#broadwell-compute-nodes","text":"Iris comprises 108 Dell C6320 \"regular\" compute nodes iris-001-108 relying on Broadwell Xeon processor generation, totalling 3024 computing cores. Each node is configured as follows: 2 Intel Xeon E5-2680v4 @ 2.4GHz [14c/120W] RAM: 128 GB DDR4 2400MT/s (4x16 GB DIMMs per socket , 8 DIMMs per node) SSD 120GB InfiniBand (IB) EDR ConnectX-4 Single Port Theoretical Peak Performance per Node: R_\\text{peak} R_\\text{peak} 1.075 TF (see processor performance ) Reserving a Broadwell node If you want to specifically reserve a broadwell node ( iris-[001-108] ), you should use the feature -C broadwell on the batch partition: {sbatch|srun|salloc} -p batch -C broadwell [...]","title":"Broadwell Compute Nodes"},{"location":"systems/iris/compute/#skylake-compute-nodes","text":"Iris also features 60 Dell C6320 \"regular\" compute nodes iris-109-168 relying on Skylake Xeon processor generation, totalling 1680 computing cores. Each node is configured as follows: 2 Intel Xeon Gold 6132 @ 2.6GHz [14c/140W] RAM: 128 GB DDR4 2400MT/s (4x16 GB DIMMs per socket , 8 DIMMs per node) SSD 120GB InfiniBand (IB) EDR ConnectX-4 Single Port Theoretical Peak Performance per Node: R_\\text{peak} R_\\text{peak} 2.061 TF (see processor performance ) Reserving a Regular Skylake node If you want to specifically reserve a regular skylake node ( iris-[109-168] ), you should use the feature -C skylake on the batch partition: {sbatch|srun|salloc} -p batch -C skylake [...]","title":"Skylake Compute Nodes"},{"location":"systems/iris/compute/#multi-gpu-compute-nodes","text":"Iris includes 24 Dell PowerEdge C4140 \"gpu\" compute nodes embedding on total 96 NVIDIA Tesla V100-SXM2 GPU Accelerators. Each node is configured as follows: 2 Intel Xeon Gold 6132 @ 2.6GHz [14c/140W] RAM: 768 GB DDR4 2666MT/s (12x 32 GB DIMMs per socket , 24 DIMMs per node) 1 Dell NVMe 1.6TB InfiniBand (IB) EDR ConnectX-4 Dual Port 4x NVIDIA Tesla V100-SXM2 GPU Accelerators over NVLink iris-[169-186] feature 16G GPU memory - use -C volta as slurm feature iris-[191-196] feature 32G GPU memory - use -C volta32 as slurm feature Theoretical Peak Performance per Node: R_\\text{peak} R_\\text{peak} 33.26 TF (see processor performance and accelerators performance ) Reserving a GPU node Multi-GPU Compute Nodes can be reserved using the gpu partition. Use the -G [<type>:]<number> to specify the total number of GPUs required for the job # Interactive job on 1 GPU nodes with 1 GPU si-gpu -G 1 nvidia-smi # Check allocated GPU # Interactive job with 4 GPUs on the same node, one task per gpu, 7 cores per task si-gpu -N 1 -G 4 --ntasks-per-node 4 --ntasks-per-socket 2 -c 7 # Job submission on 2 nodes, 4 GPUs/node and 4 tasks/node: sbatch -p gpu -N 2 -G 4 --ntasks-per-node 4 --ntasks-per-socket 2 -c 7 launcher.sh Do NOT reserve a GPU node if you don't need a GPU! Multi-GPU nodes are scarce (and very expansive ) resources and should be dedicated to GPU-enabled workflows. 16 GB vs. 32 GB Onboard GPU Memory Compute nodes with Nvidia V100-SMX2 16GB accelerators are registrered with the -C volta feature. it corresponds to the 18 Multi-GPU compute nodes iris-[169-186] If you want to reserve GPUs with more memory ( i.e. 32GB on-board HBM2), you should use -C volta32 you would then end on one of the 6 Multi-GPU compute nodes iris-[191-196]","title":"Multi-GPU Compute Nodes"},{"location":"systems/iris/compute/#large-memory-compute-nodes","text":"Iris holds 4 Dell PowerEdge R840 Large-Memory (\" bibmem \") compute nodes iris-[187-190] , totalling 448 computing cores. Each node is configured as follows: 4 Xeon Platinum 8180M @ 2.5GHz [28c/205W] RAM: 3072 GB DDR4 2666MT/s (12x64 GB DIMMs per socket , 48 DIMMs per node) 1 Dell NVMe 1.6TB InfiniBand (IB) EDR ConnectX-4 Dual Port Theoretical Peak Performance per Node: R_\\text{peak} R_\\text{peak} 8.24 TF (see processor performance ) Reserving a Large-Memory node These nodes can be reserved using the bigmem partition: {sbatch|srun|salloc} -p bigmem [...] DO NOT use bigmem nodes... ... Unless you know what you are doing. We have too few large-memory compute nodes so kindly keep them for workloads that truly need these kind of expansive resources. In short : carefully check your workflow and memory usage before considering using these node! use seff <jobid> or sacct -j <jobid> [...] for instance","title":"Large-Memory Compute Nodes"},{"location":"systems/iris/interconnect/","text":"Fast Local Interconnect Network \u00b6 The Fast local interconnect network implemented within Iris relies on the Mellanox Infiniband (IB) EDR 1 technology. For more details, see Introduction to High-Speed InfiniBand Interconnect . One of the most significant differentiators between HPC systems and lesser performing systems is, apart from the interconnect technology deployed, the supporting topology. There are several topologies commonly used in large-scale HPC deployments ( Fat-Tree , 3D-Torus , Dragonfly+ etc.). Iris (like Aion ) is part of an Island which employs a \" Fat-Tree \" Topology 2 which remains the widely used topology in HPC clusters due to its versatility, high bisection bandwidth and well understood routing. Iris 2-Level 1:1.5 Fat-Tree is composed of: 18x Infiniband EDR 1 Mellanox SB7800 switches (36 ports) 12x Leaf IB (LIB) switches (L1), each with 12 EDR L1-L2 interlinks 6x Spine IB (SIB) switches (L2), with 8 EDR downlinks (total: 48 links) used for the interconnexion with the Aion Cluster Up to 24 Iris compute nodes and servers EDR connection per L1 switch using 24 EDR ports For more details: ULHPC Fast IB Interconnect Illustration of Iris network cabling (IB and Ethernet) within one of the rack hosting the compute nodes: Enhanced Data Rate (EDR) \u2013 100 Gb/s throughput with a very low latency, typically below 0,6 \\mu \\mu s. \u21a9 \u21a9 with blocking factor 1:1.5. \u21a9","title":"Fast Local Interconnect"},{"location":"systems/iris/interconnect/#fast-local-interconnect-network","text":"The Fast local interconnect network implemented within Iris relies on the Mellanox Infiniband (IB) EDR 1 technology. For more details, see Introduction to High-Speed InfiniBand Interconnect . One of the most significant differentiators between HPC systems and lesser performing systems is, apart from the interconnect technology deployed, the supporting topology. There are several topologies commonly used in large-scale HPC deployments ( Fat-Tree , 3D-Torus , Dragonfly+ etc.). Iris (like Aion ) is part of an Island which employs a \" Fat-Tree \" Topology 2 which remains the widely used topology in HPC clusters due to its versatility, high bisection bandwidth and well understood routing. Iris 2-Level 1:1.5 Fat-Tree is composed of: 18x Infiniband EDR 1 Mellanox SB7800 switches (36 ports) 12x Leaf IB (LIB) switches (L1), each with 12 EDR L1-L2 interlinks 6x Spine IB (SIB) switches (L2), with 8 EDR downlinks (total: 48 links) used for the interconnexion with the Aion Cluster Up to 24 Iris compute nodes and servers EDR connection per L1 switch using 24 EDR ports For more details: ULHPC Fast IB Interconnect Illustration of Iris network cabling (IB and Ethernet) within one of the rack hosting the compute nodes: Enhanced Data Rate (EDR) \u2013 100 Gb/s throughput with a very low latency, typically below 0,6 \\mu \\mu s. \u21a9 \u21a9 with blocking factor 1:1.5. \u21a9","title":"Fast Local Interconnect Network"},{"location":"systems/iris/timeline/","text":"Iris Timeline \u00b6 This page records a brief timeline of significant events and user environment changes on Iris. The Iris cluster exists since the beginning of 2017 as the flagship HPC supercomputer within the University of Luxembourg until 2020 and the release of the Aion supercomputer. 2016 \u00b6 September 2016 \u00b6 Official Public release of Iris cluster tenders on TED European tender and Portail des March\u00e9s Publiques (PMP) RFP 160019 : High Performance Storage System for the High Performance Computing Facility of the University of Luxembourg. RFP 160020 : High Performance Computing Facility (incl. Interconnect) for the University of Luxembourg. October 2016 \u00b6 Bids Opening for both RFPs on October 12, 2016. Starting offers analysis by the ULHPC team, together with the procurement and legal departments of the University November 2016 \u00b6 Awarding notification to the vendors RFP 160019 attributed to the Telindus/HPE/DDN consortium to provide High Performance Storage solution of capacity 1.44 PB (raw) (over GPFS/SpectrumScale Filesystem), with a RW performance above 10GB/s RFP 160020 attributed to the Post/DELL consortium to provide a High Performance Computing (HPC) cluster of effective capacity R_\\text{max} R_\\text{max} = 94.08 TFlops (raw capacity R_\\text{peak} R_\\text{peak} = 107.52 TFlops) 2017 \u00b6 March-April 2017 \u00b6 Delivery and installation of the iris cluster composed of: iris-[1-100] , Dell PowerEdge C6320, 100 nodes, 2800 cores, 12.8 TB RAM 10/40GB Ethernet network, high-speed Infiniband EDR 100Gb/s interconnect SpectrumScale (GPFS) core storage, 1.44 PB Redundant / load-balanced services with: 2x adminfront servers (cluster management) 2x access servers (user frontend) May-June 2017 \u00b6 End of cluster validation 8 new regular nodes added iris-[101-108] , Dell PowerEdge C6320, 8 nodes, 224 cores, 1.024 TB RAM Official release of the iris cluster for production on June 12, 2017 at the occasion of the UL HPC Scool 2017 . October 2017 \u00b6 Official Public release of Iris Lustre Storage acquisition tenders on TED European tender and Portail des March\u00e9s Publiques (PMP) RFP 170035 : Complementary Lustre High Performance Storage System for the High Performance Computing Facility of the University of Luxembourg. November 2017 \u00b6 Bids Opening for Lustre RFP on November 28, 2017. Starting offers analysis by the ULHPC team, together with the procurement and legal departments of the University December 2017 \u00b6 Awarding notification to the vendors Lustre RFP 170035 attributed to the Fujitsu/DDN consortium to provide High Performance Storage solution of capacity 1.28 PB (raw) 60 new regular nodes added yet based on Skylake processors iris-[109-168] , Dell PowerEdge C6420, 60 nodes, 1680 cores, 7.68 TB RAM 2018 \u00b6 February 2018 \u00b6 iris cluster moved from CDC S-01 to CDC S-02 April 2018 \u00b6 SpectrumScale (GPFS) DDN GridScaler extension to reach 2284TB raw capacity new expansion unit and provisioning of enough complementary disks to feed the system. Delivery and installation of the complementary Lustre storage, with 1280 TB raw capacity July 2018 \u00b6 Official Public release of tenders on TED European tender and Portail des March\u00e9s Publiques (PMP) RFP 180027 : Complementary Multi-GPU and Large-Memory Computer Nodes for the High Performance Computing Facility of the University of Luxembourg. September 2018 \u00b6 Bids Opening for Multi-GPU and Large-Memory nodes RFP on September 10, 2018. Starting offers analysis by the ULHPC team, together with the procurement and legal departments of the University October 2018 \u00b6 Awarding notification to the vendors RFP 180027 attributed to the Dimension Data/Dell consortium Dec 2018 \u00b6 New Multi-GPU and Bigmem compute nodes added iris-[169-186] : Dell C4140, 18 GPU nodes x 4 Nvidia V100 SXM2 16GB, part of the gpu partition iris-[187-190] : Dell R840, 4 Bigmem nodes 4x28c i.e. 112 cores per node, part of the bigmem partition 2019 \u00b6 May 2019 \u00b6 6 new Multi-GPU nodes added iris-[191-196] : Dell C4140, 6 GPU nodes x 4 Nvidia V100 SXM2 32GB, part of the gpu partition October 2019 \u00b6 SpectrumScale (GPFS) extension to allow 1Bn files capacity replacement of 2 data pools (HDD-based) with new metadata pools (SSD-based)","title":"Timeline"},{"location":"systems/iris/timeline/#iris-timeline","text":"This page records a brief timeline of significant events and user environment changes on Iris. The Iris cluster exists since the beginning of 2017 as the flagship HPC supercomputer within the University of Luxembourg until 2020 and the release of the Aion supercomputer.","title":"Iris Timeline"},{"location":"systems/iris/timeline/#2016","text":"","title":"2016"},{"location":"systems/iris/timeline/#september-2016","text":"Official Public release of Iris cluster tenders on TED European tender and Portail des March\u00e9s Publiques (PMP) RFP 160019 : High Performance Storage System for the High Performance Computing Facility of the University of Luxembourg. RFP 160020 : High Performance Computing Facility (incl. Interconnect) for the University of Luxembourg.","title":"September 2016"},{"location":"systems/iris/timeline/#october-2016","text":"Bids Opening for both RFPs on October 12, 2016. Starting offers analysis by the ULHPC team, together with the procurement and legal departments of the University","title":"October 2016"},{"location":"systems/iris/timeline/#november-2016","text":"Awarding notification to the vendors RFP 160019 attributed to the Telindus/HPE/DDN consortium to provide High Performance Storage solution of capacity 1.44 PB (raw) (over GPFS/SpectrumScale Filesystem), with a RW performance above 10GB/s RFP 160020 attributed to the Post/DELL consortium to provide a High Performance Computing (HPC) cluster of effective capacity R_\\text{max} R_\\text{max} = 94.08 TFlops (raw capacity R_\\text{peak} R_\\text{peak} = 107.52 TFlops)","title":"November 2016"},{"location":"systems/iris/timeline/#2017","text":"","title":"2017"},{"location":"systems/iris/timeline/#march-april-2017","text":"Delivery and installation of the iris cluster composed of: iris-[1-100] , Dell PowerEdge C6320, 100 nodes, 2800 cores, 12.8 TB RAM 10/40GB Ethernet network, high-speed Infiniband EDR 100Gb/s interconnect SpectrumScale (GPFS) core storage, 1.44 PB Redundant / load-balanced services with: 2x adminfront servers (cluster management) 2x access servers (user frontend)","title":"March-April 2017"},{"location":"systems/iris/timeline/#may-june-2017","text":"End of cluster validation 8 new regular nodes added iris-[101-108] , Dell PowerEdge C6320, 8 nodes, 224 cores, 1.024 TB RAM Official release of the iris cluster for production on June 12, 2017 at the occasion of the UL HPC Scool 2017 .","title":"May-June 2017"},{"location":"systems/iris/timeline/#october-2017","text":"Official Public release of Iris Lustre Storage acquisition tenders on TED European tender and Portail des March\u00e9s Publiques (PMP) RFP 170035 : Complementary Lustre High Performance Storage System for the High Performance Computing Facility of the University of Luxembourg.","title":"October 2017"},{"location":"systems/iris/timeline/#november-2017","text":"Bids Opening for Lustre RFP on November 28, 2017. Starting offers analysis by the ULHPC team, together with the procurement and legal departments of the University","title":"November 2017"},{"location":"systems/iris/timeline/#december-2017","text":"Awarding notification to the vendors Lustre RFP 170035 attributed to the Fujitsu/DDN consortium to provide High Performance Storage solution of capacity 1.28 PB (raw) 60 new regular nodes added yet based on Skylake processors iris-[109-168] , Dell PowerEdge C6420, 60 nodes, 1680 cores, 7.68 TB RAM","title":"December 2017"},{"location":"systems/iris/timeline/#2018","text":"","title":"2018"},{"location":"systems/iris/timeline/#february-2018","text":"iris cluster moved from CDC S-01 to CDC S-02","title":"February 2018"},{"location":"systems/iris/timeline/#april-2018","text":"SpectrumScale (GPFS) DDN GridScaler extension to reach 2284TB raw capacity new expansion unit and provisioning of enough complementary disks to feed the system. Delivery and installation of the complementary Lustre storage, with 1280 TB raw capacity","title":"April 2018"},{"location":"systems/iris/timeline/#july-2018","text":"Official Public release of tenders on TED European tender and Portail des March\u00e9s Publiques (PMP) RFP 180027 : Complementary Multi-GPU and Large-Memory Computer Nodes for the High Performance Computing Facility of the University of Luxembourg.","title":"July 2018"},{"location":"systems/iris/timeline/#september-2018","text":"Bids Opening for Multi-GPU and Large-Memory nodes RFP on September 10, 2018. Starting offers analysis by the ULHPC team, together with the procurement and legal departments of the University","title":"September 2018"},{"location":"systems/iris/timeline/#october-2018","text":"Awarding notification to the vendors RFP 180027 attributed to the Dimension Data/Dell consortium","title":"October 2018"},{"location":"systems/iris/timeline/#dec-2018","text":"New Multi-GPU and Bigmem compute nodes added iris-[169-186] : Dell C4140, 18 GPU nodes x 4 Nvidia V100 SXM2 16GB, part of the gpu partition iris-[187-190] : Dell R840, 4 Bigmem nodes 4x28c i.e. 112 cores per node, part of the bigmem partition","title":"Dec 2018"},{"location":"systems/iris/timeline/#2019","text":"","title":"2019"},{"location":"systems/iris/timeline/#may-2019","text":"6 new Multi-GPU nodes added iris-[191-196] : Dell C4140, 6 GPU nodes x 4 Nvidia V100 SXM2 32GB, part of the gpu partition","title":"May 2019"},{"location":"systems/iris/timeline/#october-2019","text":"SpectrumScale (GPFS) extension to allow 1Bn files capacity replacement of 2 data pools (HDD-based) with new metadata pools (SSD-based)","title":"October 2019"}]}